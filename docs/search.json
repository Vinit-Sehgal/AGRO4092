[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R for Spatial Analysis & Visualization",
    "section": "",
    "text": "1 Introduction\nStatistical computing is essential for scientific inquiry, discovery, and storytelling. As the availability of dataset and access to computing has significantly increased over the recent years, the scope of scientific inquiry is restricted, only, by the imagination of the inquirer.\n\n“Scientific inquiry starts with observation. The more one can see, the more one can investigate” — Martin Chalfie\n\nHowever, analysis of large-scale geospatial data (regional- to global scale at high spatial and temporal resolution) can be computationally expensive and time-consuming, especially when working with multiple formats and sources of data. R- a higher-level programming language, provides a powerful computational alternative to popular Geographic Information System (GIS) software to organize, analyze and visualize geospatial datasets. R enjoys a vast collection of open-source libraries for GIS-type operations and proven statistical analysis and data visualization capabilities.\nIn this course, we equip ourselves with hands-on knowledge of accessing, analyzing, and visualizing open-source satellite remote-sensing and geospatial datasets for hydrological, agricultural, and climatological studies within the R environment. The objective of this course is to learn R for:\n\nAnalyzing geospatial datasets (raster and vector),\nPerforming statistical analysis for each feature/ layer, and,\nMapping and visualizing spatial datasets.\n\nThe course will include the latest R tools for working with global Earth-observation datasets from several remote-sensing platforms, such as NASA’s MODIS, SMAP and LANDSAT. Basic operations of geospatial analysis such as (re)projection, (re)sampling, summary statistics, merge/join, and (re)shape will be covered in this course. The students will be introduced to structured/layered spatial datasets such as NetCDF/HDF formats used in climate modeling. We will explore several open-source resources for accessing and acquiring hydrometeorological and land dataset for plant, environmental and soil studies. Special emphasis will be placed on applying out-of-the-box parallel computing techniques with custom user-defined function for geospatial analysis.\n\n\n\n\n\nWe will first start with a refresher of basic R programming in Chapter 1. In Chapter 2 and 3, we will explore spatial data visualization, before we learn about large-scale application of parallel computing for geospatial analysis.\nPurpose of this document:\nThis resource will serve as a dynamic class note, where students can access detailed concepts, codes and exercises related to this class. Notes will be updated regularly as the class progresses with up-to-date material and upcoming assignments."
  },
  {
    "objectID": "ch1.html#operators-and-data-types",
    "href": "ch1.html#operators-and-data-types",
    "title": "2  Basics of R",
    "section": "2.1 Operators and data types",
    "text": "2.1 Operators and data types\n\n2.1.1 Basic operators\nIn this section, we will learn about some basic R operators that are used to perform operations on variables. Some most commonly used operators are shown in the table below.\n\n\n\n\nR follows the conventional order (sequence) to solve mathematical operations, abbreviated as BODMAS: Brackets, Orders (exponents), Division, Multiplication, Addition, and Subtraction\n\n\n2+4+7 # Sum\n\n[1] 13\n\n4-5   # Subtraction\n\n[1] -1\n\n2*3   # Multiplication\n\n[1] 6\n\n1/2   # Division\n\n[1] 0.5\n\n# Order of operation\n1/2*3+4-5\n\n[1] 0.5\n\n1/2*(3+4-5)\n\n[1] 1\n\n1/(2*(3+4-5))\n\n[1] 0.25\n\n1/(2*3+4-5) \n\n[1] 0.2\n\n# Notice how output changes with the placement of operators\n\n# Other operators:\n2^3\n\n[1] 8\n\nlog(10)\n\n[1] 2.302585\n\nsqrt(4)\n\n[1] 2\n\npi\n\n[1] 3.141593\n\n# Clear the Environment\nrm(list=ls()) # rm is for remove,ls is short for list. The empty parenthesis i.e. () signifies all content. \n\n\n\n2.1.2 Basic data operations\nIn this section, we will create some vector data and apply built-in operations to examine the properties of a dataset.\n\n# The \"is equal to\" or \"assignment operator in R is \"&lt;-\" or \"=\" \n\n# Generate sample data. Remember \"c\" comes from for \"concatenate\". \ndata&lt;-c(1,4,2,3,9)    # Try data = c(1,4,2,3,9). Is there any difference in data in both cases?\n\n# rbind combines data by rows, and hence \"r\"bind\n# cbind combines data by columns, and hence \"c\"bind\n\n# Checking the properties of a dataset. Note: the na.rm argument ignores NA values in the dataset.\ndata=rbind(1,4,2,3,9) \ndim(data)           # [5,1]: 5 rows, 1 column\n\n[1] 5 1\n\ndata[2,1]           # Show the value in row 2, column 1\n\n[1] 4\n\ndata[c(2:5),1]      # Show a range of values in column 1\n\n[1] 4 2 3 9\n\nmean(data, na.rm=T) # Mean\n\n[1] 3.8\n\nmax(data)           # Maximum\n\n[1] 9\n\nmin(data)           # Minimum\n\n[1] 1\n\nsd(data)            # Standard deviation\n\n[1] 3.114482\n\nvar(data)           # Variance\n\n     [,1]\n[1,]  9.7\n\nsummary(data) \n\n       V1     \n Min.   :1.0  \n 1st Qu.:2.0  \n Median :3.0  \n Mean   :3.8  \n 3rd Qu.:4.0  \n Max.   :9.0  \n\nstr(data)        # Prints structure of data\n\n num [1:5, 1] 1 4 2 3 9\n\nhead(data)       # Returns the 1st 6 items in the object\n\n     [,1]\n[1,]    1\n[2,]    4\n[3,]    2\n[4,]    3\n[5,]    9\n\nhead(data, 2)    # Print first 2\n\n     [,1]\n[1,]    1\n[2,]    4\n\ntail(data, 2)    # Print last 2\n\n     [,1]\n[4,]    3\n[5,]    9\n\n# Do the same, but with \"c()\" instead of \"rbind\"\ndata=c(1,4,2,3,9) \ndim(data)        # Note: dim is NULL\n\nNULL\n\nlength(data)     # Length of a dataset is the number of variables (columns)\n\n[1] 5\n\ndata[2]          # This should give you 4 \n\n[1] 4\n\n# Other operators work in the same way\nmean(data)       # Mean\n\n[1] 3.8\n\nmax(data)        # Maximum\n\n[1] 9\n\nmin(data)        # Minimum\n\n[1] 1\n\nsd(data)         # Standard deviation\n\n[1] 3.114482\n\nvar(data)        # Variance\n\n[1] 9.7\n\n# Text data\ndata=c(\"LSU\",\"SPESS\",\"AgCenter\",\"Tigers\") \ndata             # View\n\n[1] \"LSU\"      \"SPESS\"    \"AgCenter\" \"Tigers\"  \n\ndata[1]\n\n[1] \"LSU\"\n\n# Mixed data\ndata=c(1,\"LSU\",10,\"AgCenter\") # All data is treated as text if one value is text\ndata[3]                       # Note how output is in quotes i.e. \"10\"\n\n[1] \"10\"\n\n\n\nFor help with a function in R, just type ? followed by the function to display information in the help menu. Try pasting ?sd in the console.\n\n\n\n2.1.3 Data types\nIn R, data is stored as an “array”, which can be 1-dimensional or 2-dimensional. A 1-D array is called a “vector” and a 2-D array is a “matrix”. A table in R is called a “data frame” and a “list” is a container to hold a variety of data types. In this section, we will learn how to create matrices, lists and data frames in R.\n\n\n\n\n# Lets make a random matrix\ntest_mat = matrix( c(2, 4, 3, 1, 5, 7), # The data elements \n  nrow=2,         # Number of rows \n  ncol=3,         # Number of columns \n  byrow = TRUE)   # Fill matrix by rows \n\ntest_mat = matrix( c(2, 4, 3, 1, 5, 7),nrow=2,ncol=3,byrow = TRUE) # Same result \ntest_mat\n\n     [,1] [,2] [,3]\n[1,]    2    4    3\n[2,]    1    5    7\n\ntest_mat[,2]      # Display all rows, and second column\n\n[1] 4 5\n\ntest_mat[2,]      # Display second row, all columns\n\n[1] 1 5 7\n\n# Types of datasets\nout = as.matrix(test_mat)\nout               # This is a matrix\n\n     [,1] [,2] [,3]\n[1,]    2    4    3\n[2,]    1    5    7\n\nout = as.array(test_mat)\nout               # This is also a matrix\n\n     [,1] [,2] [,3]\n[1,]    2    4    3\n[2,]    1    5    7\n\nout = as.vector(test_mat)\nout               # This is just a vector\n\n[1] 2 1 4 5 3 7\n\n# Data frame and list\ndata1=runif(50,20,30) # Create 50 random numbers between 20 and 30  \ndata2=runif(50,0,10)  # Create 50 random numbers between 0 and 10  \n\n# Lists\nout = list()        # Create and empty list\nout[[1]] = data1    # Notice the brackets \"[[ ]]\" instead of \"[ ]\"\nout[[2]] = data2\nout[[1]]          # Contains data1 at this location\n\n [1] 20.19053 27.34920 24.29754 20.00858 21.93471 26.56492 28.50896 25.11268\n [9] 25.04088 25.28993 22.38669 23.32268 25.05874 24.19622 22.69125 29.94252\n[17] 29.53343 23.80884 27.45731 29.59762 23.70404 22.06187 22.08497 22.38794\n[25] 26.57366 22.74622 21.10927 26.77793 27.71489 21.84284 23.98676 29.24595\n[33] 21.73473 23.32222 24.22195 21.07999 26.23997 29.20960 28.09831 27.64942\n[41] 20.71046 26.20026 29.92468 22.27300 21.45807 21.64099 23.49302 29.64648\n[49] 21.55291 26.04208\n\n# Data frame\nout=data.frame(x=data1, y=data2)\n\n# Let's see how it looks!\nplot(out$x, out$y)\n\n\n\nplot(out[,1])\n\n\n\n\n\nFor a data frame, the dollar “$” sign invokes the variable selection. Imagine how one would receive merchandise in a store if you give $ to the cashier. Data frame will list out the variable names for you of you when you show it some $."
  },
  {
    "objectID": "ch1.html#plotting-with-base-r",
    "href": "ch1.html#plotting-with-base-r",
    "title": "2  Basics of R",
    "section": "2.2 Plotting with base R",
    "text": "2.2 Plotting with base R\nIf you need to quickly visualize your data, base R has some functions that will help you do this in a pinch. In this section we’ll look at some basics of visualizing univariate and multivariate data.\n\n2.2.1 Overview\n\n# Create 50 random numbers between 0 and 100  \ndata=runif(50, 0, 100)   #runif stands for random numbers from a uniform distribution\n\n# Let's plot the data\nplot(data)            # The \"plot\" function initializes the plot.\n\n\n\nplot(data, type=\"l\")  # The \"type\" argument changes the plot type. \"l\" calls up a line plot\n\n\n\nplot(data, type=\"b\")  # Buffered points joined by lines\n\n\n\n# Try options type = \"o\" and type = \"c\" as well.\n\n# We can also quickly visualize boxplots, histograms, and density plots using the same procedure\nboxplot(data)        # Box-and-whisker plot\n\n\n\nhist(data)           # Histogram points\n\n\n\nplot(density(data))  # Plot with density distribution \n\n\n\n\n\n\n2.2.2 Plotting univariate data\nLet’s dig deeper into the plot function. Here, we will look at how to adjust the colors, shapes, and sizes for markers, axis labels and titles, and the plot title.\n\n# Line plots\nplot(data,type=\"o\", col=\"red\",\n     xlab=\"x-axis title\",ylab =\"y-axis title\", \n     main=\"My plot\", # Name of axis labels and title\n     cex.axis=2, cex.main=2,cex.lab=2,            # Size of axes, title and label\n     pch=23,       # Change marker style\n     bg=\"red\",     # Change color of markers\n     lty=5,        # Change line style\n     lwd=2         # Selecting line width\n) \n# Adding legend\nlegend(1, 100, legend=c(\"Data 1\"),\n       col=c(\"red\"), lty=2, cex=1.2)\n\n\n\n# Histograms\nhist(data,col=\"red\",\n     xlab=\"Number\",ylab =\"Value\", main=\"My plot\", # Name of axis labels and title\n     border=\"blue\"\n) \n\n\n\n# Try adjusting the parameters:\n# hist(data,col=\"red\",\n#      xlab=\"Number\",ylab =\"Value\", main=\"My plot\", # Name of axis labels and title\n#      cex.axis=2, cex.main=2,cex.lab=2,            # Size of axes, title and label\n#      border=\"blue\", \n#      xlim=c(0,100), # Control the limits of the x-axis\n#      las=0,      # Try different values of las: 0,1,2,3 to rotate labels\n#      breaks=5    # Try using 5,20,50, 100\n# ) # Using more options and controls\n\n\n\n2.2.3 Plotting multivariate data\nHere, we introduce you to data frames: equivalent of tables in R. A data frame is a table with a two-dimensional array-like structure in which each column contains values of one variable and each row contains one set of values from each column.\n\nplot_data=data.frame(x=runif(50,0,10), \n                     y=runif(50,20,30), \n                     z=runif(50,30,40)) \n\nplot(plot_data$x, plot_data$y) # Scatter plot of x and y data\n\n\n\n# Mandatory beautification\nplot(plot_data$x,plot_data$y, xlab=\"Data X\", ylab=\"Data Y\", main=\"X vs Y plot\",\n     col=\"darkred\",pch=20,cex=1.5) # Scatter plot of x and y data\n\n\n\n# Multiple lines on one axis\nmatplot(plot_data, type = c(\"b\"),pch=16,col = 1:4) \n\n\n\nmatplot(plot_data, type = c(\"b\",\"l\",\"o\"),pch=16,col = 1:4) # Try this now. Any difference? \nlegend(\"topleft\", legend = 1:4, col=1:4, pch=1)            # Add legend to a top left\nlegend(\"top\", legend = 1:4, col=1:4, pch=1)                # Add legend to at top center\nlegend(\"bottomright\", legend = 1:4, col=1:4, pch=1)        # Add legend at the bottom right\n\n\n\n\n\n\n2.2.4 Time series data\nWorking with time series data can be tricky at first, but here’s a quick look at how to quickly generate a time series using the as.Date function.\n\ndate=seq(as.Date('2011-01-01'),as.Date('2011-01-31'),by = 1) # Generate a sequence 31 days\ndata=runif(31,0,10)                 # Generate 31 random values between 0 and 10\ndf=data.frame(Date=date,Value=data) # Combine the data in a data frame\nplot(df,type=\"o\")\n\n\n\n\n\n\n2.2.5 Combining plots\nYou can built plots that contain subplots. Using base R, we call start by using the “par” function and then plot as we saw before.\n\npar(mfrow=c(2,2)) # Call a plot with 4 quadrants\n\n# Plot 1\nmatplot(plot_data, type = c(\"b\"),pch=16,col = 1:4) \n\n# Plot 2\nplot(plot_data$x,plot_data$y) \n\n# Plot 3\nhist(data,col=\"red\",\n     xlab=\"Number\",ylab =\"Value\", main=\"My plot\", \n     border=\"blue\") \n\n# Plot4\nplot(data,type=\"o\", col=\"red\",\n     xlab=\"Number\",ylab =\"Value\", main=\"My plot\",\n     cex.axis=2, cex.main=2,cex.lab=2, \n     pch=23,   \n     bg=\"red\", \n     lty=5, \n     lwd=2 \n) \n\n\n\n# Alternatively, we can call up a plot using a matrix\nmatrix(c(1,1,2,3), 2, 2, byrow = TRUE) # Plot 1 is plotted for first two spots, followed by plot 2 and 3 \n\n     [,1] [,2]\n[1,]    1    1\n[2,]    2    3\n\nlayout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE)) # Fixes a layout of the plots we want to make\n\n # Plot 1\nmatplot(plot_data, type = c(\"b\"),pch=16,col = 1:4)\n\n# Plot2\nplot(plot_data$x,plot_data$y) \n\n# Plot 3\nhist(data,col=\"red\",\n     xlab=\"Number\",ylab =\"Value\", main=\"My plot\",\n     border=\"blue\"\n)\n\n\n\n\n\n\n2.2.6 Saving figures to disk\nPlots can be saved as image files or a PDF. This is done by specifying the output file type, its size and resolution, then calling the plot.\n\npng(\"awesome_plot.png\", width=4, height=4, units=\"in\", res=400) \n#Tells R we will plot image in png of given specification\n\nmatplot(plot_data, type = c(\"b\",\"l\",\"o\"),pch=16,col = 1:4)  \nlegend(\"topleft\", legend = 1:4, col=1:4, pch=1)\n\ndev.off() # Very important: this sends the image to disc\n\npng \n  2 \n\n# Keep pressing till you get the following: \n# Error in dev.off() : cannot shut down device 1 (the null device) \n# This ensures that we are no longer plotting.\n\n# It looks like what everything we just plotted was squeezed together to tightly. Let's change the size.\npng(\"awesome_plot.png\", width=6, height=4, units=\"in\", res=400)  #note change in dimension\n#Tells R we will plot image in png of given specification\n\nmatplot(plot_data, type = c(\"b\",\"l\",\"o\"),pch=16,col = 1:3)  \nlegend(\"topleft\", legend = 1:3, col=1:3, pch=16)\n\ndev.off() \n\npng \n  2 \n\n\n\nSome useful resources\nIf you want to plot something a certain way and don’t know how to do it, the chances are that someone has asked that question before. Try a Google search for what your are trying to do and check out some of the forums. There is TONS of material online. Here are some additional resources:\n\nThe R Graph Gallery: https://www.r-graph-gallery.com/\nGraphical parameters: https://www.statmethods.net/advgraphs/parameters.html\nPlotting in R: https://www.harding.edu/fmccown/r/\nHistogram: https://www.r-bloggers.com/how-to-make-a-histogram-with-basic-r/"
  },
  {
    "objectID": "ch1.html#plotting-with-ggplot2",
    "href": "ch1.html#plotting-with-ggplot2",
    "title": "2  Basics of R",
    "section": "2.3 Plotting with ggplot2",
    "text": "2.3 Plotting with ggplot2\n\n2.3.1 Import libraries and create sample dataset\nFor this section, we will use the ggplot2, gridExtra, utils, and tidyr packages. gridExtra and cowplot are used to combine ggplot objects into one plot and utils and tidyr are useful for manipulating and reshaping the data. We will also install some packages here that will be required for the later sections. You will find more information in the sections to follow.\n\n###############################################################\n#~~~ Load required libraries\nlib_names=c(\"ggplot2\",\"gridExtra\",\"utils\",\"tidyr\",\"cowplot\", \"RColorBrewer\")\n\n# If you see a prompt: Do you want to restart R prior to installing: Select **No**. \n\n# Install all necessary packages (Run once)\n# invisible(suppressMessages\n#           (suppressWarnings\n#             (lapply\n#               (lib_names,install.packages,repos=\"http://cran.r-project.org\",\n#                 character.only = T))))\n\n# Load necessary packages\ninvisible(suppressMessages\n          (suppressWarnings\n            (lapply\n              (lib_names,library, character.only = T))))\n\nIn more day-to-day use, you will see yourself using a simpler version of these commands, such as, if you were to install the “ggplot2”,“gridExtra” libraries, you will type:\n\n# To install the package. Install only once\ninstall.packages(\"ggplot2\")\n# To initialize the package. Invoke every time a new session begins.\nlibrary(ggplot2)\n\nSimilarly, again for gridExtra ,\n\ninstall.packages(\"gridExtra\")\nlibrary(gridExtra)\n\nFor this exercise, let us generate a sample dataset.\n\n###############################################################\n#~~~ Generate a dataset containing random numbers within specified ranges\nYear = seq(1913,2001,1)\nJan = runif(89, -18.4, -3.2)\nFeb = runif(89, -19.4, -1.2)\nMar = runif(89, -14, -1.8)\nJanuary = runif(89, 1, 86)\ndat = data.frame(Year, Jan, Feb, Mar, January)\n\n\n\n2.3.2 Basics of ggplot\nWhereas base R has an “ink on paper” plotting paradigm, ggplot has a “grammar of graphics” paradigm that packages together a variety plotting functions. With ggplot, you assign the result of a function to an object name and then modify it by adding additional functions. Think of it as adding layers using pre-designed functions rather than having to build those functions yourself, as you would have to do with base R.\n\nl1 = ggplot(data=dat, aes(x = Year, y = Jan, color = \"blue\")) + # Tell which data to plot\n  geom_line() +      # Add a line\n  geom_point() +     # Add a points\n  xlab(\"Year\") +     # Add labels to the axes\n  ylab(\"Value\")\n\n# Or, they can be specified for any individual geometry\nl1 + geom_line(linetype = \"solid\", color=\"Blue\")  # Add a solid line\n\n\n\nl1 + geom_line(aes(x = Year, y = January)) # Add a different data set\n\n\n\n# There are tons of other built-in color scales and themes, such as scale_color_grey(), scale_color_brewer(), theme_classic(), theme_minimal(), and theme_dark()\n\n# OR, CREATE YOUR OWN THEME! You can group themes together in one list\ntheme1 = theme(\n  legend.position = \"none\",\n  panel.background = element_blank(),\n  plot.title = element_text(hjust = 0.5),\n  axis.line = element_line(color = \"black\"),\n  axis.text.y   = element_text(size = 11),\n  axis.text.x   = element_text(size = 11),\n  axis.title.y  = element_text(size = 11),\n  axis.title.x  = element_text(size = 11),\n  panel.border = element_rect(\n    colour = \"black\",\n    fill = NA,\n    size = 0.5\n  )\n)\n\n\n\n2.3.3 Multivariate plots\nFor multivariate data, ggplot takes the data in the form of groups. This means that each data row should be identifiable to a group. To get the most out of ggplot, we will need to reshape our dataset.\n\nlibrary(tidyr)\n\n# There are two generally data formats: wide (horizontal) and long (vertical). In the horizontal format, every column represents a category of the data. In the vertical format, every row represents an observation for a particular category (think of each row as a data point). Both formats have their comparative advantages. We will now convert the data frame we randomly generated in the previous section to the long format. Here are several ways to do this:\n\n# Using the gather function (the operator %&gt;% is called pipe operator)\ndat2 = dat %&gt;% gather(Month, Value, -Year)\n\n# This is equivalent to: \ndat2 = gather(data=dat, Month, Value, -Year)\n\n# Using pivot_longer and selecting all of the columns we want. This function is the best!\ndat2 = dat %&gt;% pivot_longer(cols = c(Jan, Feb, Mar), names_to = \"Month\", values_to = \"Value\") \n\n# Or we can choose to exclude the columns we don't want\ndat2 = dat %&gt;% pivot_longer(cols = -c(Year,January), names_to = \"Month\", values_to = \"Value\") \n\nhead(dat2) # The data is now shaped in the long format\n\n# A tibble: 6 × 4\n   Year January Month  Value\n  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1  1913    25.0 Jan   -13.9 \n2  1913    25.0 Feb   -16.9 \n3  1913    25.0 Mar   -11.1 \n4  1914    78.6 Jan   -14.2 \n5  1914    78.6 Feb    -9.96\n6  1914    78.6 Mar    -3.96\n\n\nLine plot\n\n# LINE PLOT\nl = ggplot(dat2, aes(x = Year, y = Value, group = Month)) +\n  geom_line(aes(color = Month)) +\n  geom_point(aes(color = Month))\nl\n\n\n\n\nDensity plot\n\n# DENSITY PLOT\nd = ggplot(dat2, aes(x = Value))\nd = d + geom_density(aes(color = Month, fill = Month), alpha=0.4) # Alpha specifies transparency\nd\n\n\n\n\nHistogram\n\n# HISTOGRAM\nh = ggplot(dat2, aes(x = Value))\nh = h + geom_histogram(aes(color = Month, fill = Month), alpha=0.4,\n                 fill = \"white\",\n                 position = \"dodge\")\nh\n\n\n\n\nGrid plotting and saving files to disk\nThere are multiple ways to arrange multiple plots and save images. One method is using grid.arrange() which is found in the gridExtra package. You can then save the file using ggsave, which comes with the ggplot2 library.\n\n# The plots can be displayed together on one image using \n# grid.arrange from the gridExtra package\nimg = grid.arrange(l, d, h, nrow=3)\n\n\n\n# Finally, plots created using ggplot can be saved using ggsave\nggsave(\"grid_plot_1.png\", \n       plot = img, \n       device = \"png\", \n       width = 6, \n       height = 4, \n       units = c(\"in\"), \n       dpi = 600)\n\nAnother approach is to use the plot_grid function, which is in the cowplot library. Notice how the axes are now beautifally aligned.\n\nimg2=cowplot::plot_grid(l, d, h, nrow = 3, align = \"v\") # \"v\" aligns vertical axes and \"h\" aligns horizontal axes\n\nggsave(\"grid_plot_2.png\", \n       plot = img2, \n       device = \"png\", \n       width = 6, \n       height = 4, \n       units = c(\"in\"), \n       dpi = 600)\n\n\n\n2.3.4 Using patchwork for combining ggplots\nPatchwork works with simple operators to combine plots. The operator | arranges plots in a row. The plus sign + does the same but it will try to wrap the plots symmetrically as a square whenever possible. The division i.e. /operator layers a plot on top of another.\n\n#install.packages(\"patchwork\")\nlibrary(patchwork)\n\nl+d\n\n\n\nl/ (h+d)\n\n\n\n# Try: l/d/h or (l+d)/h \n\n# Make your own design for arranging plots (the # sign means empty space): \ndesign &lt;- \"\n  111\n  2#3\n\"\nl + d + h + plot_layout(design = design)\n\n\n\n\n\nSome useful resources\nThe links below offer a treasure trove of examples and sample code to get you started.\n\nThe R Graph Gallery: https://www.r-graph-gallery.com/\nR charts: https://r-charts.com/\nExcellent resource for combining multiple ggplots: https://r-charts.com/ggplot2/combining-plots/"
  },
  {
    "objectID": "ch1.html#exercise-1",
    "href": "ch1.html#exercise-1",
    "title": "2  Basics of R",
    "section": "2.4 Exercise #1",
    "text": "2.4 Exercise #1\nThe U.S. Climate Reference Network (USCRN) is a systematic and sustained network of climate monitoring stations. USCRN has sites across Contiguous U.S. along with some in Alaska, and Hawaii. These stations are instrumented to measure meteorological information such as temperature, precipitation, wind speed, along with other relevant hydrologic variables such as soil moisture at uniform depths (5, 10, 20, 50, 100 cm) at sub-hourly, daily and monthly time scales. Users can access daily data set from all station suing the following link: Index of /pub/data/uscrn/products/daily01 (noaa.gov)\nLet us extract sample data from a USCRN site in Lafayette, LA, USA for 2021.\n\n# Yearly data from the sample station\nCRNdat = read.csv(url(\"https://www.ncei.noaa.gov/pub/data/uscrn/products/daily01/2021/CRND0103-2021-LA_Lafayette_13_SE.txt\"), header=FALSE,sep=\"\")\n\n# Data headers\nheaders=read.csv(url(\"https://www.ncei.noaa.gov/pub/data/uscrn/products/daily01/headers.txt\"), header=FALSE,sep=\"\")\n\n# Column names as headers from the text file\ncolnames(CRNdat)=headers[2,1:ncol(CRNdat)]\n\n# Replace fill values with NA\nCRNdat[CRNdat == -9999]=NA\nCRNdat[CRNdat == -99]=NA\nCRNdat[CRNdat == 999]=NA\n\n# View data sample\nlibrary(kableExtra)\ndataTable = kbl(head(CRNdat,6),full_width = F)\nkable_styling(dataTable,bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\n\nWBANNO\nLST_DATE\nCRX_VN\nLONGITUDE\nLATITUDE\nT_DAILY_MAX\nT_DAILY_MIN\nT_DAILY_MEAN\nT_DAILY_AVG\nP_DAILY_CALC\nSOLARAD_DAILY\nSUR_TEMP_DAILY_TYPE\nSUR_TEMP_DAILY_MAX\nSUR_TEMP_DAILY_MIN\nSUR_TEMP_DAILY_AVG\nRH_DAILY_MAX\nRH_DAILY_MIN\nRH_DAILY_AVG\nSOIL_MOISTURE_5_DAILY\nSOIL_MOISTURE_10_DAILY\nSOIL_MOISTURE_20_DAILY\nSOIL_MOISTURE_50_DAILY\nSOIL_MOISTURE_100_DAILY\nSOIL_TEMP_5_DAILY\nSOIL_TEMP_10_DAILY\nSOIL_TEMP_20_DAILY\nSOIL_TEMP_50_DAILY\nSOIL_TEMP_100_DAILY\n\n\n\n\n53960\n20210101\n2.622\n-91.87\n30.09\n14.0\n5.2\n9.6\n11.0\n0.0\n12.16\nC\n18.7\n4.6\n11.6\n92.8\n49.3\n72.0\n0.401\n0.372\n0.380\n0.405\n0.381\n16.2\n15.3\n15.5\n15.7\n15.5\n\n\n53960\n20210102\n2.622\n-91.87\n30.09\n10.4\n1.9\n6.1\n6.5\n0.0\n8.95\nC\n15.3\n0.4\n7.3\n98.6\n61.6\n78.4\n0.396\n0.370\n0.377\n0.406\n0.376\n14.4\n13.3\n14.1\n15.2\n15.0\n\n\n53960\n20210103\n2.622\n-91.87\n30.09\n16.3\n-0.1\n8.1\n7.9\n0.0\n13.93\nC\n24.3\n-0.9\n9.5\n100.0\n42.1\n76.3\n0.392\n0.368\n0.374\n0.404\n0.373\n12.8\n11.8\n12.8\n14.4\n14.2\n\n\n53960\n20210104\n2.622\n-91.87\n30.09\n22.2\n3.7\n12.9\n12.5\n0.0\n11.56\nC\n26.4\n2.6\n13.2\n98.9\n47.7\n80.2\n0.389\n0.366\n0.370\n0.400\n0.372\n13.0\n12.2\n12.7\n14.0\n14.0\n\n\n53960\n20210105\n2.622\n-91.87\n30.09\n20.7\n4.5\n12.6\n11.4\n0.0\n14.37\nC\n28.9\n3.1\n13.3\n100.0\n27.7\n71.0\n0.388\n0.364\n0.368\n0.399\n0.369\n13.0\n12.1\n12.7\n13.9\n14.0\n\n\n53960\n20210106\n2.622\n-91.87\n30.09\n19.4\n4.9\n12.2\n12.6\n20.7\n9.79\nC\n23.1\n3.5\n12.8\n98.5\n54.7\n78.9\n0.390\n0.363\n0.369\n0.399\n0.370\n12.8\n12.1\n12.5\n13.7\n13.7\n\n\n\n\n\n\n\n\nNotice the variables provided in the dataset. As an example, we can plots soil moisture data from a depth of 20 cm for this station for our reference:\n\n# Sample plot for soil moisture\nx=CRNdat$SOIL_MOISTURE_20_DAILY\n\n# Plot time series and density distribution \nplot(x, type=\"l\", ylab=\"Soil moisture (v/v)\", \n     col=\"cyan4\", lwd=3)\nplot(density(na.omit(x)), main=\" \", xlab=\"\", \n     col=\"cyan4\", lwd=3)\n\n\n\n\n\n\n\n(a) Time series of SM\n\n\n\n\n\n\n\n(b) SM kernel density\n\n\n\n\nFigure 2.1: Soil moisture values at the selected USCRN station\n\n\n\nExercise:\n\nTaking examples of any two USCRN stations across contrasting hydroclimates, compare and contrast any two recorded variables using time series plots, probability density distribution histograms and scatter plots. Select any year of your liking for the analysis.\nSelect two seasons for each elected variable and demonstrate the seasonal variability in the records for summer (MAMJJA) and winter (SONDJF) seasons using any two types of multivariate plots.\n[EXTRA]: For any chosen station, plot a time-series of soil moisture from all available layers with precipitation added as an inverted secondary axis. For inspiration, see Figure 4 in Cheng, et al. 2021. On change of soil moisture distribution with vegetation reconstruction in Mu Us sandy land of China, with newly designed lysimeter. Frontiers in Plant Science, 12, p.60952 at https://www.frontiersin.org/articles/10.3389/fpls.2021.609529/full"
  },
  {
    "objectID": "ch2.html#overview",
    "href": "ch2.html#overview",
    "title": "3  Spatial Mapping in R",
    "section": "3.1 Overview",
    "text": "3.1 Overview\nIn this chapter we will learn about different types of spatial datasets (raster and vector). We will visualize these spatial datasets using static and interactive plotting options available in R. We will also explore different color palettes options available for generating spatial maps for scientific/technical reporting of spatial datasets. We will also learn briefly explore coordinate reference systems and map projections for spatial data representation.\n\n3.1.1 Sample dataset\nWe will familiarize ourselves with several open-source global datasets and use them to practice spatial mapping and computing in R.\n\n\n\n\nGlobal surface soil moisture from NASA’s Soil Moisture Active Passive (SMAP) satellite (Entekhabi, Njoku, and O’Neill 2009)\n\nhttps://smap.jpl.nasa.gov/\n\n\n\n\nNormalized Difference Vegetation Index (NDVI) from Moderate Resolution Imaging Spectroradiometer (MODIS) (Huete, Justice, and Van Leeuwen 1999)\n\nhttps://modis.gsfc.nasa.gov/data/dataprod/mod13.php\n\n\n\n\nGlobal climate reference land regions from Coupled Model Intercomparison Project (CMIP) project (Iturbide et al. 2020)\n\nhttps://essd.copernicus.org/articles/12/2959/2020/\n\n\n\n\nClimate classification (Hyper-arid, arid, semi-arid, sub-humid and humid) based on Global Aridity Index Database (Zomer, Xu, and Trabucco 2022)\n\nhttps://csidotinfo.wordpress.com/2019/01/24/global-aridity-index-and-potential-evapotranspiration-climate-database-v3/\n\n\n\n\n\n\n3.1.2 Data download\nThe sample dataset for this resource is uploaded to GitHub for easy access. Download the sample data manually as a zip file from: https://github.com/Vinit-Sehgal/SampleData . Once downloaded, extract the zip folder to the current working directory.\nAlternatively, use the following script to programmatically download and extract the sample data from the GitHub repository.\n\n###############################################################\n#~~~ Import sample data from GitHub repository\n\nif (dir.exists(\"SampleData-master\")==FALSE){ \n# First we check if the folder already exists. If not, the download begins\ndownload.file(url = \"https://github.com/Vinit-Sehgal/SampleData/archive/master.zip\",\ndestfile = \"SampleData-master.zip\")    # Download \".Zip\"\n\n# Unzip the downloaded .zip file\nunzip(zipfile = \"SampleData-master.zip\")\n}\n\n# getwd()                           # Current working directory\nlist.files(\"./SampleData-master\")   # List folder contents. Do you see sample datasets?\n\n [1] \"CMIP_land\"                               \n [2] \"functions\"                               \n [3] \"images\"                                  \n [4] \"Largescale_geospatial_analysis_2022.html\"\n [5] \"Largescale_geospatial_analysis_2023.html\"\n [6] \"location_points.xlsx\"                    \n [7] \"ne_10m_coastline\"                        \n [8] \"raster_files\"                            \n [9] \"README.md\"                               \n[10] \"sample_pdfs\"                             \n[11] \"SMAP_L3_USA.nc\"                          \n[12] \"SMAPL4_H5\"                               \n[13] \"SMAPL4_rasters\"                          \n[14] \"SMOS_nc\"                                 \n[15] \"USA_states\"                              \n[16] \"Workbook_DVGAR21-Part1.html\"             \n[17] \"Workbook_DVGAR21-Part2.html\""
  },
  {
    "objectID": "ch2.html#using-r-as-gis",
    "href": "ch2.html#using-r-as-gis",
    "title": "3  Spatial Mapping in R",
    "section": "3.2 Using R as GIS",
    "text": "3.2 Using R as GIS\nA geographic information system, or GIS refers to a platform which can map, analyzes and manipulate geographically referenced dataset. A geographically referenced data (or geo-referenced data) is a spatial dataset which can be related to a point on Earth with the help of geographic coordinates. Types of geo-referenced spatial data include: rasters (grids of regularly sized pixels) and vectors (polygons, lines, points).\n\n\n\n\n\nA quick and helpful review of spatial data can be found here: https://spatialvision.com.au/blog-raster-and-vector-data-in-gis/\n\n3.2.1 Plotting raster data: with terra and tmap\nIn this section, we plot global raster data of surface (~5 cm) soil moisture from SMAP. Let’s start by first importing the global soil moisture raster.\n# Import package for raster operations\nlibrary(terra) \n\n# Import SMAP soil moisture raster from the downloaded folder\nsm=terra::rast(\"./SampleData-master/raster_files/SMAP_SM.tif\")\nOnce we have imported the SpatRaster (short for “spatial raster”) using rast() function from terra package, let’s note its attributes. Notice the dimensions, resolution, extent, crs i.e. coordinate reference system and values. Note that the cell of one raster layer can only hold a single numerical value.\n# Print raster attributes\nprint(sm)\nclass : SpatRaster dimensions : 456, 964, 1 (nrow, ncol, nlyr) resolution : 0.373444, 0.373444 (x, y) extent : -180, 180, -85.24595, 85.0445 (xmin, xmax, ymin, ymax) coord. ref. : lon/lat WGS 84 (EPSG:4326) source : SMAP_SM.tif name : SMAP_SM min value : 0.01999998 max value : 0.87667608\n# Try:\n# dim(sm)   # Dimension (nrow, ncol, nlyr) of the raster\n# terra::res(sm)   # X-Y resolution of the raster\n# terra::ext(sm)   # Spatial extent of the raster\n# terra::crs(sm)   # Coordinate reference system\nNow let’s plot the raster using terra::plot.\n# Basic Raster plot \nterra::plot(sm, main = \"Soil Moisture\") \n\n\n\n3.2.2 Color palettes\nUsing a good color palette is an important aspect of spatial mapping. Choice of a good colormap can help the readers understand the key aspects of the map. The selected colors must adequately represent the key features and their differences, wherever applicable, with the least distortion, ambiguity or effort. There are several libraries available in R specifically dedicated to generating color pelettes for scientific mapping. We will also learn the the usage of cetcolor and scico packages to generate perceptually uniform and color-blindness friendly palettes.\n\nSome key packages for generating color palettes for scientific mapping are:\n\nRColorBrewer: https://colorbrewer2.org/#type=sequential&scheme=BuGn&n=3\ncetcolors (Perceptually Uniform Colour Maps): https://cran.r-project.org/web/packages/cetcolor/vignettes/cet_color_schemes.html\nscico (Scientific colormaps): https://github.com/thomasp85/scico\n\n\n# Libraries for generating Colour palettes\nlibrary(RColorBrewer)\nlibrary(cetcolor)\nlibrary(scico)\n\n# To view color palette \nlibrary(unikn)   \n\n#~~ A) User defined color palette using brewer.pal\nmypal1 = RColorBrewer::brewer.pal(10, \"Spectral\") \n\n# Brewerpal outputs a max of 9-11 colors. So,the pelette may needs expansion.\nmypal2= colorRampPalette(mypal1)(20)         # Expand pelette to 20 colors\nunikn::seecol(mypal2)   \n\n\n\n# Some more advanced options include opacity and interpolation method \n# mypal2= colorRampPalette(mypal1,                         \n#                interpolate = c(\"linear\"), # Choose btw linear/spline interpolation\n#                alpha = 0.8)(20)           # Generate 20 colors, opacity set to 0.8\n# unikn::seecol(mypal2)   \n\n#~~ B) User defined color palette using scico package\nmypal1 = scico::scico(20, alpha = 1.0, direction =  -1, palette = \"vik\") \nunikn::seecol(mypal1)   \n\n\n\n#Check: scico_palette_names() for available palettes! \n# Try combinations of alpha=0.5, direction =1, and various different color palette  \n\n#~~ C) User defined color palette using cetcolor package\nmypal2 = cetcolor::cet_pal(20, name = \"r2\")   \nunikn::seecol(mypal2)  \n\n\n\n# Or reverse color pal \nmypal2 = rev(cetcolor::cet_pal(20, name = \"r2\") )  \nunikn::seecol(mypal2) \n\n\n\n\n\n\n3.2.3 Customizing terra plot options\nThere is a long list of customization operations available while plotting rasters in R.\nWe will start with basic plot from terra, and then explore the function by changing different customization options , such as: Try horizontal=TRUE, interpolate=FALSE, change xlim=c(-180, 180) with asp=1, or try legend.shrink=0.4.\n\nsm=rast(\"./SampleData-master/raster_files/SMAP_SM.tif\") # SMAP soil moisture data\n\nterra::plot(sm,\n            main = \"Scientific Plot of Raster\",\n            \n            #Color options\n            col = mypal2,                    # User Defined Color Palette\n            breaks = seq(0, 1, by=0.1),      # Sequence from 0-1 with 0.1 increment\n            colNA = \"lightgray\",             # Color of cells with NA values\n            \n            # Axis options      \n            axes=TRUE,                       # Plot axes: TRUE/ FALSE\n            xlim=c(-180, 180),               # X-axis limit\n            ylim=c(-90, 90),                 # Y-axis limit\n            xlab=\"Longitude\",                # X-axis label\n            ylab=\"Latitue\",                  # Y-axis label\n            \n            # Legend options      \n            legend=TRUE,                     # Plot legend: TRUE/ FALSE\n            \n            # Miscellaneous\n            mar = c(3.1, 3.1, 2.1, 7.1),     # Margins\n            grid = FALSE                     # Add grid lines\n        )\n\n\n\n\n\n\n3.2.4 Spatial plotting with tmap\n\nlibrary(tmap)\n# Set tmap mode: Static plot=\"plot\", Interactive plots=\"view\"\ntmap_mode(\"plot\")          \n\ntmap_SM = tm_shape(sm)+\n  tm_grid(alpha = 0.2)+                             # Transparency of grid\n  tm_raster(alpha = 0.7,                            # Transparency of raster plot\n            palette = mypal2,                       # Color pellete\n            style = \"pretty\",                       # Select style\n            title = \"Volumetric Soil Moisture\")+    # Plot main title\n  tm_layout(legend.position = \n              c(\"left\", \"bottom\"))+                 # Placement of legend\n  tm_xlab(\"Longitude\")+                             # x-lab\n  tm_ylab(\"Latitude\")                               # y-lab \n\ntmap_SM\n\n\n\n\n\n\n3.2.5 Interactive raster visualization aster data with mapview\nFunctionality of terra is largely similar to the legacy package raster (created by the same developer, Robert Hijmans). The development of terra is inspired by computational efficiency in geospatial operations. However, since terra is relatively new, and is continually developed, several other packages require conversion of the SpatRasters to rasterLayer for backwards compatibility.\nTo convert a SpatRaster to RasterLayer, use: sm2=as(sm, \"Raster\")\n\nlibrary(mapview)\nlibrary(raster)\n\n# Convert SpatRaster to raster (from package raster)\nsm2=as(sm, \"Raster\")\nmapview(sm2,                  # RasterLayer\n        col.regions = mypal2, # Color palette \n        at=seq(0, 0.8, 0.1)   # Breaks\n)\n\n\n\n\n\n\n\n3.2.6 Plotting raster data using tidyterra\ntidyterra is a package that add common methods from the tidyverse for SpatRaster and SpatVectors objects created with the terra package. It also adds specific geom_spat*() functions for plotting rasters with ggplot2.\nNote on Performance: tidyterra is conceived as a user-friendly wrapper of terra using the tidyverse} methods and verbs. This approach therefore has a cost in terms of performance.\n\nlibrary(tidyterra) \nlibrary(ggplot2)  \nggplot() +   \n  geom_spatraster(data = sm) +   \n  scale_fill_gradientn(colors=mypal2,                 # Use user-defined colormap\n         name = \"SM\",                                 #  Name of the colorbar\n         na.value = \"transparent\",                    # transparent NA cells  \n         labels=(c(\"0\", \"0.2\", \"0.4\", \"0.6\", \"0.8\")), # Labels of colorbar\n         breaks=seq(0,0.8,by=0.2),                    # Set breaks of colorbar\n         limits=c(0,0.8))+   \n    theme_void()  # Try other themes: theme_bw(), theme_gray(), theme_minimal() \n\n\n\n\nWhat if we are interested in a particular region and not the entire globe? We can plot the map for a specific extent (CONUS, in this case) by changing the range of coord_sfoption. We will also use a different theme: theme_bw. Try xlim = c(114,153) and ylim = c(-43,-11)! We will also add state boundaries and coastline to the plot.\n\nsm_conus= ggplot() +\n  geom_spatraster(data = sm) +\n  scale_fill_gradientn(colors=mypal2,                               # User-defined colormap\n                       name = \"SM\",                                 # Name of the colorbar\n                       na.value = \"transparent\",                    # transparent NA cells\n                       labels=(c(\"0\", \"0.2\", \"0.4\", \"0.6\", \"0.8\")), # Labels of colorbar\n                       breaks=seq(0,0.8,by=0.2),                    # Set breaks of colorbar\n                       limits=c(0,0.8)) +\n  coord_sf(xlim = c(-125,-67),                                      # Add extent for CONUS\n           ylim = c(24,50))+               \n  borders(\"world\",                              # Add global landmass boundaries\n          colour=\"gray43\",                      # Fill light-gray color to the landmass\n          fill=\"transparent\")+                  # Transparent background\n  borders(\"state\",                              # Add US state borders\n          colour=\"gray43\",                      # Use light-gray color\n          fill=\"transparent\")+                  # Use transparent background  \n  theme_bw()                                    # Black & white theme \n\nprint(sm_conus)\n\n\n\n\n\n\n3.2.7 Plotting vector data\nImporting and plotting shapefiles is equally easy in R. We will import the shapefile of the updated global IPCC climate reference regions and global coastlineas Simple Feature (sf) Object. Terra can also be used to import shapefiles as vectors using the function vect. However, sf is more versatile, especially for shapefiles. Notice how the attributes of the sf objects resemble an Excel data sheet.\n\nlibrary(sf)  \nlibrary(terra)\n\n##~~~~ Use sf package for shapefile \n# Import the shapefile of global IPCC climate reference regions (only for land) \nIPCC_shp = read_sf(\"./SampleData-master/CMIP_land/CMIP_land.shp\")\n# View attribute table of the shapefile\nprint(IPCC_shp) # Notice the attributes look like a data frame\n\nSimple feature collection with 41 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -168 ymin: -56 xmax: 180 ymax: 85\nGeodetic CRS:  WGS 84\n# A tibble: 41 × 5\n   V1              V2                V3       V4                        geometry\n   &lt;chr&gt;           &lt;chr&gt;             &lt;chr&gt; &lt;dbl&gt;                   &lt;POLYGON [°]&gt;\n 1 ARCTIC          Greenland/Iceland GIC       1 ((-10 58, -10.43956 58, -10.87…\n 2 NORTH-AMERICA   N.E.Canada        NEC       2 ((-55 50, -55.4386 50, -55.877…\n 3 NORTH-AMERICA   C.North-America   CNA       3 ((-90 50, -90 49.5614, -90 49.…\n 4 NORTH-AMERICA   E.North-America   ENA       4 ((-70 25, -70.43478 25, -70.86…\n 5 NORTH-AMERICA   N.W.North-America NWN       5 ((-105 50, -105.4386 50, -105.…\n 6 NORTH-AMERICA   W.North-America   WNA       6 ((-130 50, -129.5614 50, -129.…\n 7 CENTRAL-AMERICA N.Central-America NCA       7 ((-90 25, -90.37179 24.76923, …\n 8 CENTRAL-AMERICA S.Central-America SCA       8 ((-75 12, -75.28 11.67333, -75…\n 9 CENTRAL-AMERICA Caribbean         CAR       9 ((-75 12, -75.32609 12.28261, …\n10 SOUTH-AMERICA   N.W.South-America NWS      10 ((-75 12, -74.57143 12, -74.14…\n# ℹ 31 more rows\n\n##~~~~ Use terra package for shapefile \nIPCC_shp = vect(\"./SampleData-master/CMIP_land/CMIP_land.shp\") # Reference regions\nprint(IPCC_shp)\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 41, 4  (geometries, attributes)\n extent      : -168, 180, -56, 85  (xmin, xmax, ymin, ymax)\n source      : CMIP_land.shp\n coord. ref. : lon/lat WGS 84 (EPSG:4326) \n names       :            V1                V2    V3    V4\n type        :         &lt;chr&gt;             &lt;chr&gt; &lt;chr&gt; &lt;int&gt;\n values      :        ARCTIC Greenland/Iceland   GIC     1\n               NORTH-AMERICA        N.E.Canada   NEC     2\n               NORTH-AMERICA   C.North-America   CNA     3\n\ndim(IPCC_shp) # Notice the dimensions of the shapefile\n\n[1] 41  4\n\n# Load global coastline shapefile \ncoastlines = vect(\"./SampleData-master/ne_10m_coastline/ne_10m_coastline.shp\")\nprint(coastlines)\n\n class       : SpatVector \n geometry    : lines \n dimensions  : 4132, 2  (geometries, attributes)\n extent      : -180, 180, -85.22194, 83.6341  (xmin, xmax, ymin, ymax)\n source      : ne_10m_coastline.shp\n coord. ref. : lon/lat WGS 84 (EPSG:4326) \n names       : scalerank featurecla\n type        :     &lt;num&gt;      &lt;chr&gt;\n values      :         6  Coastline\n                       6  Coastline\n                       6  Coastline\n\n# Notice the difference in the geometry of coastlines and IPCC_shp\n\n\n# Alternatively, download global coastlines from the web \n# NOTE: May not work if the online server is down\n# download.file(\"https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/110m/physical/ne_110m_coastline.zip?version=4.0.1\",destfile = 'ne_110m_coastline.zip')\n# # Unzip the downloaded file\n# unzip(zipfile = \"ne_110m_coastline.zip\",exdir = 'ne-coastlines-110m')\n\n# Interactive polygon map with mapview\n\nlibrary(mapview)\nmapview(IPCC_shp) # Scroll over the region of interest and find the attributes for the region\n\n\n\n\n\n\nSubsetting vector data is similar to selecting a row from a data frame.\n\nENA_poly=IPCC_shp[4,] # Subset shapefile for Eastern North-America (ENA) \nENA_poly              # Polygon contents - notice it has 4 attributes\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 1, 4  (geometries, attributes)\n extent      : -90, -55, 25, 50  (xmin, xmax, ymin, ymax)\n coord. ref. : lon/lat WGS 84 (EPSG:4326) \n names       :            V1              V2    V3    V4\n type        :         &lt;chr&gt;           &lt;chr&gt; &lt;chr&gt; &lt;int&gt;\n values      : NORTH-AMERICA E.North-America   ENA     4\n\n# Plot ENA polygon using terra\nterra::plot(ENA_poly, main=\"Polygon for Eastern North-America\")  \n\n\n\n\nLet us see how to combine rasters with overlapping vectors with terra . Remember to execute the lines with add=TRUE with the base plot.\n\nterra::plot(sm, col=mypal2)\nterra::plot(IPCC_shp, add=TRUE, col=\"transparent\", border = \"black\")\nterra::plot(coastlines, add=TRUE, col=\"transparent\", border = \"black\")\n\n\n\n\nLets mark the location of Baton Rouge on this map. We will first make a base plot, over which coastlines and spatial location will be added. Again, remember to run these lines together.\n\n#~~~ Add spatial point to shapefile/ raster\n#~~ Make base map\nterra::plot(IPCC_shp[c(3,4,6,7),], # IPCC land regions for Contiguous US.\n             col = \"lightgray\",     # Background color\n             border = \"black\")      # Border color\n\n#~~ Add coastline\nterra::plot(coastlines, col= \"maroon\", add=TRUE)  \n\n#~~ Add spatial point to the plot\nLong=-91.0; Lat=30.62                  # Lat- Long of Baton Rouge, LA\npoints(cbind(Long,Lat),                # Lat-Long as Spatial Points\n       col=\"blue\", pch=16, cex=1.2)    # Shape, size and color of point\n\n\n\n\n\n\n3.2.8 Reprojection of rasters using terra::project\nA coordinate reference system (CRS) is used to relate locations on Earth (which is a 3-D spheroid) to a 2-D projected map using coordinates (for example latitude and longitude). Projected CRSs are usually expressed in Easting and Northing (x and y) values corresponding to long-lat values in Geographic CRS.\nA good description of coordinate reference systems and their importance can be found here:\n\nhttps://docs.qgis.org/3.4/en/docs/gentle_gis_introduction/coordinate_reference_systems.html\nhttps://datacarpentry.org/organization-geospatial/03-crs/\n\n\n\n\n\n\nIn R, the coordinate reference systems or CRS are commonly specified in EPSG (European Petroleum Survey Group) or PROJ4 format (See: https://epsg.io/). Few commonly used projection systems and their codes are summarized below:\n\n\n\n\n\n\n\n\n\nProjection system\nPROJ.4 code\nEPSG code\n\n\nNAD83 (North American Datum 1983)\n“+proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs +type=crs”\nEPSG:4269\n\n\nMercator\n“+proj=merc +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +R=6371000 +units=m +no_defs +type=crs”\nESRI:53004\n\n\nWGS 84 (World Geodetic System 1984)\n“+proj=merc +lon_0=0 +k=1 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs +type=crs”\nEPSG:3395\n\n\nWGS 84 / Pseudo-Mercator – Spherical Mercator ( used in Google Maps, OpenStreetMap)\n“+proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null +wktext +no_defs +type=crs” |\nEPSG:3857\n\n\nRobinson (better for plotting global data due to minimal distortion, except for higher latitudes)\n“+proj=robin +lon_0=0 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs +type=crs”\nESRI:54030\n\n\n\n\nThe SpatRaster reprojection process is done with project() from the terra package.\n# Importing SMAP soil moisture data\nsm=rast(\"./SampleData-master/raster_files/SMAP_SM.tif\") \n\n#~~ Projection 1: NAD83 (EPSG: 4269)\n\nsm_proj1 = terra::project(sm, \"epsg:4269\")\n\nterra::plot(sm_proj1, \n             main = \"NAD83\",    # Title of the plot\n             col = mypal2,      # Colormap for the plot\n             axes = FALSE,      # Disable axes\n             box = FALSE,       # Disable box around the plots\n             asp = NA,          # No fixed aspect ratio; asp=NA fills plot to window\n             legend=FALSE)      # Disable legend\n\n#~~ Projection 2: World Robinson projection (ESRI:54030)\n\nsm_proj2 = terra::project(sm, \"ESRI:54030\")\n\nterra::plot(sm_proj2, \n             main = \"Robinson\", # Title of the plot\n             col = mypal2,      # Colormap for the plot\n             axes = FALSE,      # Disable axes\n             box = FALSE,       # Disable box around the plots\n             asp = NA,          # No fixed aspect ratio; asp=NA fills plot to window\n             legend=FALSE)      # Disable legend\n\nLet us now plot a map of global surface soil moisture, reprojected to Robinson projection. Notice that to add the raster and vector to the plot, we use the functions geom_spatraster and geom_spatvector respectively. Another package spData provides several important datasets for global mapping, including, US states polygons (us_states), World country polygons (world), global elevation (elev.tif), among others (https://jakubnowosad.com/spData/). We will use world country polygons (world) in our map.\n\nlibrary(spData)\n# Import global political boundaries from spData package\nWorldSHP=terra::vect(spData::world)             \n\n# Generate plot\nRobinsonPlot &lt;- ggplot() +\n  geom_spatraster(data = sm)+                   # Plot SpatRaster layer               \n  geom_spatvector(data = WorldSHP, \n                  fill = \"transparent\") +       # Add world political map\n  ggtitle(\"Robinson Projection\") +              # Add title\n  scale_fill_gradientn(colors=mypal2,           # Use user-defined colormap\n                       name = \"Soil Moisture\",  # Name of the colorbar\n                       na.value = \"transparent\",# Set color for NA values\n                       lim=c(0,0.8))+           # Z axis limit\n  theme_minimal()+                              # Select theme. Try 'theme_void'\n  theme(plot.title = element_text(hjust =0.5),  # Place title in the middle of the plot\n        text = element_text(size = 12))+        # Adjust plot text size for visibility\n  coord_sf(crs = \"ESRI:54030\",                  # Reproject to World Robinson\n           xlim = c(-152,152)*100000,           # Convert x-y limits from decimal Deg. to meter\n           ylim = c(-55,90)*100000)\n\nprint(RobinsonPlot)\n\n\n\n# Save high resolution plot to disk\n  # ggsave(\n  #   \"globalSM.png\",          # Name of the file to be created\n  #   plot = RobinsonPlot,     # Plot to be exported\n  #   bg = \"white\",            # Plot background\n  #   height = 6,              # Height \n  #   width = 10,              # Width  \n  #   units = c(\"in\")          # Units (\"in\", \"cm\", \"mm\" or \"px\")\n  # )\n\nRun the commented script above, and view the exported plot saved on the disk.\n\n\n3.2.9 Reclassify and reproject `SpatRasters`\nSo far we have used continuous color scales for plotting rasters. Now we will reclassify the raster in discrete domains based on the cell values. This operation is called RATifying the raster i.e. adding the Raster Attribute Table (RAT) to the file. This allows use of discrete color scales for plotting.\n\nlibrary(spData)\n\n# Import global political boundaries from spData package\nWorldSHP=terra::vect(spData::world)    \n\n# Import raster file \nsm=rast(\"./SampleData-master/raster_files/SMAP_SM.tif\") \n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  # Defite category breaks for the raster\n  brk= seq(0,0.8, by=0.1)\n\n  # Add attribute table to the raster\n  library(dplyr)\n  sm_rat=dplyr::mutate(sm,                   # input raster\n               rat = cut(SMAP_SM,            # Raster attribute\n               breaks = seq(0,0.8, by=0.1),  # Breaks for each class\n               labels = levels(cut(values(sm$SMAP_SM), breaks=brk)) # Labels for each class\n  ))\n\n  # levels(cut(values(sm$SMAP_SM), breaks=brk)) is same as typing\n  #   c(\"(0,0.1]\", \"(0.1,0.2]\", \"(0.2,0.3]\", \n  #   \"(0.3,0.4]\", \"(0.4,0.5]\", \"(0.5,0.6]\", \n  #   \"(0.6,0.7]\", \"(0.7,0.8]\")\n  \n  plot(sm_rat$rat) # Simple plot with ratified values\n\n\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Generate global plot in Robinson projection for the RATifies raster\n  \nRobinsonPlot_cat &lt;- ggplot() +\n  geom_spatraster(data = sm_rat,    # Plot SpatRaster layer \n                  aes(fill = rat))+ # aes = the attribute to use for plotting\n  geom_spatvector(data = WorldSHP,\n                  fill = \"transparent\") +       # Add world political map\n  ggtitle(\"Robinson Projection\") +              # Add title\n  scale_fill_manual(values = brewer.pal(length(brk)-1,\"Spectral\"), # Discrete palette \n                    na.value=\"transparent\",      # Set transparent fill values for NA cells\n                    name=\"SM\",                   # Name of the legend bar\n                    labels=levels(cut(values(sm$SMAP_SM), breaks=brk)))+\n  theme_minimal()+                              # Select theme. Try 'theme_void'\n  theme(plot.title = element_text(hjust =0.5),  # Place title in the middle of the plot\n        text = element_text(size = 12))+        # Adjust plot text size for visibility\n  coord_sf(crs = \"ESRI:54030\",                  # Reproject to World Robinson\n           xlim = c(-152,152)*100000,           # Convert x-y limits from decimal Deg. to meter\n           ylim = c(-55,90)*100000)\n\nprint(RobinsonPlot_cat)\n\n\n\n\nOther resources:\n\nMore excellent examples on making maps in R can be found here: https://bookdown.org/nicohahn/making_maps_with_r5/docs/introduction.html.\nQuintessential resource for reference on charts and plots in R: https://www.r-graph-gallery.com/index.html.\n\n\n\n\n\nEntekhabi, Dara, Eni Njoku, and Peggy O’Neill. 2009. “The Soil Moisture Active and Passive Mission (SMAP): Science and Applications.” 2009 IEEE Radar Conference. https://doi.org/10.1109/radar.2009.4977030.\n\n\nHuete, Alfredo, Chris Justice, and Wim Van Leeuwen. 1999. “MODIS Vegetation Index (MOD13).” Algorithm Theoretical Basis Document 3 (213): 295–309.\n\n\nIturbide, Maialen, José M. Gutiérrez, Lincoln M. Alves, Joaquín Bedia, Ruth Cerezo-Mota, Ezequiel Cimadevilla, Antonio S. Cofiño, et al. 2020. “An Update of IPCC Climate Reference Regions for Subcontinental Analysis of Climate Model Data: Definition and Aggregated Datasets.” Earth System Science Data 12 (4): 2959–70. https://doi.org/10.5194/essd-12-2959-2020.\n\n\nZomer, Robert J, Jianchu Xu, and Antonio Trabucco. 2022. “Version 3 of the Global Aridity Index and Potential Evapotranspiration Database.” Scientific Data 9 (1): 409."
  },
  {
    "objectID": "ch3.html#raster-arithmetic-operations",
    "href": "ch3.html#raster-arithmetic-operations",
    "title": "4  Raster Arithmetic and Statistics",
    "section": "4.1 Raster Arithmetic Operations",
    "text": "4.1 Raster Arithmetic Operations\nArithmetic, comparison and logical operations on SpatRasters use the same operators as those used for simple vector-like operations. These operators are listed below:\na) Arithmetic operators\n\n\n\n\n\nb) Comparison operators\n\n\n\n\n\nc) Logical operators\n\n\n\n\n\n\n\n4.1.1 Single Raster Operations\nThe application of these operators on rasters can be conceptualized as element-wise operations on a matrix. For example, assume A is a raster with values shown in the matrix form as below:\n\nThen, A + 1 will be:\n\nNote how each element of the matrix gets an addition of 1. Similarly, for A x 2, each element of A will be multiplied by 2 as:\n\nand, A ^ 2 will be given as:\n\nLogical operations can also be carried out in a similar fashion. For example, if we were to test if the cell values of A are greater than 3 (i.e. A &gt; 3), it will be written as:\n\nNow let us try a few examples of arithmetic, comparison and logical operations on SpatRasters. Before we begin this chapter, let us ensure that we have the sample dataset necessary for the analysis, otherwise, download the files from the GitHub repository using the following codes:\n\n#~~~ Check if sample data exists on disk. If not, download from GitHub repository\n\nif (dir.exists(\"SampleData-master\")==FALSE){ \n  \ndownload.file(url = \"https://github.com/Vinit-Sehgal/SampleData/archive/master.zip\",\ndestfile = \"SampleData-master.zip\")    \n\n# Unzip the downloaded .zip file\nunzip(zipfile = \"SampleData-master.zip\")\nlist.files(\"./SampleData-master\")   # List folder contents\n\n}\n\nTaking example of SMAP soil moisture raster let’s practice the application of arithmetic operations on SpatRasters. We start by importing the SpatRaster in the current R environment.\n\nlibrary(terra) # Import library\n\n# Import SMAP soil moisture raster from the downloaded folder\nsm=terra::rast(\"./SampleData-master/raster_files/SMAP_SM.tif\")\nprint(sm)\n\nclass       : SpatRaster \ndimensions  : 456, 964, 1  (nrow, ncol, nlyr)\nresolution  : 0.373444, 0.373444  (x, y)\nextent      : -180, 180, -85.24595, 85.0445  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : SMAP_SM.tif \nname        :    SMAP_SM \nmin value   : 0.01999998 \nmax value   : 0.87667608 \n\n# Add 1 to raster values\nsm2=sm+1\nprint(sm2) #Notice the max and min values have increased by 1. \n\nclass       : SpatRaster \ndimensions  : 456, 964, 1  (nrow, ncol, nlyr)\nresolution  : 0.373444, 0.373444  (x, y)\nextent      : -180, 180, -85.24595, 85.0445  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource(s)   : memory\nvarname     : SMAP_SM \nname        :  SMAP_SM \nmin value   : 1.020000 \nmax value   : 1.876676 \n\n# Multiply raster values by 2\nsm2=sm*2\nplot(sm2, main= \"sm*2\") # Try sm2=sm*10, or sm2=sm^2 and see the difference in sm2 values\n\n\n\n\nSimilarly, logical operations can also be carried out on rasters with ease in R. Logical operators can be a convenient tool for raster data manipulation. For example, any subset of values can be manipulated based on a user-defined logical criteria.\n\n# Are cell values of sm raster greater than 0.3?\nsm=terra::rast(\"./SampleData-master/raster_files/SMAP_SM.tif\")\nsm2=sm&gt;0.3    \nplot(sm2, main=\"is cell value &gt;0.3\")    # Notice that the raster has only True and False values\n\n\n\n# Replace all values of sm&gt;0.3 with 0.5\nsm=terra::rast(\"./SampleData-master/raster_files/SMAP_SM.tif\")\nsm[sm&gt;0.3]=0.5 \nplot(sm, main=\"replace sm&gt;0.3 with 0.5\")\n\n\n\n# Or replace all values of sm&gt;0.3 with NA \nsm=terra::rast(\"./SampleData-master/raster_files/SMAP_SM.tif\")\nsm[sm&gt;0.3]=NA \nplot(sm, main=\"replace sm&gt;0.3 with NA\")\n\n\n\n# Reaplace all NA values with 0\nsm=terra::rast(\"./SampleData-master/raster_files/SMAP_SM.tif\")\nsm[is.na(sm)]=0\nplot(sm)\n\n\n\n\nSatellites record observations in regularly spaced swaths as they orbit around the globe. In the case of SMAP satellite, it takes 2-3 days to cover the Earth. So far, we used a pre-processed soil moisture raster that provided 3-day averaged soil moisture using rasters from individual days.\nNow, let us evaluate daily raster available from SMAP (pixel recorded at 6 AM local time). Notice that this raster is patchy and uses a fill value (values assigned to missing observations) of -9999. Replacing these fill values with NA prior to statistical analysis are highly useful.\n\n# Reaplace fill values with NA\nsm=rast(\"./SampleData-master/raster_files/SMAP_L3_SM_P_20150401_R17000_002_Soil_Moisture_Retrieval_Data_AM_soil_moisture_867a5e36.tif\")\n\nsmFilter=sm                   # Copy original raster before manipulation\nsmFilter[smFilter==-9999]=NA  # Replace negative values with -9999\n\n# Plot original and manipulated raster\nlayout(matrix(1:2, ncol = 2))   # Specify layout for plots\nplot(sm, main= \"raw raster with fill values\")\nplot(smFilter, main=\"Post filtering -9999\")\n\n\n\n\n\n\n\n4.1.2 Multi-Raster Operations\nSo far we saw examples of arithmetic/ logical operations on a single raster. The tenets of arithmetic operations discussed earlier are also applicable for operations on two (or more) rasters. These operations are carried out cell-wise between the corresponding pixels of the raster layers. Suppose, another raster B is given as:\n\nthen, A+B will be:\n\nSimilarly, AxB will also be a cell-wise multiplication of respective data values:\n\nFor this illustration, we will use rootzone (0-100 cm depth) soil moisture from NASA’s Catchment Land Surface Model (provided as SMAP science Level 4 product) for Contiguous U.S. over a period of three consecutive days i.e. December 13-15, 2021.\n\nSMday1=rast(\"./SampleData-master/SMAPL4_rasters/SMAP_L4_SM_gph_20211213T103000_Vv6030_001_Geophysical_Data_sm_rootzone_2052db45.tif\")\n\nSMday2=rast(\"./SampleData-master/SMAPL4_rasters/SMAP_L4_SM_gph_20211214T103000_Vv6030_001_Geophysical_Data_sm_rootzone_22156243.tif\")\n\nSMday3=rast(\"./SampleData-master/SMAPL4_rasters/SMAP_L4_SM_gph_20211215T103000_Vv6030_001_Geophysical_Data_sm_rootzone_23bd7fe2.tif\")\n\nlayout(matrix(1:3, ncol = 1))                # Specify layout\nplot(SMday1, main=\"Rootzone SM day1\")\nplot(SMday2, main=\"Rootzone SM day2\")\nplot(SMday3, main=\"Rootzone SM day3\")\n\n\n\n\nNote that we can not make out the differences in the rasters as the colors are skewed by the high negative fill value for missing data in the rasters. So, we first remove the fill values (i.e. cells valued at -9999) with NA.\n\nSMday1[SMday1==-9999]=NA  # Replace negative values with -9999\nSMday2[SMday2==-9999]=NA  # Replace negative values with -9999\nSMday3[SMday3==-9999]=NA  # Replace negative values with -9999\n\nlayout(matrix(1:3, ncol = 1))                # Specify layout\nplot(SMday1, main=\"SM day1, filtered\")\nplot(SMday2, main=\"SM day2, filtered\")\nplot(SMday3, main=\"SM day3, filtered\")\n\n\n\n\nNow, let’s calculate the change in soil moisture values between days 1-2 and 1-3. Can we identify a region which showed anomalous wetting during the three days of observation?\n\ndelta12=SMday1-SMday2\ndelta13=SMday1-SMday3\n\nlayout(matrix(1:2, ncol = 1))                # Specify layout\nplot(delta12, main=\"SM difference (day 1 & 2\")\nplot(delta13, main=\"SM difference (day 1 & 3\")\n\n\n\n\nCurious to know why Southern California showed such high levels of wetting? Check out: ’Storm of the season’ dumps record-breaking rainfall on SoCal and snow in the mountains - Los Angeles Times (latimes.com)."
  },
  {
    "objectID": "ch3.html#raster-statistics",
    "href": "ch3.html#raster-statistics",
    "title": "4  Raster Arithmetic and Statistics",
    "section": "4.2 Raster Statistics",
    "text": "4.2 Raster Statistics\nWe are all familiar with common statistical functions available in base R for summarizing arrays, such as: min, max, range, sum, stdev, median, mean, modal. These functions are applied using the global function, which calculates the “global” statistics based on the values of all cells within a raster layer. Global is a versatile function and can also be used to apply more complex user-defined operations on rasters. Let us now look at some examples:\n\nlibrary(terra) \n\n# Import SMAP soil moisture raster from the downloaded folder\nsm=terra::rast(\"./SampleData-master/raster_files/SMAP_SM.tif\")\n\n# Summary statistics\nglobal(sm, mean, na.rm = T)  # Calculate mean, while ignoring NA values\n\n            mean\nSMAP_SM 0.209402\n\n# This is equivalent to: \nsm_val=values(sm)       # Create array of cell values of raster\nmean(sm_val, na.rm = T) # Calculate mean of cell values\n\n[1] 0.209402\n\n\nUse of global function is equivalent to applying a function on all cell values\n\nglobal(sm, sd, na.rm = T) # Calculate standard deviation, while ignoring NA values\n\n               sd\nSMAP_SM 0.1434444\n\n# OR\nsd(values(sm), na.rm = T) # Calculate mean of cell values\n\n[1] 0.1434444\n\n# Similarly, for finding quantiles of a raster layer\nglobal(sm, quantile, probs = c(0.25, 0.75), na.rm = T) # Calculate 25th and 75th percentiles of the ratser layer\n\n              X25.      X75.\nSMAP_SM 0.09521707 0.2922016\n\n# OR\nquantile(values(sm), probs = c(0.25, 0.75), na.rm = T) \n\n       25%        75% \n0.09521707 0.29220164 \n\n\nWe can also generate common summary statistics plots using functions such as hist (histogram), barplot (box and whisker plot), etc.\n\nsm=terra::rast(\"./SampleData-master/raster_files/SMAP_SM.tif\")\n\n# Plot summary using standard statistical functions\nlayout(matrix(1:2, ncol = 2))                # Specify layout\nhist(sm)\nboxplot(sm)\n\n\n\n\n\n4.2.1 User-defined Functions for Raster Statistics\nUser-defined functions can also be applied to raster layer using the global function for specific operations. A user can define their own function with specific instructions which will be executed everytime the function is called. A simple template of a typical R function is given below:\n\nLet us revisit the application of the quantile function by passing it through a user-defined function with global. In the quant_fun shown below function, myRas is the raster whose values are being summarized. Notice how instruction for na.rm is passed through the ignoreNA argument.\n\n# User-defined statistics by defining own function.   \nquant_fun = function(myRas, ignoreNA=TRUE){      \n  p=quantile(myRas, probs = c(0.25, 0.75), na.rm=ignoreNA)\n  return(p)\n} \n\nglobal(sm, quant_fun)   # 25th, and 75th percentile of each layer\n\n              X25.      X75.\nSMAP_SM 0.09521707 0.2922016\n\n# This is equivalent to:\nquant_fun(values(sm))\n\n       25%        75% \n0.09521707 0.29220164"
  },
  {
    "objectID": "ch5.html#meet-the-data",
    "href": "ch5.html#meet-the-data",
    "title": "5  Spatial Operations on Rasters and Vectors",
    "section": "5.1 Meet the Data",
    "text": "5.1 Meet the Data\nFor this chapter, we will use Aridity index (AI), which is a popular metric for climate classification based on the relative availability of Mean Annual Precipitation (P) compared to the Mean Annual Reference Evapotranspiration (ET0) of a location. Aridity Index is defined as: \\(AI= P / ET_0\\). Here, ET0 is the maximum potential amount of moisture a hydrologic system can transfer to atmosphere through evaporation and transpiration. The value of \\(P / ET_0\\) progressively increases from arid to humid regions. Since P and ET0 can never be negative, AI is always &gt;0.\n\n\n\nGlobal Climate Classification Map Based on Aridity Index\n\n\nFor the purpose of this demonstration, we will use CONUS-wide raster of aridity index estimates (i.e. P/ET0) provided by (Zomer, Xu, and Trabucco 2022), at ~0.8X0.8 KM spatial resolution. Let us first start by downloading the sample data files (refer to previous chapters for the code). We will then import the raster and plot a histogram of the raster to understand the probability distribution of the AI values. Do we agree that most pixels in the raster range within 0-4?\n\nlibrary(terra)\nAI=terra::rast(\"./SampleData-master/raster_files/P_over_ET0.tif\") \n\n# Let us plot a histogram of the raster\nhist(AI, breaks=20, \n     xlim=c(0,5), \n     xlab=\"AI=P/ET0\", \n     ylab=\"Pixels\", main=\"\") \n\n\n\n# We observe most values in the raster are &lt;4. So, we set the range of the plot accordingly\nterra::plot(AI, main=\"AI=P/ET0\", range=c(0,4))"
  },
  {
    "objectID": "ch5.html#raster-resampling",
    "href": "ch5.html#raster-resampling",
    "title": "5  Spatial Operations on Rasters and Vectors",
    "section": "5.2 Raster Resampling",
    "text": "5.2 Raster Resampling\nWe have so far worked with global surface soil moisture raster (from SMAP) and we are familiar with its spatial distribution across the globe. Now, let us consider this question:\n\nHow does the climate impact the spatial distribution of soil moisture?\n\nOne approach of answering the question can be to compare pixel values of SMAP soil moisture with corresponding values of AI to establish the bivariate AI–SM relationship. So, we must first change the resolution of AI raster to match that of the SMAP soil moisture. For this, we will use raster resampling.\nRaster resampling simply refers to making changes to the pixel resolution of the raster. The term “resampling” implies that the pixel values are “sampled” and reassigned to the pixels at the new resolution. This operations often involves an interpolation method (nearest neighbor, bilinear, spline, min, max, mode, average etc). We will try three important functions for changing the resolution of a SpatRaster:\n\nterra::resample - resample to match the resolution of another raster\nterra::aggregate - resample from fine to coarse resolution\nterra::disagg - resample from coarse to fine resolution\n\nRaster resampling can be a critical step during multivariate analysis, where raster pixels must overlap to ensure the datasets from each raster corresponds to the same spatial domain. The following schematic helps illustrate the use of these functions:\n\n\n\n5.2.1 Why Resample?\nTo explore the AI–SM relationship, first, we will resample the pixels in the AI raster to match the spatial resolution of sm pixels using the terra::resample function with bilinear interpolation method. This method estimates new cell values as a weighted-average values of the adjoining pixels. The weights are calculated according to the distance of the target pixel from the adjoining cells. In addition to the bilinear approach, terra::resample has several other interpolation methods available as options such as near (for nearest neighbor interpolation), cubic, sum, min, max, average , rms (root-mean square) etc. For more information on resampling methods within the context of remote sensing, please refer to (Schowengerdt 2007).\nOnce AI raster is resamapled to match sm, we would then be able to generate scatter plots between the two rasters, and evaluate the relationship between aridity index (climate) and soil moisture. We see that the soil moisture increases as AI increases before reaching an asymptotic value as AI&gt;1 (humid climate, indicated by red vertical line in the plot). This relationship follows the famous Budyko formulation of energy and water limits on terrestrial water balance (Chen and Sivapalan 2020). For illustration, we use a simple formulation of Budyko curve to represent the non-linear interrelationship between AI and soil moisture given as:\n\\[\nSM=AI/(1+AI)^{1.5}\n\\]\n\nsm=terra::rast(\"./SampleData-master/raster_files/SMAP_SM.tif\") \n\nAIResamp=terra::resample(AI,                # Raster to be resmapled\n                              sm,                # Target resolution raster\n                              method='bilinear') # bilinear interpolation method\n\n# Check the resolution of the aridity raster after resampling\nres(AIResamp)\n\n[1] 0.373444 0.373444\n\nplot(AIResamp,sm , \n     xlim=c(0,3), ylim=c(0,0.6),\n     xlab=\"P/ET0\", ylab=\"Soil Moisture\", \n     pch=19)\ncurve(x/((1+x)^1.5), col=\"blue\",lwd=3, add = T)\nabline(v=1, col=\"red\", lwd=3) \n\n\n\n\n\nFood for thought: Can we do this analysis had we used AI raster instead of AIResamp? What will be the output is we replace AIResamp with AI in the code above.\n\n\n\n5.2.2 Aggregation and Disaggregation\nNow that we have seen the application of terra::resample function, let us now try terra::aggregate and terra::disagg when we know the factor of (dis)aggregation to be used in each direction. Several functions are available for raster aggregation including mean, max, min, median, sum, modal, sd. Disaggregation can use either near or bilinear methods.\n\nlibrary(terra)\n\n# Import AI raster\nAI=terra::rast(\"./SampleData-master/raster_files/P_over_ET0.tif\") \n# Remove the negative fill value from AI raster\nAI[AI&lt;0]=NA\n\n# Original resolution of raster for reference\nres(AI)\n\n[1] 0.008333333 0.008333333\n\n#~~ Aggregate raster to coarser resolution\nAIcoarse = terra::aggregate(AI,           # Original AI raster\n                            fact = 100,    # Reduce the spatial dimension by a factor of 100\n                            fun = mean,   # Function used to aggregate values. We use within-pixel mean\n                            na.rm=TRUE)   # Ignore NA values\nres(AIcoarse)   # Resolution changed from 0.8KMX0.8KM  to (0.8X100KM)x(0.8X100KM) \n\n[1] 0.8333333 0.8333333\n\nplot(AIcoarse, main=\"Aggregated AI raster\")\n\n\n\n#~~ Disaggregate AI to finer resolution\nAIfine = terra::disagg(AI, \n                   fact=2,               # Reduce the spatial dimension by a factor of 2\n                   method='bilinear')    # Interpolation method \"bilinear\" or \"near\"  \n\nres(AIfine)        # Resolution changed from 0.8KM X 0.8KM  to (0.8/2KM)x(0.8/2KM) \n\n[1] 0.004166667 0.004166667\n\nplot(AIfine, main=\"Disggregated AI raster\")"
  },
  {
    "objectID": "ch5.html#cropping-and-masking",
    "href": "ch5.html#cropping-and-masking",
    "title": "5  Spatial Operations on Rasters and Vectors",
    "section": "5.3 Cropping and Masking",
    "text": "5.3 Cropping and Masking\nCropping and masking operations are used to reduct the spatial extent of a raster/vector data. We will use US states shapefile from spData::us_states to select the polygons for Louisiana and adjoining states. The resource spData::us_states provides features for the Contiguous U.S. as simple feature, i.e., sf collection. We note that the state names in the us_states multipolygon are given in the field NAME, which we will use to subset the multipolygon to the specific states we are interested in. We will take “Louisiana”, “Texas”, “Mississippi”,“Alabama”, “Oklahoma”, “Arkansas” as examples.\n\nlibrary(spData)\nlibrary(sf)\n\nAI=terra::rast(\"./SampleData-master/raster_files/P_over_ET0.tif\") \n\n# Lets view the data\nhead(spData::us_states)\n\nSimple feature collection with 6 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -114.8136 ymin: 24.55868 xmax: -71.78699 ymax: 42.04964\nGeodetic CRS:  NAD83\n  GEOID        NAME   REGION             AREA total_pop_10 total_pop_15\n1    01     Alabama    South 133709.27 [km^2]      4712651      4830620\n2    04     Arizona     West 295281.25 [km^2]      6246816      6641928\n3    08    Colorado     West 269573.06 [km^2]      4887061      5278906\n4    09 Connecticut Norteast  12976.59 [km^2]      3545837      3593222\n5    12     Florida    South 151052.01 [km^2]     18511620     19645772\n6    13     Georgia    South 152725.21 [km^2]      9468815     10006693\n                        geometry\n1 MULTIPOLYGON (((-88.20006 3...\n2 MULTIPOLYGON (((-114.7196 3...\n3 MULTIPOLYGON (((-109.0501 4...\n4 MULTIPOLYGON (((-73.48731 4...\n5 MULTIPOLYGON (((-81.81169 2...\n6 MULTIPOLYGON (((-85.60516 3...\n\n# From this we will subset the sf object for the selected states. We select forst column as we are only interested in a single attribute\nUS_shp=spData::us_states\n\n# Subset selected states from the shapefile\nsouth_sf=US_shp[US_shp$NAME %in% c(\"Louisiana\", \"Texas\", \"Mississippi\",\"Alabama\", \"Oklahoma\", \"Arkansas\"),]\n\n# Convert the sf object to SpatRaster\nsouth_vect=vect(south_sf)\nplot(south_vect)\n\n\n\n# Crop AIfine raster to south_vect extent \nAI_south=crop(AIfine, south_vect)\nplot(AI_south, main=\"Cropped AI\")\nplot(south_vect, add=T)\n\n\n\n# Mask AIfine raster to match south_vect spatial domain \nAI_south_msk=mask(AI_south, south_vect)   # Try: inverse=TRUE argument for fun\nplot(AI_south_msk, main=\"Masked AI\")\nplot(south_vect, add=T)\n\n\n\n\n\nComputing Counsel\n\nFor large rasters, it is computationally efficient to crop and/or mask rasters to the region of interest before the analysis.\ncrop first, and mask later, as masking is computationally expensive than cropping."
  },
  {
    "objectID": "ch5.html#raster-ratification",
    "href": "ch5.html#raster-ratification",
    "title": "5  Spatial Operations on Rasters and Vectors",
    "section": "5.4 Raster RATification",
    "text": "5.4 Raster RATification\nIn the AI plot above, we see the expected climate pattern for the terrestrial landmass. The eastern U.S. receives abundant precipitation, and have high aridity index values. In contrast, Southwestern US have hot and dry climate, and show low values of the aridity index. However, in common use, we are more familiar with the use of generalized climate categories, and not a numerical index. For example, its easier to understand, compare and contrast the difference between the climate of Arizona and Louisiana in terms of “arid” versus “humid”, than the respective AI values.\nSuch terms come from the United Nations Environment Program (UNEP, (Nash 1999)), that divides global climate in five discrete classes based on the aridity index as below:\n\n\n\nTable 1: Aridity Index based climate classes given by UNEP\n\n\nAn attribute table can be added to a raster which serves as a look-up reference for the discrete classification of the continuous variable in the raster. This process of adding a Raster Attribute Table to a raster is called raster RAT-ification.\nAs an example, we will follow class breaks and names given in Table 1 to add a climate attribute to the AI raster. We will cut the pixel values of the AI raster into discrete classes and add the attribute table back to the original AI raster.\n\n# Import AI raster and remove negative fill value\nAI=terra::rast(\"./SampleData-master/raster_files/P_over_ET0.tif\") \n\n# Breaks for each climate class from Table 1\nclass_brk= c(0, 0.03, 0.2, 0.5, 0.65, 10)                                \n# Labels for each climate class from Table 1\nclass_names=c(\"Hyper arid\", \"Arid\", \"Semi arid\", \n              \"Sub humid\", \"Humid\")   \n\n# Divide the cell values in the AI raster into distinct levels\nattributes=base::cut(values(AI), # Notice we apply cut on raster \"values\" \n                 breaks = class_brk,\n                 labels =class_names )\n\n# Add attributes to the SpatRaster as climate class\nAI$climate = attributes\nplot(AI$climate,  plg = list(loc = \"bottomleft\"))"
  },
  {
    "objectID": "ch5.html#zonal-statistics",
    "href": "ch5.html#zonal-statistics",
    "title": "5  Spatial Operations on Rasters and Vectors",
    "section": "5.5 Zonal Statistics",
    "text": "5.5 Zonal Statistics\nZonal statistics refers to estimate statistical measures of the cell values of a raster within the zones of another dataset (raster/vector). In the subsequent examples, we will use the aridity index raster to create a polygon, demarcating the spatial boundaries of the discrete climate zones. We will then use these polygons to extract respective cell values and calculate zonal statistics for each climate.\n\n5.5.1 Zonal Statistics with Polygons\n\n5.5.1.1 Raster to Polygons\nIn the next example, we will convert the aridity raster into a polygon based on aridity classification using terra::as.polygons function.  We will use previously RATified the aridity index raster to generate polygons for the climate zones.\n\n# Convert classified raster to shapefile\narid_poly=as.polygons(AI$climate)   # Convert SpatRaster to a spatial polygon \n\n# Notice the dimension (geometries, attributes) and values of the polygon. \n# Do the values match the classes you created?\nprint(arid_poly)\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 5, 1  (geometries, attributes)\n extent      : -124.7, -66.98333, 24.55833, 49.38333  (xmin, xmax, ymin, ymax)\n coord. ref. : lon/lat WGS 84 (EPSG:4326) \n names       :    climate\n type        :      &lt;chr&gt;\n values      : Hyper arid\n                     Arid\n                Semi arid\n\n# View polygon of the climate zones\nlibrary(mapview)\nmapview(arid_poly)\n\n\n\n\n\n# You can also crop SpatVectors\nAI_poly_south=terra::crop(arid_poly, south_vect)\nmapview(AI_poly_south)\n\n\n\n\n\n\n\n\n5.5.1.2 Zonal Data Extraction and Statistics\nNow we will use terra::extract function to extract the cell values of the soil moisture raster, sm, within each climate zone. We can then use tapply or the built-in fun option within terra::extract to calculate zonal statistics.\n\nsm=terra::rast(\"./SampleData-master/raster_files/SMAP_SM.tif\")\n\n# Extract cell values for each climate class\nsm_climate_df = terra::extract(sm,      # Raster to be summarized\n                          arid_poly,    # Shapefile/ polygon to summarize the raster\n                          na.rm=TRUE)   # Ignore NA values? yes! \nhead(sm_climate_df)\n\n  ID    SMAP_SM\n1  1 0.06480984\n2  1 0.08408742\n3  1 0.05854325\n4  1 0.05259301\n5  1 0.04967655\n6  1 0.04935537\n\n# Calculate group-wise AI \ntapply(sm_climate_df$SMAP_SM,     # Column to be summarized\n       sm_climate_df$ID,          # Grouping variable\n       median,          # Function to use. User defined functions can be used too\n       na.rm = TRUE)              # Ignore NA values? yes! \n\n         1          2          3          4          5 \n0.07296192 0.06936086 0.14301015 0.21182603 0.32055609 \n\n# OR: We can specify the \"fun\" argument within terra::extract function\nsm_climate_median=terra::extract(sm,    # Raster to be summarized\n                          arid_poly,    # Shapefile/ polygon to summarize the raster\n                          fun=median,   # Statistic needed: median/mean/sum/min/max?\n                          na.rm=TRUE)   # Ignore NA values? yes! \n\n# The climate-wise median values are extracted as a dataframe\nhead(sm_climate_median)\n\n  ID    SMAP_SM\n1  1 0.07296192\n2  2 0.06936086\n3  3 0.14301015\n4  4 0.21182603\n5  5 0.32055609\n\n# Lets plot the climate-wise median of surface soil moisture\nplot(sm_climate_median,     \n     xaxt = \"n\",              # Disable x-tick labels\n     xlab=\"Climate\",          # X axis label\n     ylab=\"Soil moisture\",    # Y axis label\n     type=\"b\",                # line type\n     col=\"blue\",              # Line color\n     main=\"Climate-wise median of surface soil moisture\")\naxis(1, at=1:5, labels=c(\"Hyper-arid\", \"Arid\", \"Semi-Arid\",\"Sub-humid\",\"Humid\"))\n\n\n\n\nLet us revisit our question:\n\nHow does the climate impact the spatial distribution of soil moisture?\n\nCan we make a similar conclusion as before?\n\n\n\n5.5.1.3 extract vs exact_extract\nThis is a good time to learn about another excellent function for raster cell extraction from the exactextractr package, exact_extract. This function is computationally faster and efficient than terra::extract, which will be evident when working with rasters of large size.\n\nRemember:\n\nexactextractr::exact_extract also outputs the fractional cell coverage of each pixel extracted. For some analysis (including ours), this information may not be needed.\nexact_extract works on simple feature, sf, objects. So, we will use st_as_sf to convert SpatVector objects to sf.\n\n\n\nlibrary(exactextractr)\nlibrary(sf)\n\nsm=terra::rast(\"./SampleData-master/raster_files/SMAP_SM.tif\")\n\nzonal_extract=exactextractr::exact_extract(sm,  # Raster to be summarized\n                st_as_sf(arid_poly),    # Convert shapefile to sf (simple feature)\n                force_df = TRUE,        # Output as a data.frame?\n                include_xy = FALSE,     # Include cell lat-long in output?\n                fun = NULL,             # Specify the function to apply for each feature extracted data\n                progress = TRUE)        # Progressbar\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |======================================================================| 100%\n\nzonal_data=sapply(zonal_extract,\"[[\",1)  # Select only cell values (first column within each element of the list)\n\n# Generate boxplot for the zonal data\nboxplot(zonal_data, \n        col=c(\"#800000\", \"#FF7C00\",\"#87FF78\",\"#008BFF\",\"#00008F\"), \n        names=c(\"Hyper arid\", \"Arid\", \"Semi arid\",\"Sub humid\", \"Humid\")  )\n\n\n\n\n\n\n5.5.1.4 User-defined Functions for Zonal Statistics\nCell value extraction for each zone can help a user to carry a diverse set of analysis for each region using custom functions. Once the zonal cell values are extracted, vectorized application of user-defined function can be carried out on these datasets using lapply (list+apply) or purrr::map functions.\n\n# Apply function on cell values for each zone\n\n#~~ Using lapply\nzonal_stat=lapply(zonal_data,median, na.rm=T)     # Returns a list of zonal stats\n\n#~~ Using purrr::map\nlibrary(purrr)\nzonal_stat=purrr::map(zonal_data,\n                      ~ median(.x, na.rm=TRUE)) # Returns a list of zonal stats\n\n#~~ Try user defined function\nmyfun=function (y){\n  # User defined function for calculating median\n  p=median(y, na.rm=TRUE)\n  return(p)\n  }    \n\n#~ Implement function using lapply and map\n#~~ Using lapply\nzonal_stat=lapply(zonal_data,myfun)           # Returns a list of zonal median \n\n#~~ Using purrr::map\nlibrary(purrr)\nzonal_stat=purrr::map(zonal_data,~ myfun(.x)) # Returns a list of zonal median \nzonal_stat=unlist(zonal_stat)                 # Unlist to return a vector \n\nhead(zonal_stat) # Is this the same as the previous result?\n\n[1] 0.06029825 0.07462480 0.12249910 0.18718112 0.31052290\n\n\n\n\n\n5.5.2 Zonal Statistics with RATified Raster\nRasters with discrete data groups can also be used to summarize other rasters. Here, we will use the RATified aridity index raster to extract cell values corresponding to each climate zone.\n\nzonal_data=list()                   # Create an empty list to store values\nclimate_num=as.numeric(AI$climate)  # Convert climate class to numerical values\nclimate_num=resample(climate_num,sm, method=\"near\") # Resample to sm resolution \n\n# Extract pixel values within each climate zone\nZonalCells=function(x){\n  zonalCells=na.omit(sm[climate_num==x])\n  return(zonalCells)\n} \nclimate_zone_num=list(1,2,3,4,5)    # Store climate zone numbers as a list\n\n# Apply function using lapply \nzonal_data=lapply(climate_zone_num,\n                  ZonalCells)\n\n# Calculate and store stats for each climate zone\n#~~ Custom function for zonal median\nzonalMean=function(x){\n  zonalCells=na.omit(sm[climate_num==x])\n  p=median(zonalCells, na.rm=TRUE)\n  return(p)\n} \nclimate_zone_num=list(1,2,3,4,5)    # Store climate zone numbers as a list\nzonal_stat_list=lapply(climate_zone_num,zonalMean)\n\n# Equivalent to:\n# zonal_stat_list=lapply(list(1,2,3,4,5),function(x) (median(sm[climate_num==x], na.rm=TRUE)))\n\n# lapply returns a list, so we unlist the output to get an array\nzonal_stat=unlist(zonal_stat_list)\n\nplot(zonal_stat,     \n     xaxt = \"n\",              # Disable x-tick labels\n     xlab=\"Climate\",          # X axis label\n     ylab=\"Soil moisture\",    # Y axis label\n     type=\"b\",                # line type\n     col=\"blue\",              # Line color\n     main=\"Climate-wise median of surface soil moisture\")\naxis(1, at=1:5, labels=c(\"Hyper-arid\", \"Arid\", \"Semi-Arid\",\"Sub-humid\",\"Humid\"))\n\n\n\n\n\n\n\n\n\n\nChen, Xi, and Murugesu Sivapalan. 2020. “Hydrological Basis of the Budyko Curve: Data-Guided Exploration of the Mediating Role of Soil Moisture.” Water Resources Research 56 (10): e2020WR028221.\n\n\nNash, David J. 1999. “World Atlas of Desertification.” The Geographical Journal 165: 325.\n\n\nSchowengerdt, Robert A. 2007. “CHAPTER 7 - Correction and Calibration.” In Remote Sensing (Third Edition), edited by Robert A. Schowengerdt, Third Edition, 285–XXIII. Burlington: Academic Press. https://doi.org/https://doi.org/10.1016/B978-012369407-2/50010-3.\n\n\nZomer, Robert J, Jianchu Xu, and Antonio Trabucco. 2022. “Version 3 of the Global Aridity Index and Potential Evapotranspiration Database.” Scientific Data 9 (1): 409."
  },
  {
    "objectID": "ch6.html#popular-climate-data-format",
    "href": "ch6.html#popular-climate-data-format",
    "title": "6  Land and Climate Data",
    "section": "6.1 Popular Climate Data Format",
    "text": "6.1 Popular Climate Data Format\nRaster and netCDF are two popular formats used for gridded climate data dissemination and archiving.\nWe are all too familiar with the raster format (pixelated, georeferenced data) from the previous chapters. NetCDF (Network Common Data Form), is a common data type for multi-layered, structured, gridded dataset. NetCDF is a machine-independent data format and is a community standard for sharing scientific data.  A netCDF has certain features which makes it suitable for complex scientific data archiving and sharing, namely,\n\n\n\nSelf-Describing. A netCDF file includes information about the data it contains.\nPortable. A netCDF file can be accessed by computers with different ways of storing integers, characters, and floating-point numbers.\nScalable. A small subset of a large dataset may be accessed efficiently.\nAppendable. Data may be appended to a properly structured netCDF file without copying the dataset or redefining its structure.\nShareable. One writer and multiple readers may simultaneously access the same netCDF file.\nArchivable. Access to all earlier forms of netCDF data will be supported by current and future versions of the software.\n- From [Unidata | NetCDF (ucar.edu) https://www.unidata.ucar.edu/software/netcdf/]\n\n\n\n\n\n\nTwo widely used formats for gridded climate data storage and dissemination: (Left) Raster and (Right) netCDF\n\n\nSeveral open-source plaforms and agencies provide open access to a multitude of gridded land and climate datasets generated using satellites and land-surface/ climate models. In the following sections, we will familiarize ourselves with some of these resources."
  },
  {
    "objectID": "ch6.html#open-data-platforms",
    "href": "ch6.html#open-data-platforms",
    "title": "6  Land and Climate Data",
    "section": "6.2 Open-Data Platforms",
    "text": "6.2 Open-Data Platforms\n\n6.2.1 Climate Data from NOAA Physical Sciences Lab\n\nProduct overview / Data access/ Information: https://psl.noaa.gov/data/gridded/tables/daily.html\n\n\n\n\nA snapshot of NOAA’s Physical Sciences Lab’s portal for gridded climate data access\n\n\nThis website provides several land and climate variables such as: CPC Global Unified Gauge-Based Analysis of Daily Precipitation, CPC Global Temperature, NCEP/NCAR Reanalysis, Livneh daily CONUS near-surface gridded meteorological and derived hydrometeorological data.\n\n\n6.2.2 DAYMET: Daily Gridded Weather and Climate Data for North America [1 km x 1 km]\n\nProduct overview: https://daymet.ornl.gov/\nData access: https://daac.ornl.gov/cgi-bin/dataset_lister.pl?p=32\nData information: Refer to the User Guide provided with each dataset.\n\n\n\n\nDaymet Webpage for Gridded Climate/ Weather Data Access\n\n\nDaymet provides long-term, continuous, gridded estimates of daily weather and climatology variables at 1 km grid resolution for North America. The dataset is available in several forms, including monthly and annual climate summaries, in addition to the daily and/or sub-daily climate forcings: \n\nSub-daily Climate Forcings for Puerto Rico\nDaymet Version 4 Monthly Latency: Daily Surface Weather Data\nAnnual Climate Summaries on a 1-km Grid for North America, Version 4 R1\nMonthly Climate Summaries on a 1-km Grid for North America, Version 4 R1\nStation-Level Inputs and Cross-Validation for North America, Version 4 R1\nDaily Surface Weather Data on a 1-km Grid for North America, Version 4 R1\n\n\n\n\nDaymet Resource for Daily, 1 km Surface Weather Data Download\n\n\n\n\n6.2.3 CHIRPS Global Precipitation [~5 km x 5km]\n\nProduct overview: https://climatedataguide.ucar.edu/climate-data/chirps-climate-hazards-infrared-precipitation-station-data-version-2\nData access: https://data.chc.ucsb.edu/products/CHIRPS-2.0/\nData information: https://data.chc.ucsb.edu/products/CHIRPS-2.0/docs/README-CHIRPS.txt\n\n\n\n\nSnapshot of the web portal for CHIRPS-2.0 (Climate Hazards InfraRed Precipitation with Station data, version 2) data access\n\n\n\n\n6.2.4 GIMMS MODIS Global NDVI [~225 m x 225 m]\n\nProduct overview: https://ladsweb.modaps.eosdis.nasa.gov/missions-and-measurements/products/MOD13Q1\nData access: https://gimms.gsfc.nasa.gov/MODIS/\nData information: https://gimms.gsfc.nasa.gov/MODIS/README-global.txt\n\n\n\n\nGlobal Inventory Modeling and Mapping Studies (GIMMS) portal for global MODIS (Terra & Aqua) NDVI access\n\n\n\n\n6.2.5 Climate Prediction Center\n\nProduct overview / Data access/ Information: https://ftp.cpc.ncep.noaa.gov/GIS/\n\n\n\n\nFTP portal for NOAA’s Climate Prediction Center (CPC) data access\n\n\nIncludes several variables including (but not limited to):\n\nClimate Prediction Center (CPC) Morphing Technique (MORPH) to form a global, high resolution precipitation analysis\nJoint Agricultural Weather Facility (JAWF)\nGrid Analysis and Display System (GRADS): Global precipitation monitoring and forecasts, Tmax, Tmin\nInput variables for US Drought Monitor (USDM)\n\n\n\n6.2.6 Soil Texture for CONUS [30 m x 30 m]\nProbabilistic Remapping of SSURGO (POLARIS) is a database of 30-m probabilistic soil property maps over the Contiguous United States (CONUS) generated by removing artificial discontinuities in Soil Survey Geographic (SSURGO) database. using an artificial intelligence algorithm (Chaney et al. 2016, 2019). Estimates provided by POLARIS include soil texture, organic matter, pH, saturated hydraulic conductivity, Brooks-Corey and Van Genuchten water retention curve parameters, bulk density, and saturated water content for six profile depths, namely, 0-5 cm, 5-15 cm, 15-30 cm, 30-60 cm, 60-100 cm, 100-200 cm. \n\nProduct overview: https://www.usgs.gov/publications/polaris-properties-30-meter-probabilistic-maps-soil-properties-over-contiguous-united\nData access: http://hydrology.cee.duke.edu/POLARIS/PROPERTIES/v1.0/\nData information: http://hydrology.cee.duke.edu/POLARIS/PROPERTIES/v1.0/Readme\n\n\n\n\n\n\n\n\n6.2.7 NASA AρρEEARS\n\nProduct overview / Data access/ Information: https://appeears.earthdatacloud.nasa.gov/\n\n\n\n\nInterface for the Application for Extracting and Exploring Analysis Ready Samples (AρρEEARS)\n\n\nClimate/ land data variables can be extracted for an area or a point using the interactive interface. Click on point samples/ area samples:\n\n\n\nStart a new request\n\n\n\n\n\nSelect the variable and Lat-Long/ area of interest\n\n\n\n\n6.2.8 NASA Earth Data Search\n\nProduct overview / Data access/ Information: https://search.earthdata.nasa.gov/search\n\n\n\n\nEarth Data Search application interface\n\n\n\n6.2.8.1 Bulk Download Order\nNASA Earth Data provides customization options for bulk data download. Lets say we are interested in downloading global SMAP Level 3 soil moisture. We start by selecting the product, and specify start/ end date as needed.\n\n\n\n\n\nSearch for the product, click “Download All”. You will be taken to a log-in page.\n\n\n\n\n\n\n\nAfter logging in, Click “Edit Options”-&gt; “Customize” and select options as needed. Click “Done”.\n\n\n\n\n\n\n\nClick “Download Data”\n\n\n\n\n\n\n\nA “Download Status” page will appear. Click on the “.html” link.\n\n\n\n\n\n\n\nSeveral download options will available. For bulk download, click the link under “Retrieve list of files as a text listing (no html)”\n\n\n\n\n\n\n\nYou will be able to see the active download links.\n\n\n\n\nCopy and Paste these links in any internet download manager (my favorite in Chrono for Google Chrome), Select output location (typically an external hard drive) and let the download begin."
  },
  {
    "objectID": "ch6.html#programmatic-data-acquisition",
    "href": "ch6.html#programmatic-data-acquisition",
    "title": "6  Land and Climate Data",
    "section": "6.3 Programmatic Data Acquisition",
    "text": "6.3 Programmatic Data Acquisition\nIn the HTTP, FTP or HTP links provided before, one can download a file by clicking on the individual hyperlink. Alternatively, we can use download.file function to download the file programmatically in R. This help us by opening the path to automate download and processing of multiple files with minimal supervision.\n\n6.3.1 Downloading Raster files\nLet us take an example of us_tmax data available at: https://ftp.cpc.ncep.noaa.gov/GIS/GRADS_GIS/GeoTIFF/TEMP/us_tmax/\nRight-click on the raster file for 20240218, and copy the file path. We will then use this link to access the files programmatically using Client URL, or cURL - a utility for transferring data between systems. We will download the raster using download.file to local disk, and saved with a uder-defined name tmax_20240218.tif.\n\nCopied link: https://ftp.cpc.ncep.noaa.gov/GIS/GRADS_GIS/GeoTIFF/TEMP/us_tmax/us.tmax_nohads_ll_20240218_float.tif\n\n# Copied path of the raster\ndata_path &lt;- \"https://ftp.cpc.ncep.noaa.gov/GIS/GRADS_GIS/GeoTIFF/TEMP/us_tmax/us.tmax_nohads_ll_20240218_float.tif\"\n\n# Download the raster using download.file, assign the name tmax_20240218.tif to the downloaded \ndownload.file(url = data_path, \n              method=\"curl\",\n              destfile = \"tmax_20240218.tif\")  \n\n# Plot downloaded file\nlibrary(terra)\ntempRas=rast(\"tmax_20240218.tif\")       # Import raster to the environment \nusSHP=terra::vect(spData::us_states)    # Shapefile for CONUS\n\nplot(tempRas)\nplot(usSHP, add=TRUE)\n\n\n\n\nNow that we have the tmax_20240218 raster, let us extract the values for certain selected locations: s\n\n# Import sample locations from contrasting hydroclimate\nlibrary(readxl)\nloc= read_excel(\"./SampleData-master/location_points.xlsx\")\nprint(loc)\n\n# A tibble: 3 × 4\n  Aridity   State     Longitude Latitude\n  &lt;chr&gt;     &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n1 Humid     Louisiana     -92.7     34.3\n2 Arid      Nevada       -116.      38.7\n3 Semi-arid Kansas        -99.8     38.8\n\n# Value of the lat & lon of the locations\nlatlon=loc[,3:4] \nprint(latlon)\n\n# A tibble: 3 × 2\n  Longitude Latitude\n      &lt;dbl&gt;    &lt;dbl&gt;\n1     -92.7     34.3\n2    -116.      38.7\n3     -99.8     38.8\n\n# Extract time series using \"terra::extract\"\nloc_temp=terra::extract(tempRas,\n                         latlon,               #2-column matrix or data.frame with lat-long\n                         method='bilinear')    # Use bilinear interpolation (or ngb) option\n\nprint(loc_temp)\n\n  ID tmax_20240218\n1  1      12.58291\n2  2      11.52682\n3  3      13.41145\n\n# Add temperature attribute to the data frame as \"temp\"\nloc$temp=loc_temp$tmax_20240218\nprint(loc)\n\n# A tibble: 3 × 5\n  Aridity   State     Longitude Latitude  temp\n  &lt;chr&gt;     &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 Humid     Louisiana     -92.7     34.3  12.6\n2 Arid      Nevada       -116.      38.7  11.5\n3 Semi-arid Kansas        -99.8     38.8  13.4\n\n# Export the modified data as CSV\nwrite.csv(loc, \"df_with_temp.csv\")\n\n\n\n6.3.2 Raster Mosaic\nLets explore raster files for mean clay percentage for top 5 cm soil profile accessible through: http://hydrology.cee.duke.edu/POLARIS/PROPERTIES/v1.0/clay/mean/0_5/. The link provides rasters for 1x1 degree areal domain.\n\nlibrary(spData)\n# Note the spatial extent of Louisiana State. We need corresponding raster files\next(spData::us_states[spData::us_states$NAME==\"Louisiana\",]) # ext function gives extent of the rast/vect object\n\nSpatExtent : -94.042964, -89.066617, 28.991623, 33.019219 (xmin, xmax, ymin, ymax)\n\npth1=\"http://hydrology.cee.duke.edu/POLARIS/PROPERTIES/v1.0/clay/mean/0_5/lat3031_lon-91-90.tif\"\n\n# Download the raster using download.file\ndownload.file(url = pth1, \n              method=\"curl\",\n              destfile = \"lat3031_lon-90-89.tif\")  \n\n# Import the downloaded raster to workspace\nr1=rast(\"lat3031_lon-90-89.tif\")\n\n# Fetch shapefile for Louisiana from spData package\nLAvct=vect(spData::us_states[spData::us_states$NAME==\"Louisiana\",])\n\n# Plot downloaded raster against the map of Louisiana. Note that the raster fits 1x1 grid\nplot(LAvct, col=\"gray90\")\nplot(r1, range=c(0,100), add=TRUE)\ngrid()\n\n\n\n# Crop raster to smaller region of interest for easy visualization\n#~~~ We will crop raster to a user-defined extent\nr1crp=crop(r1, ext(c(-91,-90.8,30,30.2)))   \n\n# Explore a smaller subset of the dataset\n#~~~ Can you identify the Mississipi flood-plain using the clay percentage? \nlibrary(mapview)\nmapview(r1crp,                           # Raster to be plotted\n        at=c(0,10,20,30,40,50,60,75),    # Legend breaks\n        map.types=\"Esri.WorldImagery\",   # Select background map \n        main=\"Clay % (0-5 cm profile)\")  # Plot title\n\n\n\n\n\nWe note that the raster files in POLARIS are available for a 1x1 areal domain. For an analysis for a large spatial extent, multiple smaller rasters can be “stitched” together to generate a larger mosaic of rasters. We will use the terra::mosaic function to generate a mosaic of several smaller rasters of percentage clay content in 0-5 cm soil profile in Southeastern Louisiana. This function requires the user to specify the summary function (“sum”, “mean”, “median”, “min”, or “max”) to be applied on the overlapping pixels from 2 or more rasters. Let us download some more rasters from POLARIS and mosaic them together.\n\n\n\n“Beware of the dog” mosaic (Pompeii, Casa di Orfeo) is made of several constituent pieces.\n\n\n\n# Links to soil texture rasters \npth2=\"http://hydrology.cee.duke.edu/POLARIS/PROPERTIES/v1.0/clay/mean/0_5/lat2930_lon-91-90.tif\"\npth3=\"http://hydrology.cee.duke.edu/POLARIS/PROPERTIES/v1.0/clay/mean/0_5/lat3031_lon-90-89.tif\"\npth4=\"http://hydrology.cee.duke.edu/POLARIS/PROPERTIES/v1.0/clay/mean/0_5/lat3031_lon-92-91.tif\"\n\n# Download the raster using download.file function\ndownload.file(url = pth2, \n              method=\"curl\",\n              destfile = \"r2.tif\")  \n\ndownload.file(url = pth3, \n              method=\"curl\",\n              destfile = \"r3.tif\")\n\ndownload.file(url = pth4, \n              method=\"curl\",\n              destfile = \"r4.tif\")  \n\n# Import downloaded files to workspace\nr2=rast(\"r2.tif\")\nr3=rast(\"r3.tif\")\nr4=rast(\"r4.tif\")\n\n# DIY: Plot the downloaded raster against the map of Louisiana\n# plot(LAvct)\n# plot(r1, range=c(0,100), add=TRUE)\n# plot(r2, range=c(0,100), add=TRUE, legend=FALSE)\n# plot(r3, range=c(0,100), add=TRUE, legend=FALSE)\n# plot(r4, range=c(0,100), add=TRUE, legend=FALSE)\n\n# Raster mosaic\nr_mos=mosaic(r1,r2,r3,r4, fun=\"mean\")\n\n# Plot raster mossaic\nplot(LAvct, main=\"Clay % [0-5 cm depth]\",axes=FALSE, col=\"gray90\")\nplot(r_mos, range=c(0,100), add=TRUE)\nplot(LAvct, add=TRUE)\ngrid()\n\n\n\n\nFor easy post processing access, its a good practice to save the processed dataset on disc as either a raster or netCDF.\n\n# Export raster to disc\nterra::writeRaster(r_mos, \"clayPct.tif\",\n                overwrite=TRUE) # Overwrite if the file already exists?\n\n# Or export as netCDF\nterra::writeCDF(r_mos, \"clayPct.nc\", \n                overwrite=TRUE) # Overwrite if the file already exists?\n\n\n# Optional: Add more information to the exported netCDF\n# terra::writeCDF(r_mos,             # SpatRaster to export\n#          \"clayPct.nc\",             # Output filename\n#          varname=\"clayPCT\",        # Short name of the variable\n#          unit=\"%\",                 # Variable units\n#          longname=\"Clay Percentage, 0-5 cm, POLARIS\", # Long name of the variable\n#          zname='[-]')              # Z-var name (None, since we export a single layer)\n\n\n\n6.3.3 Downloading netCDF\nWe will now download a netCDF of global daily precipitation for the year 2023 from CHIRPS, accessible through the link: https://data.chc.ucsb.edu/products/CHIRPS-2.0/global_daily/netcdf/p05/chirps-v2.0.2023.days_p05.nc\n\n# Copied path of the raster\ndata_path &lt;- \"https://data.chc.ucsb.edu/products/CHIRPS-2.0/global_daily/netcdf/p05/chirps-v2.0.2023.days_p05.nc\"\n\n# Download the raster using download.file, assign the name \"daily_pcp_2023.nc\" to the downloaded \nif (file.exists(\"daily_pcp_2023.nc\")==FALSE){\n  \n download.file(url = data_path, \n              method=\"curl\",\n              destfile = \"daily_pcp_2023.nc\") \n  \n}\n\n# Plot downloaded file\nlibrary(terra)\npcp=rast(\"daily_pcp_2023.nc\")       # Import raster to the environment \nprint(pcp) # Notice the attributes (esp. nlyr, i.e. number of layers, unit and time)\n\nclass       : SpatRaster \ndimensions  : 2000, 7200, 365  (nrow, ncol, nlyr)\nresolution  : 0.05, 0.05  (x, y)\nextent      : -180, 180, -50, 50  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) \nsource      : daily_pcp_2023.nc \nvarname     : precip (Climate Hazards group InfraRed Precipitation with Stations) \nnames       : precip_1, precip_2, precip_3, precip_4, precip_5, precip_6, ... \nunit        :   mm/day,   mm/day,   mm/day,   mm/day,   mm/day,   mm/day, ... \ntime (days) : 2023-01-01 to 2023-12-31 \n\nhead(time(pcp))  # time variable in the netCDF indicating corresponding time of acquisition\n\n[1] \"2023-01-01\" \"2023-01-02\" \"2023-01-03\" \"2023-01-04\" \"2023-01-05\"\n[6] \"2023-01-06\"\n\nworldSHP=terra::vect(spData::world)    # Shapefile for CONUS\n\n# Plot data for a specific layer\nplot(pcp[[100]])   # Same as pcp[[which(time(pcp)==\"2023-04-10\")]]\nplot(worldSHP, add=TRUE)\npoints(latlon, pch=19, col=\"red\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChaney, Nathaniel W, Budiman Minasny, Jonathan D Herman, Travis W Nauman, Colby W Brungard, Cristine LS Morgan, Alexander B McBratney, Eric F Wood, and Yohannes Yimam. 2019. “POLARIS Soil Properties: 30-m Probabilistic Maps of Soil Properties over the Contiguous United States.” Water Resources Research 55 (4): 2916–38.\n\n\nChaney, Nathaniel W, Eric F Wood, Alexander B McBratney, Jonathan W Hempel, Travis W Nauman, Colby W Brungard, and Nathan P Odgers. 2016. “POLARIS: A 30-Meter Probabilistic Soil Series Map of the Contiguous United States.” Geoderma 274: 54–67."
  },
  {
    "objectID": "ch7.html#introduction",
    "href": "ch7.html#introduction",
    "title": "7  Multilayer Rasters: Layer–wise Operations",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction"
  },
  {
    "objectID": "ch7.html#raster-concatenation",
    "href": "ch7.html#raster-concatenation",
    "title": "7  Multilayer Rasters: Layer–wise Operations",
    "section": "7.2 Raster Concatenation",
    "text": "7.2 Raster Concatenation\nConcatenation (placing the objects one after the other) of rasters follow the same syntax as arrays i.e. by using c(obj1, obj2). Concatenation of rasters can be used for, but not limited to, creating a raster time series to evaluate time-dependent variability in the spatial data. Here we will use the global NDVI rasters from MODIS (resampled to a coarse resolution for computational ease) to create a raster time series in the form of a multilayer rast object, and will demonstrate several spatial and arithmetic operations on the multilayer rasters.\n\n\n\n\n\n\n7.2.1 Create Raster Time Series\nThe folder ./SampleData-master/raster_files/NDVI/ contains 23 rasters of global NDVI for the year 2016, with a retrieval frequency of 16 days. Let us import four sample rasters and concatenate them to generate a multilayer raster.\n\n#~~~ Create and plot NDVI SpatRaster\nlibrary(terra)\nlibrary(cetcolor)\n\n# Location of the NDVI raster files\nndvi_path=\"./SampleData-master/raster_files/NDVI/\" \n\n# List of all NDVI rasters\nras_path=list.files(ndvi_path,    # The folder which contains the files of interest\n                pattern='*.tif',  # Select files with this extention\n                full.names=TRUE)  # Print full file path? Yes (TRUE) or no (FALSE)?\n\nprint(ras_path)     # The folder contains 23 rasters, in increasing order of time \n\n [1] \"./SampleData-master/raster_files/NDVI/NDVI_resamp_2016-01-01.tif\"\n [2] \"./SampleData-master/raster_files/NDVI/NDVI_resamp_2016-01-17.tif\"\n [3] \"./SampleData-master/raster_files/NDVI/NDVI_resamp_2016-02-02.tif\"\n [4] \"./SampleData-master/raster_files/NDVI/NDVI_resamp_2016-02-18.tif\"\n [5] \"./SampleData-master/raster_files/NDVI/NDVI_resamp_2016-03-05.tif\"\n [6] \"./SampleData-master/raster_files/NDVI/NDVI_resamp_2016-03-21.tif\"\n [7] \"./SampleData-master/raster_files/NDVI/NDVI_resamp_2016-04-06.tif\"\n [8] \"./SampleData-master/raster_files/NDVI/NDVI_resamp_2016-04-22.tif\"\n [9] \"./SampleData-master/raster_files/NDVI/NDVI_resamp_2016-05-08.tif\"\n[10] \"./SampleData-master/raster_files/NDVI/NDVI_resamp_2016-05-24.tif\"\n[11] \"./SampleData-master/raster_files/NDVI/NDVI_resamp_2016-06-09.tif\"\n[12] \"./SampleData-master/raster_files/NDVI/NDVI_resamp_2016-06-25.tif\"\n[13] \"./SampleData-master/raster_files/NDVI/NDVI_resamp_2016-07-11.tif\"\n[14] \"./SampleData-master/raster_files/NDVI/NDVI_resamp_2016-07-27.tif\"\n[15] \"./SampleData-master/raster_files/NDVI/NDVI_resamp_2016-08-12.tif\"\n[16] \"./SampleData-master/raster_files/NDVI/NDVI_resamp_2016-08-28.tif\"\n[17] \"./SampleData-master/raster_files/NDVI/NDVI_resamp_2016-09-13.tif\"\n[18] \"./SampleData-master/raster_files/NDVI/NDVI_resamp_2016-09-29.tif\"\n[19] \"./SampleData-master/raster_files/NDVI/NDVI_resamp_2016-10-15.tif\"\n[20] \"./SampleData-master/raster_files/NDVI/NDVI_resamp_2016-10-31.tif\"\n[21] \"./SampleData-master/raster_files/NDVI/NDVI_resamp_2016-11-16.tif\"\n[22] \"./SampleData-master/raster_files/NDVI/NDVI_resamp_2016-12-02.tif\"\n[23] \"./SampleData-master/raster_files/NDVI/NDVI_resamp_2016-12-18.tif\"\n\n# User the file path to import raster\nr1=rast(ras_path[1])\nr2=rast(ras_path[2])\nr3=rast(ras_path[3])\nr4=rast(ras_path[4])\n\n# Concatenate the rasters i.e. arrange the rasters one after the other\nrast_ts=c(r1,r2, r3, r4)    \nnlyr(rast_ts)            # Number of layers in the multilayer raster\n\n[1] 4\n\nnames(rast_ts)           # Names of the raster layers\n\n[1] \"NDVI_resamp_2016-01-01\" \"NDVI_resamp_2016-01-17\" \"NDVI_resamp_2016-02-02\"\n[4] \"NDVI_resamp_2016-02-18\"\n\n# Plot concatenated rasters\ncolpal = cetcolor::cet_pal(20, name = \"r2\")  \nplot(rast_ts, col=rev(colpal))\n\n\n\n\n\n\n7.2.2 Programmatic Concatenation of rast Objects\nConcatenation method shown above can be good for a few rasters. However, in the case of a long time series, this method could be cumbersome. In such cases, we can write a small script to concatenate the rasters in to a multilayer rast object. We will first explore the use of loops- which will concatenate each rast layer sequentially. This method may take longer, but helps to avoid memory bottleneck when a very large number of rasters are simultaneously processed (the threshold may depend on the system memory and processor). In contrast, lapply and map vectorize the operation of creating the rast object for concatenation, and maybe faster for a moderate number of files (again, depends on the system configuration).\n\n#~~~~~~~~~~~~~~~~~~\n# Method 1: Using for loop to create raster layers from the raster location\n#~~ Suitable for large number of rasters and avoiding memory bottleneck\nras_stack=rast()\nfor (nRun in 1:length(ras_path)){\n  ras_stack=c(ras_stack,rast(ras_path[[nRun]]))\n}\n\n# Check dimension of data cube\ndim(ras_stack) #[x: y: z]- 23 raster layers with 456 x 964 cells\n\n[1] 456 964  23\n\n#~~~~~~~~~~~~~~~~~~\n# Method 2: Use lapply to create raster layer list from the raster file paths\nras_list = lapply(paste(ras_path, sep = \"\"), rast)\n# This a list of 23 raster objects stored as individual elements.\n\n# Convert raster layer lists to Spatraster \nras_stack = rast(ras_list)     # Stacking all rasters as a SpatRaster \n# This a multi-layer (23 layers in this case) SpatRaster Object.\n\n#~~~~~~~~~~~~~~~~~~\n# Method 3: maping the rast function on the raster file paths\nlibrary(purrr) \nras_list = purrr::map(ras_path, ~ rast(.x))  # Import the rasters into a list\nras_stack = rast(ras_list)                   # Convert a list of rasters to rast object\n\ndim(ras_stack) #[x: y: z]- 23 raster layers with 456 x 964 cells\n\n[1] 456 964  23\n\n\n\nComputing Counsel: Exporting concatenated rasters as a netCDF is a convenient time-saving measure. We can use terra::writeCDF to export SpatRaster object as netCDF. This allows the multilayer raster to be imported directly to the workspace and avoid repeating the steps in the preceeding section.\n\n# Export concatenated rasters as a netCDF\nterra::writeCDF(ras_stack,                 # SpatRaster\n         file.path(\"NDVI.nc\"),             # Output filename\n         overwrite=TRUE,\n         varname=\"NDVI\",\n         unit=\"[-]\",\n         longname=\"Global MODIS NDVI, 2016, 16_day, 36KM\",\n         zname='time')"
  },
  {
    "objectID": "ch7.html#spatial-operations-on-multilayer-raster",
    "href": "ch7.html#spatial-operations-on-multilayer-raster",
    "title": "7  Multilayer Rasters: Layer–wise Operations",
    "section": "7.3 Spatial Operations on Multilayer Raster",
    "text": "7.3 Spatial Operations on Multilayer Raster\n\n7.3.1 Subset Multilayer Raster\nSubsetting multilayer rasters (i.e. creating of smaller subset of layers) follows the same syntax we would use to subset a list. Notice the double square bracket i.e. [[ ]] we use to subset raster layers. When generating a spatial plot of multilayer rasters using terra::plot, to add vector boundaries to all plot layers, we need to define a custom function to add the object to each layer.\n\n# Subset raster stack/brick (notice the double [[]] bracket and similarity to lists)\nsub_ras_stack=ras_stack[[c(1,3,5,10,12)]] #Select 1st, 3rd, 5th, 10th and 12th layers\n\n#~~ Plot first 4 elements of NDVI SpatRaster\ncoastlines = vect(\"./SampleData-master/ne_10m_coastline/ne_10m_coastline.shp\")\n\n# Function to add shapefile to the plot\naddCoastlines=function(){\n  plot(coastlines, add=TRUE)   # Add Coastline vector to existing plot\n  } \n\n# Notice how a subset of first 4 rasters is created \nterra::plot(sub_ras_stack[[1:4]],  # Select raster layers to plot\n     col=colpal,                   # Specify colormap\n     asp=NA,                       # Aspect ratio= NA\n     fun=addCoastlines)            # Add coastline to each layer\n\n\n\n\n\n\n7.3.2 Crop and Mask\nCropping and masking a multilater raster follows the same format as that of a single raster we learned in Sec 5.3.\n\n# Names of the states to mask NDVI\nstates = c('Oklahoma','Texas','New Mexico', 'Louisiana', 'Arkansas')\n\n# Subset the selected states from CONUS simple feature (sf) object\nlibrary(sf)\nlibrary(spData)\nus_shp=spData::us_states\n\n# Subset sf for selected states using the NAME feature\nselectState = us_shp[(us_shp$NAME %in% states),]\n\n# Convert selectState sf to vect object\nstateVect= vect(selectState)\n\n# Reproject the state vector to match the CRS of the NDVI raster\nstateVect_proj=terra::project(stateVect,crs(ras_stack))\n\n# Raster operation\nndvi_crp = crop(ras_stack, ext(stateVect_proj))       # Crop raster\nndvi_msk = terra::mask(ndvi_crp,stateVect_proj)       # Mask\n\ndim(ndvi_msk) #Notice the number of layers in the masked rast\n\n[1] 30 53 23\n\n# Plot raster and shapefile\nplot(ndvi_msk[[1:4]], \n     col=colpal, \n     fun=function(){plot(stateVect_proj, add=TRUE)} # Add state borders\n     )\n\n\n\n\n\n\n7.3.3 Spatial Extraction\nSimilar to single-band raster operation shown in Sec. 5.5.1.2, this operation can be executed using terra::extract. However, the output will contain values from all layers in the multiband raster.\n\n# Extract cell values within each feature for all layers\nndvi_state_mean = terra::extract(ndvi_msk,       # Data cube\n                         stateVect_proj,    # Shapefile for feature reference \n                         na.rm=T)\n\n# Extract 'mean' statistic of cell values within each feature for all layers\nndvi_state_mean = terra::extract(ndvi_msk,  # Multilayer rast object\n                         stateVect_proj,    # Vect object of states\n                         fun=mean,          # Summary statistics (mean/sum/min/max)\n                         na.rm=T )          # Remove NA values? \n\n# Try: View(ndvi_state_mean)\n#~~~ Each row corresponds to one state. Columns store layer-wise mean NDVI per state\nhead(ndvi_state_mean)\n\n  ID NDVI_resamp_2016-01-01 NDVI_resamp_2016-01-17 NDVI_resamp_2016-02-02\n1  1              0.5522976              0.5080151              0.4881696\n2  2              0.3818293              0.3652746              0.3635756\n3  3              0.3994706              0.3769143              0.3628214\n4  4              0.4775158              0.4626955              0.4467108\n5  5              0.1515851              0.1885286              0.1956329\n  NDVI_resamp_2016-02-18 NDVI_resamp_2016-03-05 NDVI_resamp_2016-03-21\n1              0.4899222              0.4895655              0.5495264\n2              0.3819785              0.4111656              0.4301364\n3              0.3888056              0.4115137              0.4409466\n4              0.4653435              0.4628991              0.4804158\n5              0.2090006              0.2141808              0.2182761\n  NDVI_resamp_2016-04-06 NDVI_resamp_2016-04-22 NDVI_resamp_2016-05-08\n1              0.6105156              0.6728889              0.6717543\n2              0.4858011              0.5957775              0.6227723\n3              0.4841774              0.4977419              0.4900750\n4              0.5476653              0.6672000              0.6972526\n5              0.2341186              0.2381249              0.2497053\n  NDVI_resamp_2016-05-24 NDVI_resamp_2016-06-09 NDVI_resamp_2016-06-25\n1              0.6957812              0.7140958              0.7334875\n2              0.6317198              0.6319490              0.6212515\n3              0.5081387              0.5217799              0.4975696\n4              0.7327560              0.7381307              0.7769872\n5              0.2611739              0.2605942              0.2693318\n  NDVI_resamp_2016-07-11 NDVI_resamp_2016-07-27 NDVI_resamp_2016-08-12\n1              0.7264149              0.7297604              0.7205446\n2              0.6212688              0.5903892              0.5904805\n3              0.4569178              0.4475357              0.4784838\n4              0.7936874              0.7996058              0.7991813\n5              0.2534896              0.2710826              0.2933462\n  NDVI_resamp_2016-08-28 NDVI_resamp_2016-09-13 NDVI_resamp_2016-09-29\n1              0.7327004              0.6952247              0.6563840\n2              0.5939433              0.5947148              0.5561649\n3              0.5335192              0.5370391              0.5085575\n4              0.7747568              0.7191144              0.6574943\n5              0.3281408              0.3158202              0.2957566\n  NDVI_resamp_2016-10-15 NDVI_resamp_2016-10-31 NDVI_resamp_2016-11-16\n1              0.6274066              0.6019123              0.5819281\n2              0.5264949              0.5015890              0.4637258\n3              0.4742536              0.4476652              0.4316670\n4              0.6204193              0.5829816              0.5198666\n5              0.2680033              0.2750498              0.2497373\n  NDVI_resamp_2016-12-02 NDVI_resamp_2016-12-18\n1              0.5348509              0.5075413\n2              0.3907982              0.3692964\n3              0.3791562              0.3770174\n4              0.4779205              0.4680233\n5              0.2450552              0.2306126"
  },
  {
    "objectID": "ch7.html#layerwise-statisticalarithmetic-operations-on-multilayer-raster",
    "href": "ch7.html#layerwise-statisticalarithmetic-operations-on-multilayer-raster",
    "title": "7  Multilayer Rasters: Layer–wise Operations",
    "section": "7.4 Layer–wise Statistical/Arithmetic Operations on Multilayer Raster",
    "text": "7.4 Layer–wise Statistical/Arithmetic Operations on Multilayer Raster\n\n7.4.1 Layer–wise Statistics\nSimilar to what we learned in Sec. 4.2 Raster Statistics, we can use the global function to obtain summary statistics of the multilayer raster. Here the operation will yield the desired statistic for each constituent layer.\n\n# Layer-wise cell-statistics \n# Layer-wise Mean\nglobal(sub_ras_stack, mean, na.rm= T) # Try modal, median, min etc. \n\n                            mean\nNDVI_resamp_2016-01-01 0.2840846\nNDVI_resamp_2016-02-02 0.2948298\nNDVI_resamp_2016-03-05 0.3057182\nNDVI_resamp_2016-05-24 0.4618550\nNDVI_resamp_2016-06-25 0.4997354\n\n# Layer-wise quantiles\nglobal(sub_ras_stack, quantile, probs=c(.25,.75), na.rm= T)\n\n                             X25.      X75.\nNDVI_resamp_2016-01-01 0.07188153 0.5135894\nNDVI_resamp_2016-02-02 0.08442930 0.5177374\nNDVI_resamp_2016-03-05 0.09917563 0.5113073\nNDVI_resamp_2016-05-24 0.23671686 0.6695234\nNDVI_resamp_2016-06-25 0.26000642 0.7303487\n\n# User-defined statistics by defining own function\nquant_fun = function(x) {quantile(x, probs = c(0.25, 0.75), na.rm=TRUE)} \nglobal(sub_ras_stack, quant_fun) # 25th, and 75th percentile of each layer\n\n                             X25.      X75.\nNDVI_resamp_2016-01-01 0.07188153 0.5135894\nNDVI_resamp_2016-02-02 0.08442930 0.5177374\nNDVI_resamp_2016-03-05 0.09917563 0.5113073\nNDVI_resamp_2016-05-24 0.23671686 0.6695234\nNDVI_resamp_2016-06-25 0.26000642 0.7303487\n\n# Custom function for mean, variance and skewness\nmy_fun = function(x){ \n  meanVal=mean(x, na.rm=TRUE)              # Mean \n  varVal=var(x, na.rm=TRUE)                # Variance\n  skewVal=moments::skewness(x, na.rm=TRUE) # Skewness\n  output=c(meanVal,varVal,skewVal)         # Combine all statistics\n  names(output)=c(\"Mean\", \"Var\",\"Skew\")    # Rename output variables\n  return(output)                           # Return output\n} \n\nglobal(sub_ras_stack, my_fun) # Mean, Variance and skewness of each layer\n\n                            Mean        Var       Skew\nNDVI_resamp_2016-01-01 0.2840846 0.07224034  0.5409973\nNDVI_resamp_2016-02-02 0.2948298 0.06677124  0.5075244\nNDVI_resamp_2016-03-05 0.3057182 0.06171894  0.5018285\nNDVI_resamp_2016-05-24 0.4618550 0.05944947 -0.2721941\nNDVI_resamp_2016-06-25 0.4997354 0.06587261 -0.3392214\n\n\n\n\n7.4.2 Layer–wise Arithmetic Operations\nAgain, this operation is similar to the operations on a single-band rasters, except the operation is carried out for each constituent layer simultaneously. Let’s explore some examples.\n\n# Arithmetic operations on SpatRaster are same as lists\nadd = ndvi_msk+10                  # Add a number to raster layers\nmult = ndvi_msk*5                  # Multiply a number to raster layers\nsubset_mult = ndvi_msk[[1:3]]*10   # Multiply a number to a subset of raster layers\nlog_ra = log(ndvi_msk)        # Layer-wise Log-transformation\n\n# Data filtering based on cell-value\n#~~~ Assign NA to any value less than 0.5\n#~~~ Try ?terra::clamp for help and other options/uses\nfilter_rast = terra::clamp(ndvi_msk, lower=0.5, values=FALSE)\n\n# Let's plot the filtered rasters\nplot(filter_rast[[1:4]], \n     asp=NA,            # Aspect ratio: NA, fill to plot space\n     col=colpal,        # Assign colormap to the plot\n     nc=2,              # Number of columns to arrange plots, \n     range=c(0,0.9),    # Set custom z range for the plots\n     fun=function(){plot(stateVect_proj, add=TRUE)} # Add state borders\n)            \n\n\n\n# Summary statistics of layers using boxplots\n#~~~ Type ?boxplot in console for details on the selected options below\n#~~~ Based on the boxplot, can we be sure that clamping worked? \n\ndateStr = substr(names(ndvi_msk),13,23) # Substring of dates for X-axis labels\nboxplot(filter_rast, \n        horizontal=F, \n        col = \"royalblue\",frame=F, \n        cex.axis=1, notch=T, \n        col.axis = \"black\",\n        whisklty=1,\n        whiskcol=\"black\", whisklwd=1,\n        staplelwd = 1, \n        staplecol = \"royalblue\",\n        medcol = \"red\",medlwd=1.2, \n        col.ticks = \"black\", \n        outlier=F, \n        names=dateStr) \naxis(1,  tck=0.05, col.axis = \"transparent\", lwd=1)\n\n\n\n# Layerwise normalization to [0,1]\n#~~~ Calculate minumum and maximum cell values for each layer\nmin_val = global(ndvi_msk, min, na.rm= T) # Layer-wise minima\nmax_val = global(ndvi_msk, max, na.rm= T) # Layer-wise maxima\n#~~~ Normalize layers using respective minima and maxima\nnorm_stk=(ndvi_msk-min_val$min)/(max_val$max-min_val$min) \n\n# Plot the normalized rasters. Notice the raster values range between 0 and 1\nplot(norm_stk[[1:4]],col=colpal)"
  },
  {
    "objectID": "ch8.html#introduction",
    "href": "ch8.html#introduction",
    "title": "8  Multilayer Rasters: Cell–wise Operations",
    "section": "8.1 Introduction",
    "text": "8.1 Introduction"
  },
  {
    "objectID": "ch8.html#extracting-grid-time-series",
    "href": "ch8.html#extracting-grid-time-series",
    "title": "8  Multilayer Rasters: Cell–wise Operations",
    "section": "8.2 Extracting Grid Time Series",
    "text": "8.2 Extracting Grid Time Series\nLet us start by downloading the netCDF file for global daily precipitation for the year 2023 from CHIRPS, accessible through the link: https://data.chc.ucsb.edu/products/CHIRPS-2.0/global_daily/netcdf/p05/chirps-v2.0.2023.days_p05.nc. We can use terra::rast function to import the netCDF to the workspace as a multiband raster. Let us first familiarize ourselves with the dataset.\n\n# Copied path of the raster\ndata_path &lt;- \"https://data.chc.ucsb.edu/products/CHIRPS-2.0/global_daily/netcdf/p05/chirps-v2.0.2023.days_p05.nc\"\n\n# Download the raster using download.file. Assign \"daily_pcp_2023.nc\" name to file\nif (file.exists(\"daily_pcp_2023.nc\")==FALSE){\n  \n download.file(url = data_path, \n              method=\"curl\",\n              destfile = \"daily_pcp_2023.nc\") \n  \n}\n\n# Plot downloaded file\nlibrary(terra)\npcp=rast(\"daily_pcp_2023.nc\")       # Import raster to the environment \n\n#~~~ Notice the attributes (esp. nlyr, i.e. number of layers, unit and time)\nprint(pcp) \n\nclass       : SpatRaster \ndimensions  : 2000, 7200, 365  (nrow, ncol, nlyr)\nresolution  : 0.05, 0.05  (x, y)\nextent      : -180, 180, -50, 50  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) \nsource      : daily_pcp_2023.nc \nvarname     : precip (Climate Hazards group InfraRed Precipitation with Stations) \nnames       : precip_1, precip_2, precip_3, precip_4, precip_5, precip_6, ... \nunit        :   mm/day,   mm/day,   mm/day,   mm/day,   mm/day,   mm/day, ... \ntime (days) : 2023-01-01 to 2023-12-31 \n\n#~~~ time variable in the netCDF indicating corresponding time of acquisition\nhead(time(pcp))  \n\n[1] \"2023-01-01\" \"2023-01-02\" \"2023-01-03\" \"2023-01-04\" \"2023-01-05\"\n[6] \"2023-01-06\"\n\n# Import sample locations from contrasting hydroclimate\nlibrary(readxl)\nloc= read_excel(\"./SampleData-master/location_points.xlsx\")\nprint(loc)\n\n# A tibble: 3 × 4\n  Aridity   State     Longitude Latitude\n  &lt;chr&gt;     &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n1 Humid     Louisiana     -92.7     34.3\n2 Arid      Nevada       -116.      38.7\n3 Semi-arid Kansas        -99.8     38.8\n\n# Value of the lat & lon of the locations\nlatlon=loc[,3:4] \nprint(latlon)\n\n# A tibble: 3 × 2\n  Longitude Latitude\n      &lt;dbl&gt;    &lt;dbl&gt;\n1     -92.7     34.3\n2    -116.      38.7\n3     -99.8     38.8\n\n# Plot data for a specific layer\nworldSHP=terra::vect(spData::world)    # Shapefile for CONUS\n\nplot(pcp[[100]])   # Same as pcp[[which(time(pcp)==\"2023-04-10\")]]\nplot(worldSHP, add=TRUE)\npoints(latlon, pch=19, col=\"red\")\n\n\n\n\nWe have previously used terra::extract function to extract cell values from a raster for user-defined locations. However, unlike previous examples, precipitation data is imported from a netCDF, which has 365 layers, one for each day in the year 2023. So, when we use the extract function using point coordinates, 365 values for each location are extracted.\n\n# Import sample locations from contrasting hydroclimate\nlibrary(readxl)\nloc= read_excel(\"./SampleData-master/location_points.xlsx\")\nprint(loc)\n\n# A tibble: 3 × 4\n  Aridity   State     Longitude Latitude\n  &lt;chr&gt;     &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n1 Humid     Louisiana     -92.7     34.3\n2 Arid      Nevada       -116.      38.7\n3 Semi-arid Kansas        -99.8     38.8\n\n# Value of the lat & lon of the locations\nlatlon=loc[,3:4] \nprint(latlon)\n\n# A tibble: 3 × 2\n  Longitude Latitude\n      &lt;dbl&gt;    &lt;dbl&gt;\n1     -92.7     34.3\n2    -116.      38.7\n3     -99.8     38.8\n\n# Extract time series using \"terra::extract\"\nloc_pcp=terra::extract(pcp,\n                       latlon,               #2-column matrix or data.frame with lat-long\n                       method='bilinear')    # Use bilinear interpolation (or ngb) option\n# View data sample\nloc_pcp[,1:8] # View(loc_pcp)\n\n  ID  precip_1 precip_2 precip_3 precip_4 precip_5 precip_6 precip_7\n1  1 0.0000000 9.755308 46.80546 0.000000 0.000000 0.000000        0\n2  2 7.3486811 0.000000  0.00000 1.276224 3.523982 1.272632        0\n3  3 0.7002416 4.201450  0.00000 0.000000 0.000000 0.000000        0\n\n# Plot hyetograph for the location in Louisiana\nlibrary(ggplot2)\npcp_df1=data.frame(time=time(pcp),\n                   pcp=as.numeric(loc_pcp[1,-c(1)]))  # Select first row, exclude the first column\n\n# ggplot\nggplot(pcp_df1,aes(x=time,y=pcp)) + \n  geom_bar(stat = 'identity')+\n  theme_bw()+\n  ylab(\"Precipitation [mm/day]\")+\n  xlab(\"Time [Days]\")\n\n\n\n# Export the extracted data as CSV\nwrite.csv(loc, \"extracted_pcp2023.csv\")"
  },
  {
    "objectID": "ch8.html#cellwise-operation-on-all-layers",
    "href": "ch8.html#cellwise-operation-on-all-layers",
    "title": "8  Multilayer Rasters: Cell–wise Operations",
    "section": "8.3 Cell–wise Operation on All Layers",
    "text": "8.3 Cell–wise Operation on All Layers\nThe terra::app() function applies a function to each cell of a raster and is used to summarize (e.g., calculating the sum) the values of multiple layers into one layer. We will use the NDVI netCDF generated in Sec 7.1.2 for the year 2016 to calculate global cellwise statistics.\n\n#Let's look at the help section for app()\n# ?terra::app\n\n# Import NDVI layers from the NDVI generated in 7.1.2\nndviStack = rast(\"NDVI.nc\")\nndviStack\n\nclass       : SpatRaster \ndimensions  : 456, 964, 23  (nrow, ncol, nlyr)\nresolution  : 0.373444, 0.373444  (x, y)\nextent      : -180, 180, -85.24595, 85.0445  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : NDVI.nc \nnames       : NDVI_1, NDVI_2, NDVI_3, NDVI_4, NDVI_5, NDVI_6, ... \nunit        :    [-],    [-],    [-],    [-],    [-],    [-], ... \n\n# Subset raster stack/brick (notice the double [[]] bracket and similarity to lists)\nndviStack_sub=ndviStack[[c(1,3,5,10,12)]] #Select 1st, 3rd, 5th, 10th and 12th layers\n\n# Calculate mean of each grid cell across all layers\nmean_ras = app(ndviStack_sub, fun=mean, na.rm = T)\n\n# Calculate sum of each grid cell across all layers\nsum_ras = app(ndviStack_sub, fun=sum, na.rm = T)\n\n#~~ A user-defined function for mean, variance and skewness\nmy_fun = function(x){ \n  meanVal=mean(x, na.rm=TRUE)              # Mean \n  varVal=var(x, na.rm=TRUE)                # Variance\n  skewVal=moments::skewness(x, na.rm=TRUE) # Skewness\n  output=c(meanVal,varVal,skewVal)         # Combine all statistics\n  names(output)=c(\"Mean\", \"Var\",\"Skew\")    # Rename output variables\n  return(output)                           # Return output\n} \n\n# Apply user function to each cell across all layers\nstat_ras = app(ndviStack_sub, fun=my_fun)\n\n# Plot statistics\ncoastlines = vect(\"./SampleData-master/ne_10m_coastline/ne_10m_coastline.shp\")\nlibrary(cetcolor)\ncolpal = cetcolor::cet_pal(20, name = \"r2\")  \n\nplot(stat_ras, \n     col = rev(colpal), # Rev argument reverses the Palette (from Bu-&gt; Rd to Rd -&gt; Bu)\n     asp = NA,          # Aspect ratio: NA, fill to plot space\n     nc = 2,            # Number of columns to arrange plots\n     fun = function(){plot(coastlines, add=TRUE)} # Add coastline boundary\n)"
  },
  {
    "objectID": "ch8.html#cellwise-operation-on-layer-groups",
    "href": "ch8.html#cellwise-operation-on-layer-groups",
    "title": "8  Multilayer Rasters: Cell–wise Operations",
    "section": "8.4 Cell–wise Operation on Layer Groups",
    "text": "8.4 Cell–wise Operation on Layer Groups\nterra::tapp() is an extension of app(), allowing us to select a subset of layers for which we want to perform a certain operation. Let’s have the first two layers as group 1 and the next three as group 2. Function will be applied to each group separately and 2 layers of output will be generated.\n\n#The layers are combined based on indexing.\nstat_ras = terra::tapp(ndviStack_sub,\n                     index=c(1,1,2,2,2),\n                     fun= mean)\n\n# Try other functions: \"sum\", \"mean\", \"median\", \"modal\", \"which\", \"which.min\", \"which.max\", \"min\", \"max\", \"prod\", \"any\", \"all\", \"sd\", \"first\".\n\nnames(stat_ras) = c(\"Mean_of_rasters_1_to_2\", \"Mean_of_rasters_3_to_5\")\n# Two layers are formed, one for each group of indices\n\n# Lets plot the two output rasters\nplot(stat_ras, \n     col = rev(colpal), # Rev argument reverses the Palette (from Bu-&gt; Rd to Rd -&gt; Bu)\n     asp = 1,           # Aspect ratio: 1, i.e. X==Y\n     nc = 1,            # Number of columns to arrange plots\n     fun = function(){plot(coastlines, add=TRUE)} # Add coastline boundary\n)"
  },
  {
    "objectID": "ch8.html#layers-as-function-arguments",
    "href": "ch8.html#layers-as-function-arguments",
    "title": "8  Multilayer Rasters: Cell–wise Operations",
    "section": "8.5 Layers as Function Arguments",
    "text": "8.5 Layers as Function Arguments\nThe terra::lapp() function allows to apply a function to each cell using layers as arguments.\n\n#Let's look at the help section for app()\n# ?terra::lapp\n\n#User defined function for finding difference\ndiff_fun = function(a, b){ return(a-b) }\n\ndiff_rast = lapp(ndviStack_sub[[c(4, 2)]], fun = diff_fun)\n\n#Plot NDVI difference\nplot(diff_rast, \n     col = rev(colpal), # Rev argument reverses the Palette (from Bu-&gt; Rd to Rd -&gt; Bu)\n     asp = 1,           # Aspect ratio: 1, i.e. X==Y\n     fun = function(){plot(coastlines, add=TRUE)} # Add coastline boundary\n)"
  },
  {
    "objectID": "ch9.html#cellwise-operation",
    "href": "ch9.html#cellwise-operation",
    "title": "9  Parallel Geospatial Computing",
    "section": "9.1 Cell–wise Operation",
    "text": "9.1 Cell–wise Operation\n\n9.1.1 Apply custom function to pixel time series\n\n\n\n\n\nOnce we have imported the netCDF file as rast object, we will apply a slightly modified version of previously used function my_fun (from Ch 8) for calculating mean, variance and skewness for time series data for each cell in parallel. We will use terra::app function to apply my_fun on SpatRaster in parallel.\nFor seamless implementation of function in parallel mode, care must be taken that all necessary are accessible to ALL cores and error exceptions are handles appropriately. We will modify my_fun slightly to highlight what it means in practice.\n\n\nWe will convert input x to a numeric array\nWe will remove NA values from dataset before calculation\nWe will use minSamp to fix minimum sample counts for calculation. If the number of observations for a pixel are less than minSamp, the grid is skipped.\nWe will use tryCatch to handle error exceptions\n\n\nThe basic rules to avoid errors: (a) checking that inputs are correct, (b) avoiding non-standard evaluation, and (c) avoiding functions that can return different types of output.\n\n#~~ We will make some changes in the custom function for mean, variance and skewness\n\nmy_fun = function(x, minSamp){    \n  smTS=as.numeric(as.vector(x))     # Convert dataset to numeric array\n  smTS=as.numeric(na.omit(smTS))    # Omit NA values \n  \n  # Implement function with trycatch for catching exception \n  tryCatch(if(length(smTS)&gt;minSamp) {      # Apply minimum sample filter\n  \n  ######## OPERATION BEGINS #############    \n  meanVal=mean(smTS, na.rm=TRUE)              # Mean \n  varVal=var(smTS, na.rm=TRUE)                # Variance\n  skewVal=moments::skewness(smTS, na.rm=TRUE) # Skewness\n  output=c(meanVal,varVal,skewVal)            # Combine all statistics\n  return(output)                              # Return output\n  ######## OPERATION ENDS #############    \n\n  } else {\n    return(rep(NA,3))                         # If conditions !=TRUE, return array with NA\n  },error =function(e){return(rep(NA, 3))})   # If Error== TRUE, return array with NA\n}\n\n\n# Apply function to all grids in parallel\nlibrary(tictoc)\ntic()\nstat_brk = app(SMAPrast, \n               my_fun, \n               minSamp = 50,                                          # Minimum assured samples for statistics\n               cores =parallel::detectCores(logical = FALSE) - 1)     # Leave one core for housekeeping\n\n# Beware while using detectCores(). \n# The argument logical = FALSE returns the number of physical cores.\n# logical = TRUE returns the number of available hardware threads. \n\nnames(stat_brk)=c(\"Mean\", \"Variance\", \"Skewness\")      # Add layer names\ntoc()\n\n4.82 sec elapsed\n\n# Plot statistics\nlibrary(cetcolor)\ncolpal = cetcolor::cet_pal(20, name = \"r2\")  \nplot(stat_brk, col=colpal)\n\n\n\n\n\n\n9.1.2 Best practices for large-scale parallel operations\nError handling is the art of debugging unexpected problems in your code. One easy solution when looping through customized functions is to include print() messages after each major operation which can help indicate where the error might be happening.\nWhen working with large spatial data, the following the steps listed below can be very helpful:\n\n\nCheck if the function works as expected by testing it first on a sample series extracted (using terra::extract function) for a test location from the multilayer raster. For example, for a sample location with Long=-100, and Lat=35 (in decimal degrees) we can extract a time series for soil moisture and test the custom function as follows:\n\n# Location with Long=-100, and Lat=35 (in decimal degrees) extract time series \nts_sample=terra::extract(SMAPrast, cbind(-100,35))  \n\n# Does it generate three numeric values as expected? \nmy_fun(ts_sample, minSamp=50)\n\n[1] 0.139086710 0.002358918 0.554885498\n\n\nTry parallel operation on a smaller region before mounting large jobs for cpmputing. Pixel-wise implementation of the function can help identify errors in the code. Convert the cropped region into a data frame and apply function to time series of each cell. If your code throws error, troubleshoot carefully for the series which generates the error.\n\nlibrary(terra)   \ne &lt;- ext( c(-110,-108, 35,37) )   # Sample 2X2 degree domain   \np &lt;- as.polygons(e)   \ncrs(p) &lt;- \"EPSG:4326\"             # Use this polygon to crop and mask the larger SpatRaster\n\nUse tryCatch carefully as it may suppress legitimate errors as well, generating spurious results. Test the codes for smaller region without tryCatch to test the robustness of your codes.\n\n\nRemember, parallel computing may have some overheads upon creation and closing of clusters. A significant improvement in computing times using parallel techniques would be visible for large jobs."
  },
  {
    "objectID": "ch9.html#cellwise-operation-on-two-multilayer-rasters",
    "href": "ch9.html#cellwise-operation-on-two-multilayer-rasters",
    "title": "9  Parallel Geospatial Computing",
    "section": "9.2 Cell–wise Operation On Two Multilayer Rasters",
    "text": "9.2 Cell–wise Operation On Two Multilayer Rasters\n\n\n\n\n\nWe will use modeled daily evapotranspiration (ET) and maximum daily temperature (Tmax) from NOAA-Physical Sciences Lab’s repository for the year 2011 to find the correlation between ET and Tmax at each grid over CONUS. This dataset is available in NetCDF format (tmax.2011.nc and et.2011.nc). Due to sufficient availability of moisture in humid and sub-humid climates, an increase in temperature is matched with a corresponding increase in evapotranspiration (hence, positive correlation). In contrast, in arid and semi-arid regions, general scarcity of moisture restricts cooling of the surface by evaporation. Hence, larger fraction of incoming radiation is used up to heat up the land surface. This shows up as near zero or negative correlation between temperature and evapotranspiration in our analysis. This forms the basis of terrestrial energy balance.\n\n# Open access path of the daily ET and Tmax .nc files for the year 2011\ntmax_path = \"https://downloads.psl.noaa.gov//Datasets/livneh/metvars/tmax.2011.nc\"\net_path = \"https://downloads.psl.noaa.gov//Datasets/livneh/fluxvars/et.2011.nc\"\n\n# Download multilayer rasters from the NOAA-PSL servers\ndownload.file(url = tmax_path, method=\"curl\", destfile = \"tmax.2011.nc\") \ndownload.file(url = et_path, method=\"curl\", destfile = \"et.2011.nc\") \n\n# Custon function for correlation between time series from ET and Tmax multilayer rasters\n#~~~ arg: \"pairwise.complete.obs\" ignores NA values in either dataset\ncorfun=function (x, y) {\n  return(cor(x, y, use = \"pairwise.complete.obs\"))\n}\n\n# Import netCDFs as multilayer rasters\ntmax=rast(\"tmax.2011.nc\")\net=rast(\"et.2011.nc\")\n\n# Pixelwise correlation between daily ET and Tmax\nxcor = terra::xapp(et, tmax, fun= corfun)     \n\n# Plot map of the correlation \nplot(xcor, main=\" Correlation: ET vs Tmax\")"
  },
  {
    "objectID": "ch9.html#layerwise-parallel-computing",
    "href": "ch9.html#layerwise-parallel-computing",
    "title": "9  Parallel Geospatial Computing",
    "section": "9.3 Layer–wise Parallel Computing",
    "text": "9.3 Layer–wise Parallel Computing\n\n\n\n\n\nWe will convert SpatRaster to a list of rasters and then we will apply my_fun to each element of the list in parallel using future_lapply. Beware while using detectCores(). The argument logical = FALSE returns the number of physical cores and logical = TRUE returns the number of available hardware threads.\n\nlibrary(terra)\n# Import SMAP soil moisture NetCDF to the workspace\nSMAPrast = rast(\"./SampleData-master/SMAP_L3_USA.nc\")\n\n# Convert Spatraster to a list of rasters\nrasList=as.list(SMAPrast[[1:10]])           # What will happen if we pass rast(rasList)?\nlength(rasList)\n\n[1] 10\n\n# Custom function to be implemented on each layer\nmy_fun = function(x){                \n  x=as.numeric(values(x))                  # Create vector of numeric values of SpatRaster\n  meanVal=base::mean(x, na.rm=TRUE)        # Mean \n  varVal=stats::var(x, na.rm=TRUE)         # Variance\n  skewVal=moments::skewness(x, na.rm=TRUE) # Skewness\n  output=c(meanVal,varVal,skewVal)         # Combine all statistics\n  return(output)                           # Return output\n} \n\n# Test the function for one raster\nmy_fun(rasList[[10]])\n\n[1] 0.19304984 0.01375684 0.65087480\n\n# Apply function in parallel for all layers\nlibrary(parallel) \nlibrary(future.apply) \nlibrary(future)\nlibrary(tictoc)\n\n# Create worker nodes with shared environment  \n#~~~ employ max core-1 for processing\nfuture::plan(multicore, workers = detectCores(logical = FALSE) - 1)\n\n# Deploy function in parallel \ntic()\noutStat= future_lapply(rasList, my_fun)\ntoc()\n\n22.21 sec elapsed\n\n# Check output for one layer\n# outStat[[2]]"
  },
  {
    "objectID": "ch9.html#blockwise-parallel-computing",
    "href": "ch9.html#blockwise-parallel-computing",
    "title": "9  Parallel Geospatial Computing",
    "section": "9.4 Block–wise Parallel Computing",
    "text": "9.4 Block–wise Parallel Computing\n\n\n\n\n\nIn this section we will use a shapefile to extract cell values from a SpatRaster as a list using exact_extract. Summary statistics will be calculated in parallel using my_fun for dataset for each feature. Function exactextractr::exact_extract is faster and more suited for large applications compared to terra::extract. Although both perform similar operation with little changes in output format.\n\n#~ Extract feature data as data frame\nlibrary(exactextractr)\nlibrary(sf)\nlibrary(sp)\n\nfeatureData=exact_extract(SMAPrast,  # Raster brick \n                st_as_sf(us_shp),    # Convert shapefile to sf (simple feature)\n                force_df = FALSE,    # Output as a data.frame?\n                include_xy = FALSE,  # Include cell lat-long in output?\n                fun = NULL,          # Specify the function to apply for each feature extracted data\n                progress = TRUE)     # Progressbar\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |======================================================================| 100%\n\nlength(featureData) # Same as feature count in CONUS? i.e. nrow(conus) \n\n[1] 49\n\n# Lets try out data for Louisiana\nwhich(us_shp$NAME==\"Louisiana\")  # Find feature number for Louisiana\n\n[1] 10\n\n# View(featureData[[10]])        # View the extracted data frame\nnrow(featureData[[10]])          # No. pixels within selected feature\n\n[1] 146\n\n\nEach row in featureData[[10]] is the time series of cell values which fall within the boundary of feature number 10, i.e. Louisiana. Since exact_extract function provides coverage_fraction for each pixel in the output, we will make some minor change in the my_fun function to remove this variable before calculating the statistics.\n\n# Extract SM time series for first pixel by removing percentage fraction\ncellTS=as.numeric(featureData[[10]][1,1:nlyr(SMAPrast)])\n\n# Plot time time series for the selected feature\nplot(cellTS, type=\"l\", xlab=\"Time\", ylab=\"Soil moisture\")\n\n\n\n#~~ We will make another small change in the custom function for mean, variance and skewness\nmy_fun = function(x, minSamp =50, na.rm=TRUE){    \n  xDF=data.frame(x)                  # Convert list to data frame\n  xDF=xDF[ , !(names(xDF) %in% 'coverage_fraction')] # Remove coverage_fraction column\n  xData=as.vector(as.matrix(xDF))    # Convert data.frame to 1-D matrix\n  smTS=as.numeric(na.omit(xData))    # Omit NA values                   \n  \n  # Implement function with trycatch for catching exception \n  tryCatch(if(length(smTS)&gt;minSamp) {      # Apply minimum sample filter\n  \n  ######## OPERATION BEGINS #############    \n  meanVal=mean(smTS, na.rm=TRUE)              # Mean \n  varVal=var(smTS, na.rm=TRUE)                # Variance\n  skewVal=moments::skewness(smTS, na.rm=TRUE) # Skewness\n  output=c(meanVal,varVal,skewVal)         # Combine all statistics\n  return(output)                           # Return output\n  ######## OPERATION ENDS #############    \n\n  } else {\n    return(rep(NA,3))   # If conditions !=TRUE, return array with NA\n  },error =function(e){return(rep(NA, 3))}) # If Error== TRUE, return array with NA\n}\n\nLet’s apply my_fun to extracted data for each feature.\n\n# Test the function for one block\nmy_fun(featureData[[10]])\n\n[1]  0.37738906  0.00616663 -0.38568026\n\n# Apply function in parallel for all layers\nlibrary(parallel) \nlibrary(snow)\nlibrary(future.apply) \nlibrary(future)\n\n# Specify argument minSamp to be passed along to all nodes \nminSamp=50   # Minimum assured samples for statistics\n\n# Create worker nodes with shared environment  \nfuture::plan(multisession, workers = detectCores(logical = FALSE) - 1)\n\n# Apply the function in parallel\noutStat= future_lapply(featureData, my_fun)\n\n# Test output for one feature\noutStat[[10]]  # Is this the same as before?\n\n[1]  0.37738906  0.00616663 -0.38568026\n\n# Extract each summary stats for all features from the output list  \nFeatureMean=sapply(outStat,\"[[\",1)  # Extract mean for all features\nFeatureVar=sapply(outStat,\"[[\",2)   # Extract variance for all features\nFeatureSkew=sapply(outStat,\"[[\",3)  # Extract skewness for all features\n\n# Let's place mean statistics as an attribute to the shapefile\nus_shp$meanSM=FeatureMean\n\n# Plot mean soil moisture map for CONUS \nlibrary(rcartocolor)\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(sp)\n\nmean_map=ggplot() + \n  geom_sf(data = st_as_sf(us_shp), # CONUS shp as sf object (simple feature)\n          aes(fill = meanSM)) +   # Plot fill color= mean soil moisture\n  scale_fill_carto_c(palette = \"BluYl\",     # Using carto color palette\n                     name = \"Mean SM\",      # Legend name\n                     na.value = \"#e9e9e9\",  # Fill values for NA \n                     direction = 1)+        # To invert color, use -1\n  coord_sf(crs = 2163)+   # Reprojecting polygon 4326 or 3083 \n  theme_void() +          # Plot theme. Try: theme_bw\n  theme(legend.position = c(0.2, 0.1),  \n        legend.direction = \"horizontal\",\n        legend.key.width = unit(5, \"mm\"),\n        legend.key.height = unit(4, \"mm\"))\nmean_map"
  },
  {
    "objectID": "ch10.html#flash-cards",
    "href": "ch10.html#flash-cards",
    "title": "10  Practice Exercises",
    "section": "10.1 Flash Cards",
    "text": "10.1 Flash Cards"
  },
  {
    "objectID": "ch10.html#exercise-1",
    "href": "ch10.html#exercise-1",
    "title": "10  Practice Exercises",
    "section": "10.2 Exercise #1",
    "text": "10.2 Exercise #1\nThe U.S. Climate Reference Network (USCRN) is a systematic and sustained network of climate monitoring stations. USCRN has sites across Contiguous U.S. along with some in Alaska, and Hawaii. These stations are instrumented to measure meteorological information such as temperature, precipitation, wind speed, along with other relevant hydrologic variables such as soil moisture at uniform depths (5, 10, 20, 50, 100 cm) at sub-hourly, daily and monthly time scales. Users can access daily data set from all station suing the following link: Index of /pub/data/uscrn/products/daily01 (noaa.gov)\nLet us extract sample data from a USCRN site in Lafayette, LA, USA for 2021.\n\n# Yearly data from the sample station\nCRNdat = read.csv(url(\"https://www.ncei.noaa.gov/pub/data/uscrn/products/daily01/2021/CRND0103-2021-LA_Lafayette_13_SE.txt\"), header=FALSE,sep=\"\")\n\n# Data headers\nheaders=read.csv(url(\"https://www.ncei.noaa.gov/pub/data/uscrn/products/daily01/headers.txt\"), header=FALSE,sep=\"\")\n\n# Column names as headers from the text file\ncolnames(CRNdat)=headers[2,1:ncol(CRNdat)]\n\n# Replace fill values with NA\nCRNdat[CRNdat == -9999]=NA\nCRNdat[CRNdat == -99]=NA\nCRNdat[CRNdat == 999]=NA\n\n# View data sample\nlibrary(kableExtra)\ndataTable = kbl(head(CRNdat,6),full_width = F)\nkable_styling(dataTable,bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\n\nWBANNO\nLST_DATE\nCRX_VN\nLONGITUDE\nLATITUDE\nT_DAILY_MAX\nT_DAILY_MIN\nT_DAILY_MEAN\nT_DAILY_AVG\nP_DAILY_CALC\nSOLARAD_DAILY\nSUR_TEMP_DAILY_TYPE\nSUR_TEMP_DAILY_MAX\nSUR_TEMP_DAILY_MIN\nSUR_TEMP_DAILY_AVG\nRH_DAILY_MAX\nRH_DAILY_MIN\nRH_DAILY_AVG\nSOIL_MOISTURE_5_DAILY\nSOIL_MOISTURE_10_DAILY\nSOIL_MOISTURE_20_DAILY\nSOIL_MOISTURE_50_DAILY\nSOIL_MOISTURE_100_DAILY\nSOIL_TEMP_5_DAILY\nSOIL_TEMP_10_DAILY\nSOIL_TEMP_20_DAILY\nSOIL_TEMP_50_DAILY\nSOIL_TEMP_100_DAILY\n\n\n\n\n53960\n20210101\n2.622\n-91.87\n30.09\n14.0\n5.2\n9.6\n11.0\n0.0\n12.16\nC\n18.7\n4.6\n11.6\n92.8\n49.3\n72.0\n0.401\n0.372\n0.380\n0.405\n0.381\n16.2\n15.3\n15.5\n15.7\n15.5\n\n\n53960\n20210102\n2.622\n-91.87\n30.09\n10.4\n1.9\n6.1\n6.5\n0.0\n8.95\nC\n15.3\n0.4\n7.3\n98.6\n61.6\n78.4\n0.396\n0.370\n0.377\n0.406\n0.376\n14.4\n13.3\n14.1\n15.2\n15.0\n\n\n53960\n20210103\n2.622\n-91.87\n30.09\n16.3\n-0.1\n8.1\n7.9\n0.0\n13.93\nC\n24.3\n-0.9\n9.5\n100.0\n42.1\n76.3\n0.392\n0.368\n0.374\n0.404\n0.373\n12.8\n11.8\n12.8\n14.4\n14.2\n\n\n53960\n20210104\n2.622\n-91.87\n30.09\n22.2\n3.7\n12.9\n12.5\n0.0\n11.56\nC\n26.4\n2.6\n13.2\n98.9\n47.7\n80.2\n0.389\n0.366\n0.370\n0.400\n0.372\n13.0\n12.2\n12.7\n14.0\n14.0\n\n\n53960\n20210105\n2.622\n-91.87\n30.09\n20.7\n4.5\n12.6\n11.4\n0.0\n14.37\nC\n28.9\n3.1\n13.3\n100.0\n27.7\n71.0\n0.388\n0.364\n0.368\n0.399\n0.369\n13.0\n12.1\n12.7\n13.9\n14.0\n\n\n53960\n20210106\n2.622\n-91.87\n30.09\n19.4\n4.9\n12.2\n12.6\n20.7\n9.79\nC\n23.1\n3.5\n12.8\n98.5\n54.7\n78.9\n0.390\n0.363\n0.369\n0.399\n0.370\n12.8\n12.1\n12.5\n13.7\n13.7\n\n\n\n\n\n\n\n\nNotice the variables provided in the dataset. As an example, we can plots soil moisture data from a depth of 20 cm for this station for our reference:\n\n# Sample plot for soil moisture\nx=CRNdat$SOIL_MOISTURE_20_DAILY\n\n# Plot time series and density distribution \nplot(x, type=\"l\", ylab=\"Soil moisture (v/v)\", \n     col=\"cyan4\", lwd=3)\nplot(density(na.omit(x)), main=\" \", xlab=\"\", \n     col=\"cyan4\", lwd=3)\n\n\n\n\n\n\n\n(a) Time series of SM\n\n\n\n\n\n\n\n(b) SM kernel density\n\n\n\n\nFigure 10.1: Soil moisture values at the selected USCRN station\n\n\n\n\nTaking examples of any two USCRN stations across contrasting hydroclimates, compare and contrast any two recorded variables using time series plots, probability density distribution histograms and scatter plots. Select any year of your liking for the analysis.\nSelect two seasons for each elected variable and demonstrate the seasonal variability in the records for summer (MAMJJA) and winter (SONDJF) seasons using any two types of multivariate plots."
  },
  {
    "objectID": "ch10.html#exercise-2",
    "href": "ch10.html#exercise-2",
    "title": "10  Practice Exercises",
    "section": "10.3 Exercise #2",
    "text": "10.3 Exercise #2\nUse the raster files in SampleData-master/SMAPL4_rasters folder, perform the tasks as listed below:\n\nTask 1: Write a user-defined function to count the number of pixels within the range [0.3, 0.4] for 20211213. Use global to implement your function. Pass the specified range as an input argument to the function.\nTask 2: Compared to 20211213, what is the percentage change in 20211215 for the pixels within the [0.3, 0.4] range?\nTask 3: Make a multivariate plot (2 or more variables on the same axis) comparing the density distribution of soil moisture for 20211213 and 20211215?"
  },
  {
    "objectID": "ch10.html#exercise-3",
    "href": "ch10.html#exercise-3",
    "title": "10  Practice Exercises",
    "section": "10.4 Exercise #3",
    "text": "10.4 Exercise #3\nThe Harmonized Landsat Sentinel-2 (HLS) project provides consistent multiband surface reflectance (SR) data from NASA/USGS Landsat 8 and Landsat 9 satellites (using Operational Land Imager, OLI) and Europe’s Copernicus Sentinel-2A and Sentinel-2B satellites (using Multi-Spectral Instrument, MSI). These measurements provide global land observations with a 2–3 days temporal frequency at 30-meter spatial resolution. Sample rasters for band 4 (Red) and band 5 (Near-Infrared) from this resource are provided at the following location in the sample data: “./SampleData-master/raster_files/landsat/”\nFor LANDSAT 8, the Normalized Difference Vegetation Index (a proxy for vegetation health) is calculated as:\n\\[\nNDVI=(Band 5- Band 4)/(Band 5+ Band 4)\n\\] For more information on the HSL products, bands and applications, please refer to: https://lpdaac.usgs.gov/data/get-started-data/collection-overview/missions/harmonized-landsat-sentinel-2-hls-overview/\n\n# Import surface reflectance bands\nlibrary(terra)\nlibrary(RColorBrewer)\n\nb4=rast(\"./SampleData-master/raster_files/landsat/HLS.L30.T15RYP.2024055T163211.v2.0.B04.tif\")\nb5=rast(\"./SampleData-master/raster_files/landsat/HLS.L30.T15RYP.2024055T163211.v2.0.B05.tif\")\n\n# Plot rasters\nplot(b4, main=\"Red band\",col= brewer.pal(11,\"BrBG\"))\nplot(b5, main=\"NIR band\",col= brewer.pal(11,\"BrBG\"))\n\n\n\n\n\n\n\n\n\n\n\n\nUsing the spectral band rasters provided above, complete the following objectives:\n\n(10) Generate and plot NDVI raster for the areal domain. [HINT: Use multi-raster arithmetic from 4.1.2]\n(10) Reproject the NDVI raster to the following projection: North American Datum 1983 (NAD83, refer to Sec 3.2.8 for help)\n(10) Clamp the range of the NDVI to [-1,1]. [Hint: Checkout the flash card at the end of chapter 4 or try ?t erra::clamp]\n(10) Plot the reprojected NDVI raster and add the following locations of interest to the map [Hint: Check out Sec 3.2.8]:\n\nLocation 1: Lon1=-90.6, Lat1= 30.4\nLocation 2:Lon2=-90, Lat2= 30.1\n\n(10) Extract NDVI values for the two locations given above. [Hint look at the example for extract in 6.3.1]\n(20) Crop the NDVI raster to the following extent and plot the cropped raster: [Xmin, Xmax, Ymin, Ymax]= [-90.5, -90.4, 30.05, 30.15]. [HINT: Create a vector using the extent and use it for cropping]\n(30) Use NASA Earth Data Search (as shown in Section 6.2.6) to download Harmonized Landsat Sentinel-2 (HLS) surface reflectance for band 4 and 5 for any date and region of your choice. Repeat objectives 1 through 3 and plot the resultant NDVI raster."
  },
  {
    "objectID": "ch10.html#exercise-4",
    "href": "ch10.html#exercise-4",
    "title": "10  Practice Exercises",
    "section": "10.5 Exercise #4",
    "text": "10.5 Exercise #4\nSoil moisture is an important hydrological variable with strong linkages to crop health and yield. Satellites such as NASA’s SMAP and ESA’s SMOS provide soil moisture information for the surface profile (0-5 cm). However, temporal variability in the surface soil moisture gets attenuated and dampened as water infiltrates through the soil layers. This is followed by evapotranspirative losses to the atmosphere from the rootzone. The time delay in surface soil moisture and evapotranspiration variability indicates several critical aspects of ecosystem functioning, terrestrial water and energy balance and soil hydraulic processes.\nIn this exercise, we will simulate the attenuation and dampening effect on the surface soil moisture using a moving average filter and study how increasing smoothing of surface soil moisture correlates with the variability in evapotranspiration. We will use simulated gridded surface soil moisture and evapotranspiration from VIC land surface model for the year 2011.\n\n\nDownload CONUS-wide daily simulated soil moisture (soilmoist.2011.nc) and evapotranspiration (et.2011.nc) for the year 2011 from NOAA-PSL’s servers. using the following file location:\nsm_path = \"https://downloads.psl.noaa.gov//Datasets/livneh/fluxvars/soilmoist.2011.nc\"\net_path = \"https://downloads.psl.noaa.gov//Datasets/livneh/fluxvars/et.2011.nc\"\n\n\n\nImport the downloaded netCDFs to workspace as spatRasters. Print the time stamp and variable names of the two spatRasters using terra::time() and terra::names() functions. You will notice that et.2011.nc has one raster layer for each day. soilmoist.2011.nc contains three raster layers (one for each soil profile level) per day.\n\n\n\nCreate a new multilayer spatRaster for raster layers for soil profile level 1. (Hint: using substr() on the layer names() from soilmoist.2011.nc, subset the layers with lev= “1” in their name)\n\n\n\nCreate a polygon with with extent: xmin=255, xmax=265, ymin=40, ymax=48.\na) Plot a map of level 1 soil moisture for day 10 and overlay the polygon on the map.\nb) Crop evapotranspiration and soil moisture spatRasters to the spatial polygon.\nc) Plot the raster layers for day 10 from the cropped evapotranspiration and soil moisture SpatRasters.\n\n\n\n\n\n\n\nIf smTS is the soil moisture time series, and Tagg is the time-period of moving average, the function generates a right aligned rolling mean (moving average) of the time series. NA is used as fill values for the first Tagg-1 cells of the output.\nzoo::rollmean(smTS, Tagg, fill=NA, align = \"right\")\nCreate and report a custom function for applying the moving average on a time series with the following components:\na) Two input arguments:\ni) minSamp (minimum number of samples in a time series)\nii) Tagg (time period of aggregation)\nb) Error exception using trycatch, where the function returns NA series for the pixel where error occurs.\nFor a point of interest at Lon=260, Lat=45:\n\nExtract a sample time series from level 1 soil moisture\nApply the custom function made in the previous step to the sample time series.\nGenerate soil moisture moving average for the selected location with for four Tagg values, 7, 15, 30 and 45 days. Keep minSamp fixed at 100.\nPlot the extracted level 1 soil moisture time series for the point of interest. Overlay aggregated time series at Tagg=7, 15, 30, 45 on the same x-axis. Comment on the relationship between Tagg and time series smoothing.\n\n\n\n\n\n\n\n\nApply the custom moving average function in parallel on each grid of the cropped multilayer soil moisture SpatRaster (from step 4) with Tagg=7, 15, 30, 45. Keep minSamp fixed at 100. Use all-but-one available system cores for parallel computing.\nIf x and y are two time series, then the following operation gives the Pearson’s correlation between the two datasets:\ncor(x, y, use = \"pairwise.complete.obs\")\nUsing a custom function, calculate cell-wise correlation between evapotranspiration and moving averaged level 1 soil moisture for Tagg=7, 15, 30 and 45. (Hint: ?terra::xapp)\nCreate a multilayer raster by concatenating the output rasters from Step 8.\n\nPlot the multilayer rasters of grid-wise correlation between evapotranspiration and level 1 soil moisture with Tagg =7, 15, 30 and 45. Fix legend range as (0,1).\nUsing global() function on the concatenated rasters, calculate and report the layer-wise median correlation between evapotranspiration and moving averaged level 1 soil moisture for the areal domain."
  },
  {
    "objectID": "chN.html#acknowledgement",
    "href": "chN.html#acknowledgement",
    "title": "11  Acknowledgement and References",
    "section": "11.1 Acknowledgement",
    "text": "11.1 Acknowledgement\nSpecial thanks to Debasish Mishra (Texas A&M University) for his contribution in the development of this resource."
  },
  {
    "objectID": "chN.html#references",
    "href": "chN.html#references",
    "title": "11  Acknowledgement and References",
    "section": "11.2 References",
    "text": "11.2 References"
  }
]