[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R for Spatial Analysis & Visualization",
    "section": "",
    "text": "1 Introduction\nStatistical computing is essential for scientific inquiry, discovery, and storytelling. As the availability of dataset and access to computing has significantly increased over the recent years, the scope of scientific inquiry is restricted, only, by the imagination of the inquirer.\n\n“Scientific inquiry starts with observation. The more one can see, the more one can investigate” — Martin Chalfie\n\nHowever, analysis of large-scale geospatial data (regional- to global scale at high spatial and temporal resolution) can be computationally expensive and time-consuming, especially when working with multiple formats and sources of data. R- a higher-level programming language, provides a powerful computational alternative to popular Geographic Information System (GIS) software to organize, analyze and visualize geospatial datasets. R enjoys a vast collection of open-source libraries for GIS-type operations and proven statistical analysis and data visualization capabilities.\nIn this course, we equip ourselves with hands-on knowledge of accessing, analyzing, and visualizing open-source satellite remote-sensing and geospatial datasets for hydrological, agricultural, and climatological studies within the R environment. The objective of this course is to learn R for:\n\nAnalyzing geospatial datasets (raster and vector),\nPerforming statistical analysis for each feature/ layer, and,\nMapping and visualizing spatial datasets.\n\nThe course will include the latest R tools for working with global Earth-observation datasets from several remote-sensing platforms, such as NASA’s MODIS and SMAP. Basic operations of geospatial analysis such as (re)projection, (re)sampling, summary statistics, merge/join, and (re)shape will be covered. The students will be introduced to structured/layered spatial datasets such as NetCDF and HDF formats used in climate modeling. Special emphasis will be placed on applying available out-of-the-box parallel computing techniques for geospatial analysis available in R.\nWe will first start with a refresher of basic R programming in Chapter 1. In Chapter 2 and 3, we will explore spatial data visualization, before we learn about large-scale application of parallel computing for geospatial analysis.\nPurpose of this document:\nThis resource will serve as a dynamic class note, where students can access detailed concepts, codes and exercises related to this class. Notes will be updated regularly as the class progresses with up-to-date material and upcoming assignments."
  },
  {
    "objectID": "ch1.html#operators-and-data-types",
    "href": "ch1.html#operators-and-data-types",
    "title": "2  Basics of R",
    "section": "2.1 Operators and data types",
    "text": "2.1 Operators and data types\n\n2.1.1 Basic operators\nIn this section, we will learn about some basic R operators that are used to perform operations on variables. Some most commonly used operators are shown in the table below.\n\n\n\n\nR follows the conventional order (sequence) to solve mathematical operations, abbreviated as BODMAS: Brackets, Orders (exponents), Division, Multiplication, Addition, and Subtraction\n\n\n2+4+7 # Sum\n\n[1] 13\n\n4-5   # Subtraction\n\n[1] -1\n\n2*3   # Multiplication\n\n[1] 6\n\n1/2   # Division\n\n[1] 0.5\n\n# Order of operation\n1/2*3+4-5\n\n[1] 0.5\n\n1/2*(3+4-5)\n\n[1] 1\n\n1/(2*(3+4-5))\n\n[1] 0.25\n\n1/(2*3+4-5) \n\n[1] 0.2\n\n# Notice how output changes with the placement of operators\n\n# Other operators:\n2^3\n\n[1] 8\n\nlog(10)\n\n[1] 2.302585\n\nsqrt(4)\n\n[1] 2\n\npi\n\n[1] 3.141593\n\n# Clear the Environment\nrm(list=ls()) # rm is for remove,ls is short for list. The empty parenthesis i.e. () signifies all content. \n\n\n\n2.1.2 Basic data operations\nIn this section, we will create some vector data and apply built-in operations to examine the properties of a dataset.\n\n# The \"is equal to\" or \"assignment operator in R is \"&lt;-\" or \"=\" \n\n# Generate sample data. Remember \"c\" comes from for \"concatenate\". \ndata&lt;-c(1,4,2,3,9)    # Try data = c(1,4,2,3,9). Is there any difference in data in both cases?\n\n# rbind combines data by rows, and hence \"r\"bind\n# cbind combines data by columns, and hence \"c\"bind\n\n# Checking the properties of a dataset. Note: the na.rm argument ignores NA values in the dataset.\ndata=rbind(1,4,2,3,9) \ndim(data)           # [5,1]: 5 rows, 1 column\n\n[1] 5 1\n\ndata[2,1]           # Show the value in row 2, column 1\n\n[1] 4\n\ndata[c(2:5),1]      # Show a range of values in column 1\n\n[1] 4 2 3 9\n\nmean(data, na.rm=T) # Mean\n\n[1] 3.8\n\nmax(data)           # Maximum\n\n[1] 9\n\nmin(data)           # Minimum\n\n[1] 1\n\nsd(data)            # Standard deviation\n\n[1] 3.114482\n\nvar(data)           # Variance\n\n     [,1]\n[1,]  9.7\n\nsummary(data) \n\n       V1     \n Min.   :1.0  \n 1st Qu.:2.0  \n Median :3.0  \n Mean   :3.8  \n 3rd Qu.:4.0  \n Max.   :9.0  \n\nstr(data)        # Prints structure of data\n\n num [1:5, 1] 1 4 2 3 9\n\nhead(data)       # Returns the 1st 6 items in the object\n\n     [,1]\n[1,]    1\n[2,]    4\n[3,]    2\n[4,]    3\n[5,]    9\n\nhead(data, 2)    # Print first 2\n\n     [,1]\n[1,]    1\n[2,]    4\n\ntail(data, 2)    # Print last 2\n\n     [,1]\n[4,]    3\n[5,]    9\n\n# Do the same, but with \"c()\" instead of \"rbind\"\ndata=c(1,4,2,3,9) \ndim(data)        # Note: dim is NULL\n\nNULL\n\nlength(data)     # Length of a dataset is the number of variables (columns)\n\n[1] 5\n\ndata[2]          # This should give you 4 \n\n[1] 4\n\n# Other operators work in the same way\nmean(data)       # Mean\n\n[1] 3.8\n\nmax(data)        # Maximum\n\n[1] 9\n\nmin(data)        # Minimum\n\n[1] 1\n\nsd(data)         # Standard deviation\n\n[1] 3.114482\n\nvar(data)        # Variance\n\n[1] 9.7\n\n# Text data\ndata=c(\"LSU\",\"SPESS\",\"AgCenter\",\"Tigers\") \ndata             # View\n\n[1] \"LSU\"      \"SPESS\"    \"AgCenter\" \"Tigers\"  \n\ndata[1]\n\n[1] \"LSU\"\n\n# Mixed data\ndata=c(1,\"LSU\",10,\"AgCenter\") # All data is treated as text if one value is text\ndata[3]                       # Note how output is in quotes i.e. \"10\"\n\n[1] \"10\"\n\n\n\nFor help with a function in R, just type ? followed by the function to display information in the help menu. Try pasting ?sd in the console.\n\n\n\n2.1.3 Data types\nIn R, data is stored as an “array”, which can be 1-dimensional or 2-dimensional. A 1-D array is called a “vector” and a 2-D array is a “matrix”. A table in R is called a “data frame” and a “list” is a container to hold a variety of data types. In this section, we will learn how to create matrices, lists and data frames in R.\n\n\n\n\n# Lets make a random matrix\ntest_mat = matrix( c(2, 4, 3, 1, 5, 7), # The data elements \n  nrow=2,         # Number of rows \n  ncol=3,         # Number of columns \n  byrow = TRUE)   # Fill matrix by rows \n\ntest_mat = matrix( c(2, 4, 3, 1, 5, 7),nrow=2,ncol=3,byrow = TRUE) # Same result \ntest_mat\n\n     [,1] [,2] [,3]\n[1,]    2    4    3\n[2,]    1    5    7\n\ntest_mat[,2]      # Display all rows, and second column\n\n[1] 4 5\n\ntest_mat[2,]      # Display second row, all columns\n\n[1] 1 5 7\n\n# Types of datasets\nout = as.matrix(test_mat)\nout               # This is a matrix\n\n     [,1] [,2] [,3]\n[1,]    2    4    3\n[2,]    1    5    7\n\nout = as.array(test_mat)\nout               # This is also a matrix\n\n     [,1] [,2] [,3]\n[1,]    2    4    3\n[2,]    1    5    7\n\nout = as.vector(test_mat)\nout               # This is just a vector\n\n[1] 2 1 4 5 3 7\n\n# Data frame and list\ndata1=runif(50,20,30) # Create 50 random numbers between 20 and 30  \ndata2=runif(50,0,10)  # Create 50 random numbers between 0 and 10  \n\n# Lists\nout = list()        # Create and empty list\nout[[1]] = data1    # Notice the brackets \"[[ ]]\" instead of \"[ ]\"\nout[[2]] = data2\nout[[1]]          # Contains data1 at this location\n\n [1] 21.63499 20.92145 20.20013 21.46071 27.80221 21.89280 20.86262 23.26512\n [9] 20.98027 27.37972 26.34618 20.93468 22.57255 29.96447 23.77231 25.50423\n[17] 29.78027 22.58069 20.41028 24.81644 26.48014 29.20168 29.21720 24.39963\n[25] 20.32535 26.89823 29.77202 21.53691 29.77874 27.83340 21.33108 27.49801\n[33] 23.93277 29.28182 27.04368 23.63366 23.28232 22.83714 26.48518 21.28981\n[41] 27.88603 23.19168 29.09860 23.86346 25.30793 20.47214 21.98343 28.88783\n[49] 24.57598 22.79293\n\n# Data frame\nout=data.frame(x=data1, y=data2)\n\n# Let's see how it looks!\nplot(out$x, out$y)\n\n\n\nplot(out[,1])\n\n\n\n\n\nFor a data frame, the dollar “$” sign invokes the variable selection. Imagine how one would receive merchandise in a store if you give $ to the cashier. Data frame will list out the variable names for you of you when you show it some $."
  },
  {
    "objectID": "ch1.html#plotting-with-base-r",
    "href": "ch1.html#plotting-with-base-r",
    "title": "2  Basics of R",
    "section": "2.2 Plotting with base R",
    "text": "2.2 Plotting with base R\nIf you need to quickly visualize your data, base R has some functions that will help you do this in a pinch. In this section we’ll look at some basics of visualizing univariate and multivariate data.\n\n2.2.1 Overview\n\n# Create 50 random numbers between 0 and 100  \ndata=runif(50, 0, 100)   #runif stands for random numbers from a uniform distribution\n\n# Let's plot the data\nplot(data)            # The \"plot\" function initializes the plot.\n\n\n\nplot(data, type=\"l\")  # The \"type\" argument changes the plot type. \"l\" calls up a line plot\n\n\n\nplot(data, type=\"b\")  # Buffered points joined by lines\n\n\n\n# Try options type = \"o\" and type = \"c\" as well.\n\n# We can also quickly visualize boxplots, histograms, and density plots using the same procedure\nboxplot(data)        # Box-and-whisker plot\n\n\n\nhist(data)           # Histogram points\n\n\n\nplot(density(data))  # Plot with density distribution \n\n\n\n\n\n\n2.2.2 Plotting univariate data\nLet’s dig deeper into the plot function. Here, we will look at how to adjust the colors, shapes, and sizes for markers, axis labels and titles, and the plot title.\n\n# Line plots\nplot(data,type=\"o\", col=\"red\",\n     xlab=\"x-axis title\",ylab =\"y-axis title\", \n     main=\"My plot\", # Name of axis labels and title\n     cex.axis=2, cex.main=2,cex.lab=2,            # Size of axes, title and label\n     pch=23,       # Change marker style\n     bg=\"red\",     # Change color of markers\n     lty=5,        # Change line style\n     lwd=2         # Selecting line width\n) \n# Adding legend\nlegend(1, 100, legend=c(\"Data 1\"),\n       col=c(\"red\"), lty=2, cex=1.2)\n\n\n\n# Histograms\nhist(data,col=\"red\",\n     xlab=\"Number\",ylab =\"Value\", main=\"My plot\", # Name of axis labels and title\n     border=\"blue\"\n) \n\n\n\n# Try adjusting the parameters:\n# hist(data,col=\"red\",\n#      xlab=\"Number\",ylab =\"Value\", main=\"My plot\", # Name of axis labels and title\n#      cex.axis=2, cex.main=2,cex.lab=2,            # Size of axes, title and label\n#      border=\"blue\", \n#      xlim=c(0,100), # Control the limits of the x-axis\n#      las=0,      # Try different values of las: 0,1,2,3 to rotate labels\n#      breaks=5    # Try using 5,20,50, 100\n# ) # Using more options and controls\n\n\n\n2.2.3 Plotting multivariate data\nHere, we introduce you to data frames: equivalent of tables in R. A data frame is a table with a two-dimensional array-like structure in which each column contains values of one variable and each row contains one set of values from each column.\n\nplot_data=data.frame(x=runif(50,0,10), \n                     y=runif(50,20,30), \n                     z=runif(50,30,40)) \n\nplot(plot_data$x, plot_data$y) # Scatter plot of x and y data\n\n\n\n# Mandatory beautification\nplot(plot_data$x,plot_data$y, xlab=\"Data X\", ylab=\"Data Y\", main=\"X vs Y plot\",\n     col=\"darkred\",pch=20,cex=1.5) # Scatter plot of x and y data\n\n\n\n# Multiple lines on one axis\nmatplot(plot_data, type = c(\"b\"),pch=16,col = 1:4) \n\n\n\nmatplot(plot_data, type = c(\"b\",\"l\",\"o\"),pch=16,col = 1:4) # Try this now. Any difference? \nlegend(\"topleft\", legend = 1:4, col=1:4, pch=1)            # Add legend to a top left\nlegend(\"top\", legend = 1:4, col=1:4, pch=1)                # Add legend to at top center\nlegend(\"bottomright\", legend = 1:4, col=1:4, pch=1)        # Add legend at the bottom right\n\n\n\n\n\n\n2.2.4 Time series data\nWorking with time series data can be tricky at first, but here’s a quick look at how to quickly generate a time series using the as.Date function.\n\ndate=seq(as.Date('2011-01-01'),as.Date('2011-01-31'),by = 1) # Generate a sequence 31 days\ndata=runif(31,0,10)                 # Generate 31 random values between 0 and 10\ndf=data.frame(Date=date,Value=data) # Combine the data in a data frame\nplot(df,type=\"o\")\n\n\n\n\n\n\n2.2.5 Combining plots\nYou can built plots that contain subplots. Using base R, we call start by using the “par” function and then plot as we saw before.\n\npar(mfrow=c(2,2)) # Call a plot with 4 quadrants\n\n# Plot 1\nmatplot(plot_data, type = c(\"b\"),pch=16,col = 1:4) \n\n# Plot 2\nplot(plot_data$x,plot_data$y) \n\n# Plot 3\nhist(data,col=\"red\",\n     xlab=\"Number\",ylab =\"Value\", main=\"My plot\", \n     border=\"blue\") \n\n# Plot4\nplot(data,type=\"o\", col=\"red\",\n     xlab=\"Number\",ylab =\"Value\", main=\"My plot\",\n     cex.axis=2, cex.main=2,cex.lab=2, \n     pch=23,   \n     bg=\"red\", \n     lty=5, \n     lwd=2 \n) \n\n\n\n# Alternatively, we can call up a plot using a matrix\nmatrix(c(1,1,2,3), 2, 2, byrow = TRUE) # Plot 1 is plotted for first two spots, followed by plot 2 and 3 \n\n     [,1] [,2]\n[1,]    1    1\n[2,]    2    3\n\nlayout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE)) # Fixes a layout of the plots we want to make\n\n # Plot 1\nmatplot(plot_data, type = c(\"b\"),pch=16,col = 1:4)\n\n# Plot2\nplot(plot_data$x,plot_data$y) \n\n# Plot 3\nhist(data,col=\"red\",\n     xlab=\"Number\",ylab =\"Value\", main=\"My plot\",\n     border=\"blue\"\n)\n\n\n\n\n\n\n2.2.6 Saving figures to disk\nPlots can be saved as image files or a PDF. This is done by specifying the output file type, its size and resolution, then calling the plot.\n\npng(\"awesome_plot.png\", width=4, height=4, units=\"in\", res=400) \n#Tells R we will plot image in png of given specification\n\nmatplot(plot_data, type = c(\"b\",\"l\",\"o\"),pch=16,col = 1:4)  \nlegend(\"topleft\", legend = 1:4, col=1:4, pch=1)\n\ndev.off() # Very important: this sends the image to disc\n\npng \n  2 \n\n# Keep pressing till you get the following: \n# Error in dev.off() : cannot shut down device 1 (the null device) \n# This ensures that we are no longer plotting.\n\n# It looks like what everything we just plotted was squeezed together to tightly. Let's change the size.\npng(\"awesome_plot.png\", width=6, height=4, units=\"in\", res=400)  #note change in dimension\n#Tells R we will plot image in png of given specification\n\nmatplot(plot_data, type = c(\"b\",\"l\",\"o\"),pch=16,col = 1:3)  \nlegend(\"topleft\", legend = 1:3, col=1:3, pch=16)\n\ndev.off() \n\npng \n  2 \n\n\n\nSome useful resources\nIf you want to plot something a certain way and don’t know how to do it, the chances are that someone has asked that question before. Try a Google search for what your are trying to do and check out some of the forums. There is TONS of material online. Here are some additional resources:\n\nThe R Graph Gallery: https://www.r-graph-gallery.com/\nGraphical parameters: https://www.statmethods.net/advgraphs/parameters.html\nPlotting in R: https://www.harding.edu/fmccown/r/\nHistogram: https://www.r-bloggers.com/how-to-make-a-histogram-with-basic-r/"
  },
  {
    "objectID": "ch1.html#plotting-with-ggplot2",
    "href": "ch1.html#plotting-with-ggplot2",
    "title": "2  Basics of R",
    "section": "2.3 Plotting with ggplot2",
    "text": "2.3 Plotting with ggplot2\n\n2.3.1 Import libraries and create sample dataset\nFor this section, we will use the ggplot2, gridExtra, utils, and tidyr packages. gridExtra and cowplot are used to combine ggplot objects into one plot and utils and tidyr are useful for manipulating and reshaping the data. We will also install some packages here that will be required for the later sections. You will find more information in the sections to follow.\n\n###############################################################\n#~~~ Load required libraries\nlib_names=c(\"ggplot2\",\"gridExtra\",\"utils\",\"tidyr\",\"cowplot\", \"RColorBrewer\")\n\n# If you see a prompt: Do you want to restart R prior to installing: Select **No**. \n\n# Install all necessary packages (Run once)\n# invisible(suppressMessages\n#           (suppressWarnings\n#             (lapply\n#               (lib_names,install.packages,repos=\"http://cran.r-project.org\",\n#                 character.only = T))))\n\n# Load necessary packages\ninvisible(suppressMessages\n          (suppressWarnings\n            (lapply\n              (lib_names,library, character.only = T))))\n\nIn more day-to-day use, you will see yourself using a simpler version of these commands, such as, if you were to install the “ggplot2”,“gridExtra” libraries, you will type:\n\n# To install the package. Install only once\ninstall.packages(\"ggplot2\")\n# To initialize the package. Invoke every time a new session begins.\nlibrary(ggplot2)\n\nSimilarly, again for gridExtra ,\n\ninstall.packages(\"gridExtra\")\nlibrary(gridExtra)\n\nFor this exercise, let us generate a sample dataset.\n\n###############################################################\n#~~~ Generate a dataset containing random numbers within specified ranges\nYear = seq(1913,2001,1)\nJan = runif(89, -18.4, -3.2)\nFeb = runif(89, -19.4, -1.2)\nMar = runif(89, -14, -1.8)\nJanuary = runif(89, 1, 86)\ndat = data.frame(Year, Jan, Feb, Mar, January)\n\n\n\n2.3.2 Basics of ggplot\nWhereas base R has an “ink on paper” plotting paradigm, ggplot has a “grammar of graphics” paradigm that packages together a variety plotting functions. With ggplot, you assign the result of a function to an object name and then modify it by adding additional functions. Think of it as adding layers using pre-designed functions rather than having to build those functions yourself, as you would have to do with base R.\n\nl1 = ggplot(data=dat, aes(x = Year, y = Jan, color = \"blue\")) + # Tell which data to plot\n  geom_line() +      # Add a line\n  geom_point() +     # Add a points\n  xlab(\"Year\") +     # Add labels to the axes\n  ylab(\"Value\")\n\n# Or, they can be specified for any individual geometry\nl1 + geom_line(linetype = \"solid\", color=\"Blue\")  # Add a solid line\n\n\n\nl1 + geom_line(aes(x = Year, y = January)) # Add a different data set\n\n\n\n# There are tons of other built-in color scales and themes, such as scale_color_grey(), scale_color_brewer(), theme_classic(), theme_minimal(), and theme_dark()\n\n# OR, CREATE YOUR OWN THEME! You can group themes together in one list\ntheme1 = theme(\n  legend.position = \"none\",\n  panel.background = element_blank(),\n  plot.title = element_text(hjust = 0.5),\n  axis.line = element_line(color = \"black\"),\n  axis.text.y   = element_text(size = 11),\n  axis.text.x   = element_text(size = 11),\n  axis.title.y  = element_text(size = 11),\n  axis.title.x  = element_text(size = 11),\n  panel.border = element_rect(\n    colour = \"black\",\n    fill = NA,\n    size = 0.5\n  )\n)\n\n\n\n2.3.3 Multivariate plots\nFor multivariate data, ggplot takes the data in the form of groups. This means that each data row should be identifiable to a group. To get the most out of ggplot, we will need to reshape our dataset.\n\nlibrary(tidyr)\n\n# There are two generally data formats: wide (horizontal) and long (vertical). In the horizontal format, every column represents a category of the data. In the vertical format, every row represents an observation for a particular category (think of each row as a data point). Both formats have their comparative advantages. We will now convert the data frame we randomly generated in the previous section to the long format. Here are several ways to do this:\n\n# Using the gather function (the operator %&gt;% is called pipe operator)\ndat2 = dat %&gt;% gather(Month, Value, -Year)\n\n# This is equivalent to: \ndat2 = gather(data=dat, Month, Value, -Year)\n\n# Using pivot_longer and selecting all of the columns we want. This function is the best!\ndat2 = dat %&gt;% pivot_longer(cols = c(Jan, Feb, Mar), names_to = \"Month\", values_to = \"Value\") \n\n# Or we can choose to exclude the columns we don't want\ndat2 = dat %&gt;% pivot_longer(cols = -c(Year,January), names_to = \"Month\", values_to = \"Value\") \n\nhead(dat2) # The data is now shaped in the long format\n\n# A tibble: 6 × 4\n   Year January Month  Value\n  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1  1913    54.4 Jan   -11.9 \n2  1913    54.4 Feb    -7.22\n3  1913    54.4 Mar   -10.4 \n4  1914    73.8 Jan   -10.4 \n5  1914    73.8 Feb   -18.4 \n6  1914    73.8 Mar    -8.26\n\n\nLine plot\n\n# LINE PLOT\nl = ggplot(dat2, aes(x = Year, y = Value, group = Month)) +\n  geom_line(aes(color = Month)) +\n  geom_point(aes(color = Month))\nl\n\n\n\n\nDensity plot\n\n# DENSITY PLOT\nd = ggplot(dat2, aes(x = Value))\nd = d + geom_density(aes(color = Month, fill = Month), alpha=0.4) # Alpha specifies transparency\nd\n\n\n\n\nHistogram\n\n# HISTOGRAM\nh = ggplot(dat2, aes(x = Value))\nh = h + geom_histogram(aes(color = Month, fill = Month), alpha=0.4,\n                 fill = \"white\",\n                 position = \"dodge\")\nh\n\n\n\n\nGrid plotting and saving files to disk\nThere are multiple ways to arrange multiple plots and save images. One method is using grid.arrange() which is found in the gridExtra package. You can then save the file using ggsave, which comes with the ggplot2 library.\n\n# The plots can be displayed together on one image using \n# grid.arrange from the gridExtra package\nimg = grid.arrange(l, d, h, nrow=3)\n\n\n\n# Finally, plots created using ggplot can be saved using ggsave\nggsave(\"grid_plot_1.png\", \n       plot = img, \n       device = \"png\", \n       width = 6, \n       height = 4, \n       units = c(\"in\"), \n       dpi = 600)\n\nAnother approach is to use the plot_grid function, which is in the cowplot library. Notice how the axes are now beautifally aligned.\n\nimg2=cowplot::plot_grid(l, d, h, nrow = 3, align = \"v\") # \"v\" aligns vertical axes and \"h\" aligns horizontal axes\n\nggsave(\"grid_plot_2.png\", \n       plot = img2, \n       device = \"png\", \n       width = 6, \n       height = 4, \n       units = c(\"in\"), \n       dpi = 600)\n\n\n\n2.3.4 Using patchwork for combining ggplots\nPatchwork works with simple operators to combine plots. The operator | arranges plots in a row. The plus sign + does the same but it will try to wrap the plots symmetrically as a square whenever possible. The division i.e. /operator layers a plot on top of another.\n\n#install.packages(\"patchwork\")\nlibrary(patchwork)\n\nl+d\n\n\n\nl/ (h+d)\n\n\n\n# Try: l/d/h or (l+d)/h \n\n# Make your own design for arranging plots (the # sign means empty space): \ndesign &lt;- \"\n  111\n  2#3\n\"\nl + d + h + plot_layout(design = design)\n\n\n\n\n\nSome useful resources\nThe links below offer a treasure trove of examples and sample code to get you started.\n\nThe R Graph Gallery: https://www.r-graph-gallery.com/\nR charts: https://r-charts.com/\nExcellent resource for combining multiple ggplots: https://r-charts.com/ggplot2/combining-plots/"
  },
  {
    "objectID": "ch1.html#exercise-1",
    "href": "ch1.html#exercise-1",
    "title": "2  Basics of R",
    "section": "2.4 Exercise #1",
    "text": "2.4 Exercise #1\nThe U.S. Climate Reference Network (USCRN) is a systematic and sustained network of climate monitoring stations. USCRN has sites across Contiguous U.S. along with some in Alaska, and Hawaii. These stations are instrumented to measure meteorological information such as temperature, precipitation, wind speed, along with other relevant hydrologic variables such as soil moisture at uniform depths (5, 10, 20, 50, 100 cm) at sub-hourly, daily and monthly time scales. Users can access daily data set from all station suing the following link: Index of /pub/data/uscrn/products/daily01 (noaa.gov)\nLet us extract sample data from a USCRN site in Lafayette, LA, USA for 2021.\n\n# Yearly data from the sample station\nCRNdat = read.csv(url(\"https://www.ncei.noaa.gov/pub/data/uscrn/products/daily01/2021/CRND0103-2021-LA_Lafayette_13_SE.txt\"), header=FALSE,sep=\"\")\n\n# Data headers\nheaders=read.csv(url(\"https://www.ncei.noaa.gov/pub/data/uscrn/products/daily01/headers.txt\"), header=FALSE,sep=\"\")\n\n# Column names as headers from the text file\ncolnames(CRNdat)=headers[2,1:ncol(CRNdat)]\n\n# Replace fill values with NA\nCRNdat[CRNdat == -9999]=NA\nCRNdat[CRNdat == -99]=NA\nCRNdat[CRNdat == 999]=NA\n\n# View data sample\nlibrary(kableExtra)\ndataTable = kbl(head(CRNdat,6),full_width = F)\nkable_styling(dataTable,bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\n\nWBANNO\nLST_DATE\nCRX_VN\nLONGITUDE\nLATITUDE\nT_DAILY_MAX\nT_DAILY_MIN\nT_DAILY_MEAN\nT_DAILY_AVG\nP_DAILY_CALC\nSOLARAD_DAILY\nSUR_TEMP_DAILY_TYPE\nSUR_TEMP_DAILY_MAX\nSUR_TEMP_DAILY_MIN\nSUR_TEMP_DAILY_AVG\nRH_DAILY_MAX\nRH_DAILY_MIN\nRH_DAILY_AVG\nSOIL_MOISTURE_5_DAILY\nSOIL_MOISTURE_10_DAILY\nSOIL_MOISTURE_20_DAILY\nSOIL_MOISTURE_50_DAILY\nSOIL_MOISTURE_100_DAILY\nSOIL_TEMP_5_DAILY\nSOIL_TEMP_10_DAILY\nSOIL_TEMP_20_DAILY\nSOIL_TEMP_50_DAILY\nSOIL_TEMP_100_DAILY\n\n\n\n\n53960\n20210101\n2.622\n-91.87\n30.09\n14.0\n5.2\n9.6\n11.0\n0.0\n12.16\nC\n18.7\n4.6\n11.6\n92.8\n49.3\n72.0\n0.401\n0.372\n0.380\n0.405\n0.381\n16.2\n15.3\n15.5\n15.7\n15.5\n\n\n53960\n20210102\n2.622\n-91.87\n30.09\n10.4\n1.9\n6.1\n6.5\n0.0\n8.95\nC\n15.3\n0.4\n7.3\n98.6\n61.6\n78.4\n0.396\n0.370\n0.377\n0.406\n0.376\n14.4\n13.3\n14.1\n15.2\n15.0\n\n\n53960\n20210103\n2.622\n-91.87\n30.09\n16.3\n-0.1\n8.1\n7.9\n0.0\n13.93\nC\n24.3\n-0.9\n9.5\n100.0\n42.1\n76.3\n0.392\n0.368\n0.374\n0.404\n0.373\n12.8\n11.8\n12.8\n14.4\n14.2\n\n\n53960\n20210104\n2.622\n-91.87\n30.09\n22.2\n3.7\n12.9\n12.5\n0.0\n11.56\nC\n26.4\n2.6\n13.2\n98.9\n47.7\n80.2\n0.389\n0.366\n0.370\n0.400\n0.372\n13.0\n12.2\n12.7\n14.0\n14.0\n\n\n53960\n20210105\n2.622\n-91.87\n30.09\n20.7\n4.5\n12.6\n11.4\n0.0\n14.37\nC\n28.9\n3.1\n13.3\n100.0\n27.7\n71.0\n0.388\n0.364\n0.368\n0.399\n0.369\n13.0\n12.1\n12.7\n13.9\n14.0\n\n\n53960\n20210106\n2.622\n-91.87\n30.09\n19.4\n4.9\n12.2\n12.6\n20.7\n9.79\nC\n23.1\n3.5\n12.8\n98.5\n54.7\n78.9\n0.390\n0.363\n0.369\n0.399\n0.370\n12.8\n12.1\n12.5\n13.7\n13.7\n\n\n\n\n\n\n\n\nNotice the variables provided in the dataset. As an example, we can plots soil moisture data from a depth of 20 cm for this station for our reference:\n\n# Sample plot for soil moisture\nx=CRNdat$SOIL_MOISTURE_20_DAILY\n\n# Plot time series and density distribution \nplot(x, type=\"l\", ylab=\"Soil moisture (v/v)\", \n     col=\"cyan4\", lwd=3)\nplot(density(na.omit(x)), main=\" \", xlab=\"\", \n     col=\"cyan4\", lwd=3)\n\n\n\n\n\n\n\n(a) Time series of SM\n\n\n\n\n\n\n\n(b) SM kernel density\n\n\n\n\nFigure 2.1: Soil moisture values at the selected USCRN station\n\n\n\nExercise:\n\nTaking examples of any two USCRN stations across contrasting hydroclimates, compare and contrast any two recorded variables using time series plots, probability density distribution histograms and scatter plots. Select any year of your liking for the analysis.\nSelect two seasons for each elected variable and demonstrate the seasonal variability in the records for summer (MAMJJA) and winter (SONDJF) seasons using any two types of multivariate plots.\n[EXTRA]: For any chosen station, plot a time-series of soil moisture from all available layers with precipitation added as an inverted secondary axis. For inspiration, see Figure 4 in Cheng, et al. 2021. On change of soil moisture distribution with vegetation reconstruction in Mu Us sandy land of China, with newly designed lysimeter. Frontiers in Plant Science, 12, p.60952 at https://www.frontiersin.org/articles/10.3389/fpls.2021.609529/full"
  },
  {
    "objectID": "ch2.html#overview",
    "href": "ch2.html#overview",
    "title": "3  Spatial Mapping in R",
    "section": "3.1 Overview",
    "text": "3.1 Overview\nIn this chapter we will learn about different types of spatial datasets (raster and vector). We will visualize these spatial datasets using static and interactive plotting options available in R. We will also explore different color palettes options available for generating spatial maps for scientific/technical reporting of spatial datasets. We will also learn briefly explore coordinate reference systems and map projections for spatial data representation.\n\n3.1.1 Sample dataset\nWe will familiarize ourselves with several open-source global datasets and use them to practice spatial mapping and computing in R.\n\n\n\n\nGlobal surface soil moisture from NASA’s Soil Moisture Active Passive (SMAP) satellite (Entekhabi, Njoku, and O’Neill 2009)\n\nhttps://smap.jpl.nasa.gov/\n\n\n\n\nNormalized Difference Vegetation Index (NDVI) from Moderate Resolution Imaging Spectroradiometer (MODIS) (Huete, Justice, and Van Leeuwen 1999)\n\nhttps://modis.gsfc.nasa.gov/data/dataprod/mod13.php\n\n\n\n\nGlobal climate reference land regions from Coupled Model Intercomparison Project (CMIP) project (Iturbide et al. 2020)\n\nhttps://essd.copernicus.org/articles/12/2959/2020/\n\n\n\n\nClimate classification (Hyper-arid, arid, semi-arid, sub-humid and humid) based on Global Aridity Index Database (Zomer, Xu, and Trabucco 2022)\n\nhttps://csidotinfo.wordpress.com/2019/01/24/global-aridity-index-and-potential-evapotranspiration-climate-database-v3/\n\n\n\n\n\n\n3.1.2 Data download\nThe sample dataset for this resource is uploaded to GitHub for easy access. Download the sample data manually as a zip file from: https://github.com/Vinit-Sehgal/SampleData . Once downloaded, extract the zip folder to the current working directory.\nAlternatively, use the following script to programmatically download and extract the sample data from the GitHub repository.\n\n###############################################################\n#~~~ Import sample data from GitHub repository\n\nif (dir.exists(\"SampleData-master\")==FALSE){ \n# First we check if the folder already exists. If not, the download begins\ndownload.file(url = \"https://github.com/Vinit-Sehgal/SampleData/archive/master.zip\",\ndestfile = \"SampleData-master.zip\")    # Download \".Zip\"\n\n# Unzip the downloaded .zip file\nunzip(zipfile = \"SampleData-master.zip\")\n}\n\n# getwd()                           # Current working directory\nlist.files(\"./SampleData-master\")   # List folder contents. Do you see sample datasets?\n\n [1] \"CMIP_land\"                               \n [2] \"functions\"                               \n [3] \"images\"                                  \n [4] \"Largescale_geospatial_analysis_2022.html\"\n [5] \"Largescale_geospatial_analysis_2023.html\"\n [6] \"location_points.xlsx\"                    \n [7] \"ne_10m_coastline\"                        \n [8] \"raster_files\"                            \n [9] \"README.md\"                               \n[10] \"sample_pdfs\"                             \n[11] \"SMAP_L3_USA.nc\"                          \n[12] \"SMAPL4_H5\"                               \n[13] \"SMAPL4_rasters\"                          \n[14] \"SMOS_nc\"                                 \n[15] \"USA_states\"                              \n[16] \"Workbook_DVGAR21-Part1.html\"             \n[17] \"Workbook_DVGAR21-Part2.html\""
  },
  {
    "objectID": "ch2.html#using-r-as-gis",
    "href": "ch2.html#using-r-as-gis",
    "title": "3  Spatial Mapping in R",
    "section": "3.2 Using R as GIS",
    "text": "3.2 Using R as GIS\nA geographic information system, or GIS refers to a platform which can map, analyzes and manipulate geographically referenced dataset. A geographically referenced data (or geo-referenced data) is a spatial dataset which can be related to a point on Earth with the help of geographic coordinates. Types of geo-referenced spatial data include: rasters (grids of regularly sized pixels) and vectors (polygons, lines, points).\n\n\n\n\n\nA quick and helpful review of spatial data can be found here: https://spatialvision.com.au/blog-raster-and-vector-data-in-gis/\n\n3.2.1 Plotting raster data: with terra and tmap\nIn this section, we plot global raster data of surface (~5 cm) soil moisture from SMAP. Let’s start by first importing the global soil moisture raster.\n# Import package for raster operations\nlibrary(terra) \n\n# Import SMAP soil moisture raster from the downloaded folder\nsm=terra::rast(\"./SampleData-master/raster_files/SMAP_SM.tif\")\nOnce we have imported the SpatRaster (short for “spatial raster”) using rast() function from terra package, let’s note its attributes. Notice the dimensions, resolution, extent, crs i.e. coordinate reference system and values. Note that the cell of one raster layer can only hold a single numerical value.\n# Print raster attributes\nprint(sm)\nclass : SpatRaster dimensions : 456, 964, 1 (nrow, ncol, nlyr) resolution : 0.373444, 0.373444 (x, y) extent : -180, 180, -85.24595, 85.0445 (xmin, xmax, ymin, ymax) coord. ref. : lon/lat WGS 84 (EPSG:4326) source : SMAP_SM.tif name : SMAP_SM min value : 0.01999998 max value : 0.87667608\n# Try:\n# dim(sm)   # Dimension (nrow, ncol, nlyr) of the raster\n# terra::res(sm)   # X-Y resolution of the raster\n# terra::ext(sm)   # Spatial extent of the raster\n# terra::crs(sm)   # Coordinate reference system\nNow let’s plot the raster using terra::plot.\n# Basic Raster plot \nterra::plot(sm, main = \"Soil Moisture\") \n\n\n\n3.2.2 Color palettes\nUsing a good color palette is an important aspect of spatial mapping. Choice of a good colormap can help the readers understand the key aspects of the map. The selected colors must adequately represent the key features and their differences, wherever applicable, with the least distortion, ambiguity or effort. There are several libraries available in R specifically dedicated to generating color pelettes for scientific mapping. We will also learn the the usage of cetcolor and scico packages to generate perceptually uniform and color-blindness friendly palettes.\n\nSome key packages for generating color palettes for scientific mapping are:\n\nRColorBrewer: https://colorbrewer2.org/#type=sequential&scheme=BuGn&n=3\ncetcolors (Perceptually Uniform Colour Maps): https://cran.r-project.org/web/packages/cetcolor/vignettes/cet_color_schemes.html\nscico (Scientific colormaps): https://github.com/thomasp85/scico\n\n\n# Libraries for generating Colour palettes\nlibrary(RColorBrewer)\nlibrary(cetcolor)\nlibrary(scico)\n\n# To view color palette \nlibrary(unikn)   \n\n#~~ A) User defined color palette using brewer.pal\nmypal1 = RColorBrewer::brewer.pal(10, \"Spectral\") \n\n# Brewerpal outputs a max of 9-11 colors. So,the pelette may needs expansion.\nmypal2= colorRampPalette(mypal1)(20)         # Expand pelette to 20 colors\nunikn::seecol(mypal2)   \n\n\n\n# Some more advanced options include opacity and interpolation method \n# mypal2= colorRampPalette(mypal1,                         \n#                interpolate = c(\"linear\"), # Choose btw linear/spline interpolation\n#                alpha = 0.8)(20)           # Generate 20 colors, opacity set to 0.8\n# unikn::seecol(mypal2)   \n\n#~~ B) User defined color palette using scico package\nmypal1 = scico::scico(20, alpha = 1.0, direction =  -1, palette = \"vik\") \nunikn::seecol(mypal1)   \n\n\n\n#Check: scico_palette_names() for available palettes! \n# Try combinations of alpha=0.5, direction =1, and various different color palette  \n\n#~~ C) User defined color palette using cetcolor package\nmypal2 = cetcolor::cet_pal(20, name = \"r2\")   \nunikn::seecol(mypal2)  \n\n\n\n# Or reverse color pal \nmypal2 = rev(cetcolor::cet_pal(20, name = \"r2\") )  \nunikn::seecol(mypal2) \n\n\n\n\n\n\n3.2.3 Customizing terra plot options\nThere is a long list of customization operations available while plotting rasters in R.\nWe will start with basic plot from terra, and then explore the function by changing different customization options , such as: Try horizontal=TRUE, interpolate=FALSE, change xlim=c(-180, 180) with asp=1, or try legend.shrink=0.4.\n\nsm=rast(\"./SampleData-master/raster_files/SMAP_SM.tif\") # SMAP soil moisture data\n\nterra::plot(sm,\n            main = \"Scientific Plot of Raster\",\n            \n            #Color options\n            col = mypal2,                    # User Defined Color Palette\n            breaks = seq(0, 1, by=0.1),      # Sequence from 0-1 with 0.1 increment\n            colNA = \"lightgray\",             # Color of cells with NA values\n            \n            # Axis options      \n            axes=TRUE,                       # Plot axes: TRUE/ FALSE\n            xlim=c(-180, 180),               # X-axis limit\n            ylim=c(-90, 90),                 # Y-axis limit\n            xlab=\"Longitude\",                # X-axis label\n            ylab=\"Latitue\",                  # Y-axis label\n            \n            # Legend options      \n            legend=TRUE,                     # Plot legend: TRUE/ FALSE\n            \n            # Miscellaneous\n            mar = c(3.1, 3.1, 2.1, 7.1),     # Margins\n            grid = FALSE                     # Add grid lines\n        )\n\n\n\n\n\n\n3.2.4 Spatial plotting with tmap\n\nlibrary(tmap)\n# Set tmap mode: Static plot=\"plot\", Interactive plots=\"view\"\ntmap_mode(\"plot\")          \n\ntmap_SM = tm_shape(sm)+\n  tm_grid(alpha = 0.2)+                             # Transparency of grid\n  tm_raster(alpha = 0.7,                            # Transparency of raster plot\n            palette = mypal2,                       # Color pellete\n            style = \"pretty\",                       # Select style\n            title = \"Volumetric Soil Moisture\")+    # Plot main title\n  tm_layout(legend.position = \n              c(\"left\", \"bottom\"))+                 # Placement of legend\n  tm_xlab(\"Longitude\")+                             # x-lab\n  tm_ylab(\"Latitude\")                               # y-lab \n\ntmap_SM\n\n\n\n\n\n\n3.2.5 Interactive raster visualization aster data with mapview\nFunctionality of terra is largely similar to the legacy package raster (created by the same developer, Robert Hijmans). The development of terra is inspired by computational efficiency in geospatial operations. However, since terra is relatively new, and is continually developed, several other packages require conversion of the SpatRasters to rasterLayer for backwards compatibility.\nTo convert a SpatRaster to RasterLayer, use: sm2=as(sm, \"Raster\")\n\nlibrary(mapview)\nlibrary(raster)\n\n# Convert SpatRaster to raster (from package raster)\nsm2=as(sm, \"Raster\")\nmapview(sm2,                  # RasterLayer\n        col.regions = mypal2, # Color palette \n        at=seq(0, 0.8, 0.1)   # Breaks\n)\n\n\n\n\n\n\n\n3.2.6 Plotting raster data using tidyterra\ntidyterra is a package that add common methods from the tidyverse for SpatRaster and SpatVectors objects created with the terra package. It also adds specific geom_spat*() functions for plotting rasters with ggplot2.\nNote on Performance: tidyterra is conceived as a user-friendly wrapper of terra using the tidyverse} methods and verbs. This approach therefore has a cost in terms of performance.\n\nlibrary(tidyterra) \nlibrary(ggplot2)  \nggplot() +   \n  geom_spatraster(data = sm) +   \n  scale_fill_gradientn(colors=mypal2,                 # Use user-defined colormap\n         name = \"SM\",                                 #  Name of the colorbar\n         na.value = \"transparent\",                    # transparent NA cells  \n         labels=(c(\"0\", \"0.2\", \"0.4\", \"0.6\", \"0.8\")), # Labels of colorbar\n         breaks=seq(0,0.8,by=0.2),                    # Set breaks of colorbar\n         limits=c(0,0.8))+   \n    theme_void()  # Try other themes: theme_bw(), theme_gray(), theme_minimal() \n\n\n\n\nWhat if we are interested in a particular region and not the entire globe? We can plot the map for a specific extent (CONUS, in this case) by changing the range of coord_sfoption. We will also use a different theme: theme_bw. Try xlim = c(114,153) and ylim = c(-43,-11)! We will also add state boundaries and coastline to the plot.\n\nsm_conus= ggplot() +\n  geom_spatraster(data = sm) +\n  scale_fill_gradientn(colors=mypal2,                               # User-defined colormap\n                       name = \"SM\",                                 # Name of the colorbar\n                       na.value = \"transparent\",                    # transparent NA cells\n                       labels=(c(\"0\", \"0.2\", \"0.4\", \"0.6\", \"0.8\")), # Labels of colorbar\n                       breaks=seq(0,0.8,by=0.2),                    # Set breaks of colorbar\n                       limits=c(0,0.8)) +\n  coord_sf(xlim = c(-125,-67),                                      # Add extent for CONUS\n           ylim = c(24,50))+               \n  borders(\"world\",                              # Add global landmass boundaries\n          colour=\"gray43\",                      # Fill light-gray color to the landmass\n          fill=\"transparent\")+                  # Transparent background\n  borders(\"state\",                              # Add US state borders\n          colour=\"gray43\",                      # Use light-gray color\n          fill=\"transparent\")+                  # Use transparent background  \n  theme_bw()                                    # Black & white theme \n\nprint(sm_conus)\n\n\n\n\n\n\n3.2.7 Plotting vector data\nImporting and plotting shapefiles is equally easy in R. We will import the shapefile of the updated global IPCC climate reference regions and global coastlineas Simple Feature (sf) Object. Terra can also be used to import shapefiles as vectors using the function vect. However, sf is more versatile, especially for shapefiles. Notice how the attributes of the sf objects resemble an Excel data sheet.\n\nlibrary(sf)  \nlibrary(terra)\n\n##~~~~ Use sf package for shapefile \n# Import the shapefile of global IPCC climate reference regions (only for land) \nIPCC_shp = read_sf(\"./SampleData-master/CMIP_land/CMIP_land.shp\")\n# View attribute table of the shapefile\nprint(IPCC_shp) # Notice the attributes look like a data frame\n\nSimple feature collection with 41 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -168 ymin: -56 xmax: 180 ymax: 85\nGeodetic CRS:  WGS 84\n# A tibble: 41 × 5\n   V1              V2                V3       V4                        geometry\n   &lt;chr&gt;           &lt;chr&gt;             &lt;chr&gt; &lt;dbl&gt;                   &lt;POLYGON [°]&gt;\n 1 ARCTIC          Greenland/Iceland GIC       1 ((-10 58, -10.43956 58, -10.87…\n 2 NORTH-AMERICA   N.E.Canada        NEC       2 ((-55 50, -55.4386 50, -55.877…\n 3 NORTH-AMERICA   C.North-America   CNA       3 ((-90 50, -90 49.5614, -90 49.…\n 4 NORTH-AMERICA   E.North-America   ENA       4 ((-70 25, -70.43478 25, -70.86…\n 5 NORTH-AMERICA   N.W.North-America NWN       5 ((-105 50, -105.4386 50, -105.…\n 6 NORTH-AMERICA   W.North-America   WNA       6 ((-130 50, -129.5614 50, -129.…\n 7 CENTRAL-AMERICA N.Central-America NCA       7 ((-90 25, -90.37179 24.76923, …\n 8 CENTRAL-AMERICA S.Central-America SCA       8 ((-75 12, -75.28 11.67333, -75…\n 9 CENTRAL-AMERICA Caribbean         CAR       9 ((-75 12, -75.32609 12.28261, …\n10 SOUTH-AMERICA   N.W.South-America NWS      10 ((-75 12, -74.57143 12, -74.14…\n# ℹ 31 more rows\n\n##~~~~ Use terra package for shapefile \nIPCC_shp = vect(\"./SampleData-master/CMIP_land/CMIP_land.shp\") # Reference regions\nprint(IPCC_shp)\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 41, 4  (geometries, attributes)\n extent      : -168, 180, -56, 85  (xmin, xmax, ymin, ymax)\n source      : CMIP_land.shp\n coord. ref. : lon/lat WGS 84 (EPSG:4326) \n names       :            V1                V2    V3    V4\n type        :         &lt;chr&gt;             &lt;chr&gt; &lt;chr&gt; &lt;int&gt;\n values      :        ARCTIC Greenland/Iceland   GIC     1\n               NORTH-AMERICA        N.E.Canada   NEC     2\n               NORTH-AMERICA   C.North-America   CNA     3\n\ndim(IPCC_shp) # Notice the dimensions of the shapefile\n\n[1] 41  4\n\n# Load global coastline shapefile \ncoastlines = vect(\"./SampleData-master/ne_10m_coastline/ne_10m_coastline.shp\")\nprint(coastlines)\n\n class       : SpatVector \n geometry    : lines \n dimensions  : 4132, 2  (geometries, attributes)\n extent      : -180, 180, -85.22194, 83.6341  (xmin, xmax, ymin, ymax)\n source      : ne_10m_coastline.shp\n coord. ref. : lon/lat WGS 84 (EPSG:4326) \n names       : scalerank featurecla\n type        :     &lt;num&gt;      &lt;chr&gt;\n values      :         6  Coastline\n                       6  Coastline\n                       6  Coastline\n\n# Notice the difference in the geometry of coastlines and IPCC_shp\n\n\n# Alternatively, download global coastlines from the web \n# NOTE: May not work if the online server is down\n# download.file(\"https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/110m/physical/ne_110m_coastline.zip?version=4.0.1\",destfile = 'ne_110m_coastline.zip')\n# # Unzip the downloaded file\n# unzip(zipfile = \"ne_110m_coastline.zip\",exdir = 'ne-coastlines-110m')\n\n# Interactive polygon map with mapview\n\nlibrary(mapview)\nmapview(IPCC_shp) # Scroll over the region of interest and find the attributes for the region\n\n\n\n\n\n\nSubsetting vector data is similar to selecting a row from a data frame.\n\nENA_poly=IPCC_shp[4,] # Subset shapefile for Eastern North-America (ENA) \nENA_poly              # Polygon contents - notice it has 4 attributes\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 1, 4  (geometries, attributes)\n extent      : -90, -55, 25, 50  (xmin, xmax, ymin, ymax)\n coord. ref. : lon/lat WGS 84 (EPSG:4326) \n names       :            V1              V2    V3    V4\n type        :         &lt;chr&gt;           &lt;chr&gt; &lt;chr&gt; &lt;int&gt;\n values      : NORTH-AMERICA E.North-America   ENA     4\n\n# Plot ENA polygon using terra\nterra::plot(ENA_poly, main=\"Polygon for Eastern North-America\")  \n\n\n\n\nLet us see how to combine rasters with overlapping vectors with terra . Remember to execute the lines with add=TRUE with the base plot.\n\nterra::plot(sm, col=mypal2)\nterra::plot(IPCC_shp, add=TRUE, col=\"transparent\", border = \"black\")\nterra::plot(coastlines, add=TRUE, col=\"transparent\", border = \"black\")\n\n\n\n\nLets mark the location of Baton Rouge on this map. We will first make a base plot, over which coastlines and spatial location will be added. Again, remember to run these lines together.\n\n#~~~ Add spatial point to shapefile/ raster\n#~~ Make base map\nterra::plot(IPCC_shp[c(3,4,6,7),], # IPCC land regions for Contiguous US.\n             col = \"lightgray\",     # Background color\n             border = \"black\")      # Border color\n\n#~~ Add coastline\nterra::plot(coastlines, col= \"maroon\", add=TRUE)  \n\n#~~ Add spatial point to the plot\nLong=-91.0; Lat=30.62                  # Lat- Long of Baton Rouge, LA\npoints(cbind(Long,Lat),                # Lat-Long as Spatial Points\n       col=\"blue\", pch=16, cex=1.2)    # Shape, size and color of point\n\n\n\n\n\n\n3.2.8 Reprojection of rasters using terra::project\nA coordinate reference system (CRS) is used to relate locations on Earth (which is a 3-D spheroid) to a 2-D projected map using coordinates (for example latitude and longitude). Projected CRSs are usually expressed in Easting and Northing (x and y) values corresponding to long-lat values in Geographic CRS.\nA good description of coordinate reference systems and their importance can be found here:\n\nhttps://docs.qgis.org/3.4/en/docs/gentle_gis_introduction/coordinate_reference_systems.html\nhttps://datacarpentry.org/organization-geospatial/03-crs/\n\n\n\n\n\n\nIn R, the coordinate reference systems or CRS are commonly specified in EPSG (European Petroleum Survey Group) or PROJ4 format (See: https://epsg.io/). Few commonly used projection systems and their codes are summarized below:\n\n\n\n\n\n\n\n\n\nProjection system\nPROJ.4 code\nEPSG code\n\n\nNAD83 (North American Datum 1983)\n“+proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs +type=crs”\nEPSG:4269\n\n\nMercator\n“+proj=merc +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +R=6371000 +units=m +no_defs +type=crs”\nESRI:53004\n\n\nWGS 84 (World Geodetic System 1984)\n“+proj=merc +lon_0=0 +k=1 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs +type=crs”\nEPSG:3395\n\n\nWGS 84 / Pseudo-Mercator – Spherical Mercator ( used in Google Maps, OpenStreetMap)\n“+proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null +wktext +no_defs +type=crs” |\nEPSG:3857\n\n\nRobinson (better for plotting global data due to minimal distortion, except for higher latitudes)\n“+proj=robin +lon_0=0 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs +type=crs”\nESRI:54030\n\n\n\n\nThe SpatRaster reprojection process is done with project() from the terra package.\n# Importing SMAP soil moisture data\nsm=rast(\"./SampleData-master/raster_files/SMAP_SM.tif\") \n\n#~~ Projection 1: NAD83 (EPSG: 4269)\n\nsm_proj1 = terra::project(sm, \"epsg:4269\")\n\nterra::plot(sm_proj1, \n             main = \"NAD83\",    # Title of the plot\n             col = mypal2,      # Colormap for the plot\n             axes = FALSE,      # Disable axes\n             box = FALSE,       # Disable box around the plots\n             asp = NA,          # No fixed aspect ratio; asp=NA fills plot to window\n             legend=FALSE)      # Disable legend\n\n#~~ Projection 2: World Robinson projection (ESRI:54030)\n\nsm_proj2 = terra::project(sm, \"ESRI:54030\")\n\nterra::plot(sm_proj2, \n             main = \"Robinson\", # Title of the plot\n             col = mypal2,      # Colormap for the plot\n             axes = FALSE,      # Disable axes\n             box = FALSE,       # Disable box around the plots\n             asp = NA,          # No fixed aspect ratio; asp=NA fills plot to window\n             legend=FALSE)      # Disable legend\n\nLet us now plot a map of global surface soil moisture, reprojected to Robinson projection. Notice that to add the raster and vector to the plot, we use the functions geom_spatraster and geom_spatvector respectively. Another package spData provides several important datasets for global mapping, including, US states polygons (us_states), World country polygons (world), global elevation (elev.tif), among others (https://jakubnowosad.com/spData/). We will use world country polygons (world) in our map.\n\nlibrary(spData)\n# Import global political boundaries from spData package\nWorldSHP=terra::vect(spData::world)             \n\n# Generate plot\nRobinsonPlot &lt;- ggplot() +\n  geom_spatraster(data = sm)+                   # Plot SpatRaster layer               \n  geom_spatvector(data = WorldSHP, \n                  fill = \"transparent\") +       # Add world political map\n  ggtitle(\"Robinson Projection\") +              # Add title\n  scale_fill_gradientn(colors=mypal2,           # Use user-defined colormap\n                       name = \"Soil Moisture\",  # Name of the colorbar\n                       na.value = \"transparent\",# Set color for NA values\n                       lim=c(0,0.8))+           # Z axis limit\n  theme_minimal()+                              # Select theme. Try 'theme_void'\n  theme(plot.title = element_text(hjust =0.5),  # Place title in the middle of the plot\n        text = element_text(size = 12))+        # Adjust plot text size for visibility\n  coord_sf(crs = \"ESRI:54030\",                  # Reproject to World Robinson\n           xlim = c(-152,152)*100000,           # Convert x-y limits from decimal Deg. to meter\n           ylim = c(-55,90)*100000)\n\nprint(RobinsonPlot)\n\n\n\n# Save high resolution plot to disk\n  # ggsave(\n  #   \"globalSM.png\",          # Name of the file to be created\n  #   plot = RobinsonPlot,     # Plot to be exported\n  #   bg = \"white\",            # Plot background\n  #   height = 6,              # Height \n  #   width = 10,              # Width  \n  #   units = c(\"in\")          # Units (\"in\", \"cm\", \"mm\" or \"px\")\n  # )\n\nRun the commented script above, and view the exported plot saved on the disk.\n\n\n3.2.9 Reclassify and reproject `SpatRasters`\nSo far we have used continuous color scales for plotting rasters. Now we will reclassify the raster in discrete domains based on the cell values. This operation is called RATifying the raster i.e. adding the Raster Attribute Table (RAT) to the file. This allows use of discrete color scales for plotting.\n\nlibrary(spData)\n\n# Import global political boundaries from spData package\nWorldSHP=terra::vect(spData::world)    \n\n# Import raster file \nsm=rast(\"./SampleData-master/raster_files/SMAP_SM.tif\") \n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n  # Defite category breaks for the raster\n  brk= seq(0,0.8, by=0.1)\n\n  # Add attribute table to the raster\n  library(dplyr)\n  sm_rat=dplyr::mutate(sm,                   # input raster\n               rat = cut(SMAP_SM,            # Raster attribute\n               breaks = seq(0,0.8, by=0.1),  # Breaks for each class\n               labels = levels(cut(values(sm$SMAP_SM), breaks=brk)) # Labels for each class\n  ))\n\n  # levels(cut(values(sm$SMAP_SM), breaks=brk)) is same as typing\n  #   c(\"(0,0.1]\", \"(0.1,0.2]\", \"(0.2,0.3]\", \n  #   \"(0.3,0.4]\", \"(0.4,0.5]\", \"(0.5,0.6]\", \n  #   \"(0.6,0.7]\", \"(0.7,0.8]\")\n  \n  plot(sm_rat$rat) # Simple plot with ratified values\n\n\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Generate global plot in Robinson projection for the RATifies raster\n  \nRobinsonPlot_cat &lt;- ggplot() +\n  geom_spatraster(data = sm_rat,    # Plot SpatRaster layer \n                  aes(fill = rat))+ # aes = the attribute to use for plotting\n  geom_spatvector(data = WorldSHP,\n                  fill = \"transparent\") +       # Add world political map\n  ggtitle(\"Robinson Projection\") +              # Add title\n  scale_fill_manual(values = brewer.pal(length(brk)-1,\"Spectral\"), # Discrete palette \n                    na.value=\"transparent\",      # Set transparent fill values for NA cells\n                    name=\"SM\",                   # Name of the legend bar\n                    labels=levels(cut(values(sm$SMAP_SM), breaks=brk)))+\n  theme_minimal()+                              # Select theme. Try 'theme_void'\n  theme(plot.title = element_text(hjust =0.5),  # Place title in the middle of the plot\n        text = element_text(size = 12))+        # Adjust plot text size for visibility\n  coord_sf(crs = \"ESRI:54030\",                  # Reproject to World Robinson\n           xlim = c(-152,152)*100000,           # Convert x-y limits from decimal Deg. to meter\n           ylim = c(-55,90)*100000)\n\nprint(RobinsonPlot_cat)\n\n\n\n\nOther resources:\n\nMore excellent examples on making maps in R can be found here: https://bookdown.org/nicohahn/making_maps_with_r5/docs/introduction.html.\nQuintessential resource for reference on charts and plots in R: https://www.r-graph-gallery.com/index.html.\n\n\n\n\n\nEntekhabi, Dara, Eni Njoku, and Peggy O’Neill. 2009. “The Soil Moisture Active and Passive Mission (SMAP): Science and Applications.” 2009 IEEE Radar Conference. https://doi.org/10.1109/radar.2009.4977030.\n\n\nHuete, Alfredo, Chris Justice, and Wim Van Leeuwen. 1999. “MODIS Vegetation Index (MOD13).” Algorithm Theoretical Basis Document 3 (213): 295–309.\n\n\nIturbide, Maialen, José M. Gutiérrez, Lincoln M. Alves, Joaquín Bedia, Ruth Cerezo-Mota, Ezequiel Cimadevilla, Antonio S. Cofiño, et al. 2020. “An Update of IPCC Climate Reference Regions for Subcontinental Analysis of Climate Model Data: Definition and Aggregated Datasets.” Earth System Science Data 12 (4): 2959–70. https://doi.org/10.5194/essd-12-2959-2020.\n\n\nZomer, Robert J, Jianchu Xu, and Antonio Trabucco. 2022. “Version 3 of the Global Aridity Index and Potential Evapotranspiration Database.” Scientific Data 9 (1): 409."
  },
  {
    "objectID": "ch3.html#raster-arithmetic-operations",
    "href": "ch3.html#raster-arithmetic-operations",
    "title": "4  Raster Arithmetic and Statistics",
    "section": "4.1 Raster Arithmetic Operations",
    "text": "4.1 Raster Arithmetic Operations\nArithmetic, comparison and logical operations on SpatRasters use the same operators as those used for simple vector-like operations. These operators are listed below:\na) Arithmetic operators\n\n\n\n\n\nb) Comparison operators\n\n\n\n\n\nc) Logical operators\n\n\n\n\n\n\n\n4.1.1 Single Raster Operations\nThe application of these operators on rasters can be conceptualized as element-wise operations on a matrix. For example, assume A is a raster with values shown in the matrix form as below:\n\nThen, A + 1 will be:\n\nNote how each element of the matrix gets an addition of 1. Similarly, for A x 2, each element of A will be multiplied by 2 as:\n\nand, A ^ 2 will be given as:\n\nLogical operations can also be carried out in a similar fashion. For example, if we were to test if the cell values of A are greater than 3 (i.e. A &gt; 3), it will be written as:\n\nNow let us try a few examples of arithmetic, comparison and logical operations on SpatRasters. Before we begin this chapter, let us ensure that we have the sample dataset necessary for the analysis, otherwise, download the files from the GitHub repository using the following codes:\n\n#~~~ Check if sample data exists on disk. If not, download from GitHub repository\n\nif (dir.exists(\"SampleData-master\")==FALSE){ \n  \ndownload.file(url = \"https://github.com/Vinit-Sehgal/SampleData/archive/master.zip\",\ndestfile = \"SampleData-master.zip\")    \n\n# Unzip the downloaded .zip file\nunzip(zipfile = \"SampleData-master.zip\")\nlist.files(\"./SampleData-master\")   # List folder contents\n\n}\n\nTaking example of SMAP soil moisture raster let’s practice the application of arithmetic operations on SpatRasters. We start by importing the SpatRaster in the current R environment.\n\nlibrary(terra) # Import library\n\n# Import SMAP soil moisture raster from the downloaded folder\nsm=terra::rast(\"./SampleData-master/raster_files/SMAP_SM.tif\")\nprint(sm)\n\nclass       : SpatRaster \ndimensions  : 456, 964, 1  (nrow, ncol, nlyr)\nresolution  : 0.373444, 0.373444  (x, y)\nextent      : -180, 180, -85.24595, 85.0445  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : SMAP_SM.tif \nname        :    SMAP_SM \nmin value   : 0.01999998 \nmax value   : 0.87667608 \n\n# Add 1 to raster values\nsm2=sm+1\nprint(sm2) #Notice the max and min values have increased by 1. \n\nclass       : SpatRaster \ndimensions  : 456, 964, 1  (nrow, ncol, nlyr)\nresolution  : 0.373444, 0.373444  (x, y)\nextent      : -180, 180, -85.24595, 85.0445  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource(s)   : memory\nvarname     : SMAP_SM \nname        :  SMAP_SM \nmin value   : 1.020000 \nmax value   : 1.876676 \n\n# Multiply raster values by 2\nsm2=sm*2\nplot(sm2, main= \"sm*2\") # Try sm2=sm*10, or sm2=sm^2 and see the difference in sm2 values\n\n\n\n\nSimilarly, logical operations can also be carried out on rasters with ease in R. Logical operators can be a convenient tool for raster data manipulation. For example, any subset of values can be manipulated based on a user-defined logical criteria.\n\n# Are cell values of sm raster greater than 0.3?\nsm=terra::rast(\"./SampleData-master/raster_files/SMAP_SM.tif\")\nsm2=sm&gt;0.3    \nplot(sm2, main=\"is cell value &gt;0.3\")    # Notice that the raster has only True and False values\n\n\n\n# Replace all values of sm&gt;0.3 with 0.5\nsm=terra::rast(\"./SampleData-master/raster_files/SMAP_SM.tif\")\nsm[sm&gt;0.3]=0.5 \nplot(sm, main=\"replace sm&gt;0.3 with 0.5\")\n\n\n\n# Or replace all values of sm&gt;0.3 with NA \nsm=terra::rast(\"./SampleData-master/raster_files/SMAP_SM.tif\")\nsm[sm&gt;0.3]=NA \nplot(sm, main=\"replace sm&gt;0.3 with NA\")\n\n\n\n# Reaplace all NA values with 0\nsm=terra::rast(\"./SampleData-master/raster_files/SMAP_SM.tif\")\nsm[is.na(sm)]=0\nplot(sm)\n\n\n\n\nSatellites record observations in regularly spaced swaths as they orbit around the globe. In the case of SMAP satellite, it takes 2-3 days to cover the Earth. So far, we used a pre-processed soil moisture raster that provided 3-day averaged soil moisture using rasters from individual days.\nNow, let us evaluate daily raster available from SMAP (pixel recorded at 6 AM local time). Notice that this raster is patchy and uses a fill value (values assigned to missing observations) of -9999. Replacing these fill values with NA prior to statistical analysis are highly useful.\n\n# Reaplace fill values with NA\nsm=rast(\"./SampleData-master/raster_files/SMAP_L3_SM_P_20150401_R17000_002_Soil_Moisture_Retrieval_Data_AM_soil_moisture_867a5e36.tif\")\n\nsmFilter=sm                   # Copy original raster before manipulation\nsmFilter[smFilter==-9999]=NA  # Replace negative values with -9999\n\n# Plot original and manipulated raster\nlayout(matrix(1:2, ncol = 2))   # Specify layout for plots\nplot(sm, main= \"raw raster with fill values\")\nplot(smFilter, main=\"Post filtering -9999\")\n\n\n\n\n\n\n\n4.1.2 Multi-Raster Operations\nSo far we saw examples of arithmetic/ logical operations on a single raster. The tenets of arithmetic operations discussed earlier are also applicable for operations on two (or more) rasters. These operations are carried out cell-wise between the corresponding pixels of the raster layers. Suppose, another raster B is given as:\n\nthen, A+B will be:\n\nSimilarly, AxB will also be a cell-wise multiplication of respective data values:\n\nFor this illustration, we will use rootzone (0-100 cm depth) soil moisture from NASA’s Catchment Land Surface Model (provided as SMAP science Level 4 product) for Contiguous U.S. over a period of three consecutive days i.e. December 13-15, 2021.\n\nSMday1=rast(\"./SampleData-master/SMAPL4_rasters/SMAP_L4_SM_gph_20211213T103000_Vv6030_001_Geophysical_Data_sm_rootzone_2052db45.tif\")\n\nSMday2=rast(\"./SampleData-master/SMAPL4_rasters/SMAP_L4_SM_gph_20211214T103000_Vv6030_001_Geophysical_Data_sm_rootzone_22156243.tif\")\n\nSMday3=rast(\"./SampleData-master/SMAPL4_rasters/SMAP_L4_SM_gph_20211215T103000_Vv6030_001_Geophysical_Data_sm_rootzone_23bd7fe2.tif\")\n\nlayout(matrix(1:3, ncol = 1))                # Specify layout\nplot(SMday1, main=\"Rootzone SM day1\")\nplot(SMday2, main=\"Rootzone SM day2\")\nplot(SMday3, main=\"Rootzone SM day3\")\n\n\n\n\nNote that we can not make out the differences in the rasters as the colors are skewed by the high negative fill value for missing data in the rasters. So, we first remove the fill values (i.e. cells valued at -9999) with NA.\n\nSMday1[SMday1==-9999]=NA  # Replace negative values with -9999\nSMday2[SMday2==-9999]=NA  # Replace negative values with -9999\nSMday3[SMday3==-9999]=NA  # Replace negative values with -9999\n\nlayout(matrix(1:3, ncol = 1))                # Specify layout\nplot(SMday1, main=\"SM day1, filtered\")\nplot(SMday2, main=\"SM day2, filtered\")\nplot(SMday3, main=\"SM day3, filtered\")\n\n\n\n\nNow, let’s calculate the change in soil moisture values between days 1-2 and 1-3. Can we identify a region which showed anomalous wetting during the three days of observation?\n\ndelta12=SMday1-SMday2\ndelta13=SMday1-SMday3\n\nlayout(matrix(1:2, ncol = 1))                # Specify layout\nplot(delta12, main=\"SM difference (day 1 & 2\")\nplot(delta13, main=\"SM difference (day 1 & 3\")\n\n\n\n\nCurious to know why Southern California showed such high levels of wetting? Check out: ’Storm of the season’ dumps record-breaking rainfall on SoCal and snow in the mountains - Los Angeles Times (latimes.com)."
  },
  {
    "objectID": "ch3.html#raster-statistics",
    "href": "ch3.html#raster-statistics",
    "title": "4  Raster Arithmetic and Statistics",
    "section": "4.2 Raster Statistics",
    "text": "4.2 Raster Statistics\nWe are all familiar with common statistical functions available in base R for summarizing arrays, such as: min, max, range, sum, stdev, median, mean, modal. These functions are applied using the global function, which calculates the “global” statistics based on the values of all cells within a raster layer. Global is a versatile function and can also be used to apply more complex user-defined operations on rasters. Let us now look at some examples:\n\nlibrary(terra) \n\n# Import SMAP soil moisture raster from the downloaded folder\nsm=terra::rast(\"./SampleData-master/raster_files/SMAP_SM.tif\")\n\n# Summary statistics\nglobal(sm, mean, na.rm = T)  # Calculate mean, while ignoring NA values\n\n            mean\nSMAP_SM 0.209402\n\n# This is equivalent to: \nsm_val=values(sm)       # Create array of cell values of raster\nmean(sm_val, na.rm = T) # Calculate mean of cell values\n\n[1] 0.209402\n\n\nUse of global function is equivalent to applying a function on all cell values\n\nglobal(sm, sd, na.rm = T) # Calculate standard deviation, while ignoring NA values\n\n               sd\nSMAP_SM 0.1434444\n\n# OR\nsd(values(sm), na.rm = T) # Calculate mean of cell values\n\n[1] 0.1434444\n\n# Similarly, for finding quantiles of a raster layer\nglobal(sm, quantile, probs = c(0.25, 0.75), na.rm = T) # Calculate 25th and 75th percentiles of the ratser layer\n\n              X25.      X75.\nSMAP_SM 0.09521707 0.2922016\n\n# OR\nquantile(values(sm), probs = c(0.25, 0.75), na.rm = T) \n\n       25%        75% \n0.09521707 0.29220164 \n\n\nWe can also generate common summary statistics plots using functions such as hist (histogram), barplot (box and whisker plot), etc.\n\nsm=terra::rast(\"./SampleData-master/raster_files/SMAP_SM.tif\")\n\n# Plot summary using standard statistical functions\nlayout(matrix(1:2, ncol = 2))                # Specify layout\nhist(sm)\nboxplot(sm)\n\n\n\n\n\n4.2.1 User-defined Functions for Raster Statistics\nUser-defined functions can also be applied to raster layer using the global function for specific operations. A user can define their own function with specific instructions which will be executed everytime the function is called. A simple template of a typical R function is given below:\n\nLet us revisit the application of the quantile function by passing it through a user-defined function with global. In the quant_fun shown below function, myRas is the raster whose values are being summarized. Notice how instruction for na.rm is passed through the ignoreNA argument.\n\n# User-defined statistics by defining own function.   \nquant_fun = function(myRas, ignoreNA=TRUE){      \n  p=quantile(myRas, probs = c(0.25, 0.75), na.rm=ignoreNA)\n  return(p)\n} \n\nglobal(sm, quant_fun)   # 25th, and 75th percentile of each layer\n\n              X25.      X75.\nSMAP_SM 0.09521707 0.2922016\n\n# This is equivalent to:\nquant_fun(values(sm))\n\n       25%        75% \n0.09521707 0.29220164"
  },
  {
    "objectID": "ch3.html#exercise-2",
    "href": "ch3.html#exercise-2",
    "title": "4  Raster Arithmetic and Statistics",
    "section": "4.3 Exercise #2",
    "text": "4.3 Exercise #2\nUse the raster files in SampleData-master/SMAPL4_rasters folder, perform the tasks as listed below:\n\nTask 1: Write a user-defined function to count the number of pixels within the range [0.3, 0.4] for 20211213. Use global to implement your function. Pass the specified range as an input argument to the function.\nTask 2: Compared to 20211213, what is the percentage change in 20211215 for the pixels within the [0.3, 0.4] range?\nTask 3: Make a multivariate plot (2 or more variables on the same axis) comparing the density distribution of soil moisture for 20211213 and 20211215?"
  },
  {
    "objectID": "ch5.html#meet-the-data",
    "href": "ch5.html#meet-the-data",
    "title": "5  Spatial Operations on Rasters and Vectors",
    "section": "5.1 Meet the Data",
    "text": "5.1 Meet the Data\nFor this chapter, we will use Aridity index (AI), which is a popular metric for climate classification based on the relative availability of Mean Annual Precipitation (P) compared to the Mean Annual Reference Evapotranspiration (ET0) of a location. Aridity Index is defined as: \\(AI= P / ET_0\\). Here, ET0 is the maximum potential amount of moisture a hydrologic system can transfer to atmosphere through evaporation and transpiration. The value of \\(P / ET_0\\) progressively increases from arid to humid regions. Since P and ET0 can never be negative, AI is always &gt;0.\n\n\n\nGlobal Climate Classification Map Based on Aridity Index\n\n\nFor the purpose of this demonstration, we will use CONUS-wide raster of aridity index estimates (i.e. P/ET0) provided by (Zomer, Xu, and Trabucco 2022), at ~0.8X0.8 KM spatial resolution. Let us first start by downloading the sample data files (refer to previous chapters for the code). We will then import the raster and plot a histogram of the raster to understand the probability distribution of the AI values. Do we agree that most pixels in the raster range within 0-4?\n\nlibrary(terra)\nAI=terra::rast(\"./SampleData-master/raster_files/P_over_ET0.tif\") \n\n# Let us plot a histogram of the raster\nhist(AI, breaks=20, \n     xlim=c(0,5), \n     xlab=\"AI=P/ET0\", \n     ylab=\"Pixels\", main=\"\") \n\n\n\n# We observe most values in the raster are &lt;4. So, we set the range of the plot accordingly\nterra::plot(AI, main=\"AI=P/ET0\", range=c(0,4))"
  },
  {
    "objectID": "ch5.html#raster-resampling",
    "href": "ch5.html#raster-resampling",
    "title": "5  Spatial Operations on Rasters and Vectors",
    "section": "5.2 Raster Resampling",
    "text": "5.2 Raster Resampling\nWe have so far worked with global surface soil moisture raster (from SMAP) and we are familiar with its spatial distribution across the globe. Now, let us consider this question:\n\nHow does the climate impact the spatial distribution of soil moisture?\n\nOne approach of answering the question can be to compare pixel values of SMAP soil moisture with corresponding values of AI to establish the bivariate AI–SM relationship. So, we must first change the resolution of AI raster to match that of the SMAP soil moisture. For this, we will use raster resampling.\nRaster resampling simply refers to making changes to the pixel resolution of the raster. The term “resampling” implies that the pixel values are “sampled” and reassigned to the pixels at the new resolution. This operations often involves an interpolation method (nearest neighbor, bilinear, spline, min, max, mode, average etc). We will try three important functions for changing the resolution of a SpatRaster:\n\nterra::resample - resample to match the resolution of another raster\nterra::aggregate - resample from fine to coarse resolution\nterra::disagg - resample from coarse to fine resolution\n\nRaster resampling can be a critical step during multivariate analysis, where raster pixels must overlap to ensure the datasets from each raster corresponds to the same spatial domain. The following schematic helps illustrate the use of these functions:\n\n\n\n5.2.1 Why Resample?\nTo explore the AI–SM relationship, first, we will resample the pixels in the AI raster to match the spatial resolution of sm pixels using the terra::resample function with bilinear interpolation method. This method estimates new cell values as a weighted-average values of the adjoining pixels. The weights are calculated according to the distance of the target pixel from the adjoining cells. In addition to the bilinear approach, terra::resample has several other interpolation methods available as options such as near (for nearest neighbor interpolation), cubic, sum, min, max, average , rms (root-mean square) etc. For more information on resampling methods within the context of remote sensing, please refer to (Schowengerdt 2007).\nOnce AI raster is resamapled to match sm, we would then be able to generate scatter plots between the two rasters, and evaluate the relationship between aridity index (climate) and soil moisture. We see that the soil moisture increases as AI increases before reaching an asymptotic value as AI&gt;1 (humid climate, indicated by red vertical line in the plot). This relationship follows the famous Budyko formulation of energy and water limits on terrestrial water balance (Chen and Sivapalan 2020). For illustration, we use a simple formulation of Budyko curve to represent the non-linear interrelationship between AI and soil moisture given as:\n\\[\nSM=AI/(1+AI)^{1.5}\n\\]\n\nsm=terra::rast(\"./SampleData-master/raster_files/SMAP_SM.tif\") \n\nAIResamp=terra::resample(AI,                # Raster to be resmapled\n                              sm,                # Target resolution raster\n                              method='bilinear') # bilinear interpolation method\n\n# Check the resolution of the aridity raster after resampling\nres(AIResamp)\n\n[1] 0.373444 0.373444\n\nplot(AIResamp,sm , \n     xlim=c(0,3), ylim=c(0,0.6),\n     xlab=\"P/ET0\", ylab=\"Soil Moisture\", \n     pch=19)\ncurve(x/((1+x)^1.5), col=\"blue\",lwd=3, add = T)\nabline(v=1, col=\"red\", lwd=3) \n\n\n\n\n\nFood for thought: Can we do this analysis had we used AI raster instead of AIResamp? What will be the output is we replace AIResamp with AI in the code above.\n\n\n\n5.2.2 Aggregation and Disaggregation\nNow that we have seen the application of terra::resample function, let us now try terra::aggregate and terra::disagg when we know the factor of (dis)aggregation to be used in each direction. Several functions are available for raster aggregation including mean, max, min, median, sum, modal, sd. Disaggregation can use either near or bilinear methods.\n\nlibrary(terra)\n\n# Import AI raster\nAI=terra::rast(\"./SampleData-master/raster_files/P_over_ET0.tif\") \n# Remove the negative fill value from AI raster\nAI[AI&lt;0]=NA\n\n# Original resolution of raster for reference\nres(AI)\n\n[1] 0.008333333 0.008333333\n\n#~~ Aggregate raster to coarser resolution\nAIcoarse = terra::aggregate(AI,           # Original AI raster\n                            fact = 100,    # Reduce the spatial dimension by a factor of 100\n                            fun = mean,   # Function used to aggregate values. We use within-pixel mean\n                            na.rm=TRUE)   # Ignore NA values\nres(AIcoarse)   # Resolution changed from 0.8KMX0.8KM  to (0.8X100KM)x(0.8X100KM) \n\n[1] 0.8333333 0.8333333\n\nplot(AIcoarse, main=\"Aggregated AI raster\")\n\n\n\n#~~ Disaggregate AI to finer resolution\nAIfine = terra::disagg(AI, \n                   fact=2,               # Reduce the spatial dimension by a factor of 2\n                   method='bilinear')    # Interpolation method \"bilinear\" or \"near\"  \n\nres(AIfine)        # Resolution changed from 0.8KM X 0.8KM  to (0.8/2KM)x(0.8/2KM) \n\n[1] 0.004166667 0.004166667\n\nplot(AIfine, main=\"Disggregated AI raster\")"
  },
  {
    "objectID": "ch5.html#cropping-and-masking",
    "href": "ch5.html#cropping-and-masking",
    "title": "5  Spatial Operations on Rasters and Vectors",
    "section": "5.3 Cropping and Masking",
    "text": "5.3 Cropping and Masking\nCropping and masking operations are used to reduct the spatial extent of a raster/vector data. We will use US states shapefile from spData::us_states to select the polygons for Louisiana and adjoining states. The resource spData::us_states provides features for the Contiguous U.S. as simple feature, i.e., sf collection. We note that the state names in the us_states multipolygon are given in the field NAME, which we will use to subset the multipolygon to the specific states we are interested in. We will take “Louisiana”, “Texas”, “Mississippi”,“Alabama”, “Oklahoma”, “Arkansas” as examples.\n\nlibrary(spData)\nlibrary(sf)\n\nAI=terra::rast(\"./SampleData-master/raster_files/P_over_ET0.tif\") \n\n# Lets view the data\nhead(spData::us_states)\n\nSimple feature collection with 6 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -114.8136 ymin: 24.55868 xmax: -71.78699 ymax: 42.04964\nGeodetic CRS:  NAD83\n  GEOID        NAME   REGION             AREA total_pop_10 total_pop_15\n1    01     Alabama    South 133709.27 [km^2]      4712651      4830620\n2    04     Arizona     West 295281.25 [km^2]      6246816      6641928\n3    08    Colorado     West 269573.06 [km^2]      4887061      5278906\n4    09 Connecticut Norteast  12976.59 [km^2]      3545837      3593222\n5    12     Florida    South 151052.01 [km^2]     18511620     19645772\n6    13     Georgia    South 152725.21 [km^2]      9468815     10006693\n                        geometry\n1 MULTIPOLYGON (((-88.20006 3...\n2 MULTIPOLYGON (((-114.7196 3...\n3 MULTIPOLYGON (((-109.0501 4...\n4 MULTIPOLYGON (((-73.48731 4...\n5 MULTIPOLYGON (((-81.81169 2...\n6 MULTIPOLYGON (((-85.60516 3...\n\n# From this we will subset the sf object for the selected states. We select forst column as we are only interested in a single attribute\nUS_shp=spData::us_states\n\n# Subset selected states from the shapefile\nsouth_sf=US_shp[US_shp$NAME %in% c(\"Louisiana\", \"Texas\", \"Mississippi\",\"Alabama\", \"Oklahoma\", \"Arkansas\"),]\n\n# Convert the sf object to SpatRaster\nsouth_vect=vect(south_sf)\nplot(south_vect)\n\n\n\n# Crop AIfine raster to south_vect extent \nAI_south=crop(AIfine, south_vect)\nplot(AI_south, main=\"Cropped AI\")\nplot(south_vect, add=T)\n\n\n\n# Mask AIfine raster to match south_vect spatial domain \nAI_south_msk=mask(AI_south, south_vect)   # Try: inverse=TRUE argument for fun\nplot(AI_south_msk, main=\"Masked AI\")\nplot(south_vect, add=T)\n\n\n\n\n\nComputing Counsel\n\nFor large rasters, it is computationally efficient to crop and/or mask rasters to the region of interest before the analysis.\ncrop first, and mask later, as masking is computationally expensive than cropping."
  },
  {
    "objectID": "ch5.html#raster-ratification",
    "href": "ch5.html#raster-ratification",
    "title": "5  Spatial Operations on Rasters and Vectors",
    "section": "5.4 Raster RATification",
    "text": "5.4 Raster RATification\nIn the AI plot above, we see the expected climate pattern for the terrestrial landmass. The eastern U.S. receives abundant precipitation, and have high aridity index values. In contrast, Southwestern US have hot and dry climate, and show low values of the aridity index. However, in common use, we are more familiar with the use of generalized climate categories, and not a numerical index. For example, its easier to understand, compare and contrast the difference between the climate of Arizona and Louisiana in terms of “arid” versus “humid”, than the respective AI values.\nSuch terms come from the United Nations Environment Program (UNEP, (Nash 1999)), that divides global climate in five discrete classes based on the aridity index as below:\n\n\n\nTable 1: Aridity Index based climate classes given by UNEP\n\n\nAn attribute table can be added to a raster which serves as a look-up reference for the discrete classification of the continuous variable in the raster. This process of adding a Raster Attribute Table to a raster is called raster RAT-ification.\nAs an example, we will follow class breaks and names given in Table 1 to add a climate attribute to the AI raster. We will cut the pixel values of the AI raster into discrete classes and add the attribute table back to the original AI raster.\n\n# Import AI raster and remove negative fill value\nAI=terra::rast(\"./SampleData-master/raster_files/P_over_ET0.tif\") \n\n# Breaks for each climate class from Table 1\nclass_brk= c(0, 0.03, 0.2, 0.5, 0.65, 10)                                \n# Labels for each climate class from Table 1\nclass_names=c(\"Hyper arid\", \"Arid\", \"Semi arid\", \n              \"Sub humid\", \"Humid\")   \n\n# Divide the cell values in the AI raster into distinct levels\nattributes=base::cut(values(AI), # Notice we apply cut on raster \"values\" \n                 breaks = class_brk,\n                 labels =class_names )\n\n# Add attributes to the SpatRaster as climate class\nAI$climate = attributes\nplot(AI$climate,  plg = list(loc = \"bottomleft\"))"
  },
  {
    "objectID": "ch5.html#zonal-statistics",
    "href": "ch5.html#zonal-statistics",
    "title": "5  Spatial Operations on Rasters and Vectors",
    "section": "5.5 Zonal Statistics",
    "text": "5.5 Zonal Statistics\nZonal statistics refers to estimate statistical measures of the cell values of a raster within the zones of another dataset (raster/vector). In the subsequent examples, we will use the aridity index raster to create a polygon, demarcating the spatial boundaries of the discrete climate zones. We will then use these polygons to extract respective cell values and calculate zonal statistics for each climate.\n\n5.5.1 Zonal Statistics with Polygons\n\n5.5.1.1 Raster to Polygons\nIn the next example, we will convert the aridity raster into a polygon based on aridity classification using terra::as.polygons function.  We will use previously RATified the aridity index raster to generate polygons for the climate zones.\n\n# Convert classified raster to shapefile\narid_poly=as.polygons(AI$climate)   # Convert SpatRaster to a spatial polygon \n\n# Notice the dimension (geometries, attributes) and values of the polygon. \n# Do the values match the classes you created?\nprint(arid_poly)\n\n class       : SpatVector \n geometry    : polygons \n dimensions  : 5, 1  (geometries, attributes)\n extent      : -124.7, -66.98333, 24.55833, 49.38333  (xmin, xmax, ymin, ymax)\n coord. ref. : lon/lat WGS 84 (EPSG:4326) \n names       :    climate\n type        :      &lt;chr&gt;\n values      : Hyper arid\n                     Arid\n                Semi arid\n\n# View polygon of the climate zones\nlibrary(mapview)\nmapview(arid_poly)\n\n\n\n\n\n# You can also crop SpatVectors\nAI_poly_south=terra::crop(arid_poly, south_vect)\nmapview(AI_poly_south)\n\n\n\n\n\n\n\n\n5.5.1.2 Zonal Data Extraction and Statistics\nNow we will use terra::extract function to extract the cell values of the soil moisture raster, sm, within each climate zone. We can then use tapply or the built-in fun option within terra::extract to calculate zonal statistics.\n\nsm=terra::rast(\"./SampleData-master/raster_files/SMAP_SM.tif\")\n\n# Extract cell values for each climate class\nsm_climate_df = terra::extract(sm,      # Raster to be summarized\n                          arid_poly,    # Shapefile/ polygon to summarize the raster\n                          na.rm=TRUE)   # Ignore NA values? yes! \nhead(sm_climate_df)\n\n  ID    SMAP_SM\n1  1 0.06480984\n2  1 0.08408742\n3  1 0.05854325\n4  1 0.05259301\n5  1 0.04967655\n6  1 0.04935537\n\n# Calculate group-wise AI \ntapply(sm_climate_df$SMAP_SM,     # Column to be summarized\n       sm_climate_df$ID,          # Grouping variable\n       median,          # Function to use. User defined functions can be used too\n       na.rm = TRUE)              # Ignore NA values? yes! \n\n         1          2          3          4          5 \n0.07296192 0.06936086 0.14301015 0.21182603 0.32055609 \n\n# OR: We can specify the \"fun\" argument within terra::extract function\nsm_climate_median=terra::extract(sm,    # Raster to be summarized\n                          arid_poly,    # Shapefile/ polygon to summarize the raster\n                          fun=median,   # Statistic needed: median/mean/sum/min/max?\n                          na.rm=TRUE)   # Ignore NA values? yes! \n\n# The climate-wise median values are extracted as a dataframe\nhead(sm_climate_median)\n\n  ID    SMAP_SM\n1  1 0.07296192\n2  2 0.06936086\n3  3 0.14301015\n4  4 0.21182603\n5  5 0.32055609\n\n# Lets plot the climate-wise median of surface soil moisture\nplot(sm_climate_median,     \n     xaxt = \"n\",              # Disable x-tick labels\n     xlab=\"Climate\",          # X axis label\n     ylab=\"Soil moisture\",    # Y axis label\n     type=\"b\",                # line type\n     col=\"blue\",              # Line color\n     main=\"Climate-wise median of surface soil moisture\")\naxis(1, at=1:5, labels=c(\"Hyper-arid\", \"Arid\", \"Semi-Arid\",\"Sub-humid\",\"Humid\"))\n\n\n\n\nLet us revisit our question:\n\nHow does the climate impact the spatial distribution of soil moisture?\n\nCan we make a similar conclusion as before?\n\n\n\n5.5.1.3 extract vs exact_extract\nThis is a good time to learn about another excellent function for raster cell extraction from the exactextractr package, exact_extract. This function is computationally faster and efficient than terra::extract, which will be evident when working with rasters of large size.\n\nRemember:\n\nexactextractr::exact_extract also outputs the fractional cell coverage of each pixel extracted. For some analysis (including ours), this information may not be needed.\nexact_extract works on simple feature, sf, objects. So, we will use st_as_sf to convert SpatVector objects to sf.\n\n\n\nlibrary(exactextractr)\nlibrary(sf)\n\nsm=terra::rast(\"./SampleData-master/raster_files/SMAP_SM.tif\")\n\nzonal_extract=exactextractr::exact_extract(sm,  # Raster to be summarized\n                st_as_sf(arid_poly),    # Convert shapefile to sf (simple feature)\n                force_df = TRUE,        # Output as a data.frame?\n                include_xy = FALSE,     # Include cell lat-long in output?\n                fun = NULL,             # Specify the function to apply for each feature extracted data\n                progress = TRUE)        # Progressbar\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |======================================================================| 100%\n\nzonal_data=sapply(zonal_extract,\"[[\",1)  # Select only cell values (first column within each element of the list)\n\n# Generate boxplot for the zonal data\nboxplot(zonal_data, \n        col=c(\"#800000\", \"#FF7C00\",\"#87FF78\",\"#008BFF\",\"#00008F\"), \n        names=c(\"Hyper arid\", \"Arid\", \"Semi arid\",\"Sub humid\", \"Humid\")  )\n\n\n\n\n\n\n5.5.1.4 User-defined Functions for Zonal Statistics\nCell value extraction for each zone can help a user to carry a diverse set of analysis for each region using custom functions. Once the zonal cell values are extracted, vectorized application of user-defined function can be carried out on these datasets using lapply (list+apply) or purrr::map functions.\n\n# Apply function on cell values for each zone\n\n#~~ Using lapply\nzonal_stat=lapply(zonal_data,median, na.rm=T)     # Returns a list of zonal stats\n\n#~~ Using purrr::map\nlibrary(purrr)\nzonal_stat=purrr::map(zonal_data,\n                      ~ median(.x, na.rm=TRUE)) # Returns a list of zonal stats\n\n#~~ Try user defined function\nmyfun=function (y){\n  # User defined function for calculating median\n  p=median(y, na.rm=TRUE)\n  return(p)\n  }    \n\n#~ Implement function using lapply and map\n#~~ Using lapply\nzonal_stat=lapply(zonal_data,myfun)           # Returns a list of zonal median \n\n#~~ Using purrr::map\nlibrary(purrr)\nzonal_stat=purrr::map(zonal_data,~ myfun(.x)) # Returns a list of zonal median \nzonal_stat=unlist(zonal_stat)                 # Unlist to return a vector \n\nhead(zonal_stat) # Is this the same as the previous result?\n\n[1] 0.06029825 0.07462480 0.12249910 0.18718112 0.31052290\n\n\n\n\n\n5.5.2 Zonal Statistics with RATified Raster\nRasters with discrete data groups can also be used to summarize other rasters. Here, we will use the RATified aridity index raster to extract cell values corresponding to each climate zone.\n\nzonal_data=list()                   # Create an empty list to store values\nclimate_num=as.numeric(AI$climate)  # Convert climate class to numerical values\nclimate_num=resample(climate_num,sm, method=\"near\") # Resample to sm resolution \n\n# Extract pixel values within each climate zone\nZonalCells=function(x){\n  zonalCells=na.omit(sm[climate_num==x])\n  return(zonalCells)\n} \nclimate_zone_num=list(1,2,3,4,5)    # Store climate zone numbers as a list\n\n# Apply function using lapply \nzonal_data=lapply(climate_zone_num,\n                  ZonalCells)\n\n# Calculate and store stats for each climate zone\n#~~ Custom function for zonal median\nzonalMean=function(x){\n  zonalCells=na.omit(sm[climate_num==x])\n  p=median(zonalCells, na.rm=TRUE)\n  return(p)\n} \nclimate_zone_num=list(1,2,3,4,5)    # Store climate zone numbers as a list\nzonal_stat_list=lapply(climate_zone_num,zonalMean)\n\n# Equivalent to:\n# zonal_stat_list=lapply(list(1,2,3,4,5),function(x) (median(sm[climate_num==x], na.rm=TRUE)))\n\n# lapply returns a list, so we unlist the output to get an array\nzonal_stat=unlist(zonal_stat_list)\n\nplot(zonal_stat,     \n     xaxt = \"n\",              # Disable x-tick labels\n     xlab=\"Climate\",          # X axis label\n     ylab=\"Soil moisture\",    # Y axis label\n     type=\"b\",                # line type\n     col=\"blue\",              # Line color\n     main=\"Climate-wise median of surface soil moisture\")\naxis(1, at=1:5, labels=c(\"Hyper-arid\", \"Arid\", \"Semi-Arid\",\"Sub-humid\",\"Humid\"))\n\n\n\n\n\n\n\n\n\n\nChen, Xi, and Murugesu Sivapalan. 2020. “Hydrological Basis of the Budyko Curve: Data-Guided Exploration of the Mediating Role of Soil Moisture.” Water Resources Research 56 (10): e2020WR028221.\n\n\nNash, David J. 1999. “World Atlas of Desertification.” The Geographical Journal 165: 325.\n\n\nSchowengerdt, Robert A. 2007. “CHAPTER 7 - Correction and Calibration.” In Remote Sensing (Third Edition), edited by Robert A. Schowengerdt, Third Edition, 285–XXIII. Burlington: Academic Press. https://doi.org/https://doi.org/10.1016/B978-012369407-2/50010-3.\n\n\nZomer, Robert J, Jianchu Xu, and Antonio Trabucco. 2022. “Version 3 of the Global Aridity Index and Potential Evapotranspiration Database.” Scientific Data 9 (1): 409."
  },
  {
    "objectID": "ch6.html#popular-gridded-data-format",
    "href": "ch6.html#popular-gridded-data-format",
    "title": "6  Gridded Land and Climate Data",
    "section": "6.1 Popular Gridded Data Format",
    "text": "6.1 Popular Gridded Data Format\nRaster and netCDF are two popular formats used for gridded climate data dissemination and archiving.\nWe are all too familiar with the raster format (pixelated, georeferenced data) from the previous chapters. NetCDF (Network Common Data Form), is a common data type for multi-layered, structured, gridded dataset. NetCDF is a machine-independent data format and is a community standard for sharing scientific data.  A netCDF has certain features which makes it suitable for complex scientific data archiving and sharing, namely,\n\n\n\nSelf-Describing. A netCDF file includes information about the data it contains.\nPortable. A netCDF file can be accessed by computers with different ways of storing integers, characters, and floating-point numbers.\nScalable. A small subset of a large dataset may be accessed efficiently.\nAppendable. Data may be appended to a properly structured netCDF file without copying the dataset or redefining its structure.\nShareable. One writer and multiple readers may simultaneously access the same netCDF file.\nArchivable. Access to all earlier forms of netCDF data will be supported by current and future versions of the software.\n- From [Unidata | NetCDF (ucar.edu) https://www.unidata.ucar.edu/software/netcdf/]\n\n\n\n\n\n\nTwo widely used formats for gridded climate data storage and dissemination: (Left) Raster and (Right) netCDF\n\n\nSeveral open-source plaforms and agencies provide open access to a multitude of gridded land and climate datasets generated using satellites and land-surface/ climate models. In the following sections, we will familiarize ourselves with some of these resources."
  },
  {
    "objectID": "ch6.html#open-data-platforms",
    "href": "ch6.html#open-data-platforms",
    "title": "6  Gridded Land and Climate Data",
    "section": "6.2 Open-Data Platforms",
    "text": "6.2 Open-Data Platforms\n\n6.2.1 Climate Data from NOAA Physical Sciences Lab\n\nProduct overview / Data access/ Information: https://psl.noaa.gov/data/gridded/tables/daily.html\n\n\n\n\nA snapshot of NOAA’s Physical Sciences Lab’s portal for gridded climate data access\n\n\nThis website provides several land and climate variables such as: CPC Global Unified Gauge-Based Analysis of Daily Precipitation, CPC Global Temperature, NCEP/NCAR Reanalysis, Livneh daily CONUS near-surface gridded meteorological and derived hydrometeorological data\n\n\n6.2.2 CHIRPS Global Precipitation [~5 km x 5km]\n\nProduct overview: https://climatedataguide.ucar.edu/climate-data/chirps-climate-hazards-infrared-precipitation-station-data-version-2\nData access: https://data.chc.ucsb.edu/products/CHIRPS-2.0/\nData information: https://data.chc.ucsb.edu/products/CHIRPS-2.0/docs/README-CHIRPS.txt\n\n\n\n\nSnapshot of the web portal for CHIRPS-2.0 (Climate Hazards InfraRed Precipitation with Station data, version 2) data access\n\n\n\n\n6.2.3 GIMMS MODIS Global NDVI [~225 m x 225 m]\n\nProduct overview: https://ladsweb.modaps.eosdis.nasa.gov/missions-and-measurements/products/MOD13Q1\nData access: https://gimms.gsfc.nasa.gov/MODIS/\nData information: https://gimms.gsfc.nasa.gov/MODIS/README-global.txt\n\n\n\n\nGlobal Inventory Modeling and Mapping Studies (GIMMS) portal for global MODIS (Terra & Aqua) NDVI access\n\n\n\n\n6.2.4 Climate Prediction Center\n\nProduct overview / Data access/ Information: https://ftp.cpc.ncep.noaa.gov/GIS/\n\n\n\n\nFTP portal for NOAA’s Climate Prediction Center (CPC) data access\n\n\nIncludes several variables including (but not limited to):\n\nClimate Prediction Center (CPC) Morphing Technique (MORPH) to form a global, high resolution precipitation analysis\nJoint Agricultural Weather Facility (JAWF)\nGrid Analysis and Display System (GRADS): Global precipitation monitoring and forecasts, Tmax, Tmin\nInput variables for US Drought Monitor (USDM)\n\n\n\n6.2.5 NASA AρρEEARS\n\nProduct overview / Data access/ Information: https://appeears.earthdatacloud.nasa.gov/\n\n\n\n\nInterface for the Application for Extracting and Exploring Analysis Ready Samples (AρρEEARS)\n\n\nClimate/ land data variables can be extracted for an area or a point using the interactive interface. Click on point samples/ area samples:\n\n\n\nStart a new request\n\n\n\n\n\nSelect the variable and Lat-Long/ area of interest\n\n\n\n\n6.2.6 NASA Earth Data Search\n\nProduct overview / Data access/ Information: https://search.earthdata.nasa.gov/search\n\n\n\n\nEarth Data Search application interface\n\n\n\n6.2.6.1 Bulk Download Order\nNASA Earth Data provides customization options for bulk data download. Lets say we are interested in downloading global SMAP Level 3 soil moisture. We start by selecting the product, and specify start/ end date as needed.\n\n\n\n\n\nSearch for the product, click “Download All”. You will be taken to a log-in page.\n\n\n\n\n\n\n\nAfter logging in, Click “Edit Options”-&gt; “Customize” and select options as needed. Click “Done”.\n\n\n\n\n\n\n\nClick “Download Data”\n\n\n\n\n\n\n\nA “Download Status” page will appear. Click on the “.html” link.\n\n\n\n\n\n\n\nSeveral download options will available. For bulk download, click the link under “Retrieve list of files as a text listing (no html)”\n\n\n\n\n\n\n\nYou will be able to see the active download links.\n\n\n\n\nCopy and Paste these links in any internet download manager (my favorite in Chrono for Google Chrome), Select output location (typically an external hard drive) and let the download begin."
  },
  {
    "objectID": "ch6.html#programmatic-data-acquisition",
    "href": "ch6.html#programmatic-data-acquisition",
    "title": "6  Gridded Land and Climate Data",
    "section": "6.3 Programmatic Data Acquisition",
    "text": "6.3 Programmatic Data Acquisition\nIn the HTTP, FTP or HTP links provided before, one can download a file by clicking on the individual hyperlink. Alternatively, we can use download.file function to download the file programmatically in R. This help us by opening the path to automate download and processing of multiple files with minimal supervision.\n\n6.3.1 Downloading Raster files\nLet us take an example of us_tmax data available at: https://ftp.cpc.ncep.noaa.gov/GIS/GRADS_GIS/GeoTIFF/TEMP/us_tmax/\nRight-click on the raster file for 20240218, and copy the file path. We will then use this link to access the files programmatically using Client URL, or cURL - a utility for transferring data between systems. We will download the raster using download.file to local disk, and saved with a uder-defined name tmax_20240218.tif.\n\nCopied link: https://ftp.cpc.ncep.noaa.gov/GIS/GRADS_GIS/GeoTIFF/TEMP/us_tmax/us.tmax_nohads_ll_20240218_float.tif\n\n# Copied path of the raster\ndata_path &lt;- \"https://ftp.cpc.ncep.noaa.gov/GIS/GRADS_GIS/GeoTIFF/TEMP/us_tmax/us.tmax_nohads_ll_20240218_float.tif\"\n\n# Download the raster using download.file, assign the name tmax_20240218.tif to the downloaded \ndownload.file(url = data_path, \n              method=\"curl\",\n              destfile = \"tmax_20240218.tif\")  \n\n# Plot downloaded file\nlibrary(terra)\ntempRas=rast(\"tmax_20240218.tif\")       # Import raster to the environment \nusSHP=terra::vect(spData::us_states)    # Shapefile for CONUS\n\nplot(tempRas)\nplot(usSHP, add=TRUE)\n\n\n\n\nNow that we have the tmax_20240218 raster, let us extract the values for certain selected locations: s\n\n# Import sample locations from contrasting hydroclimate\nlibrary(readxl)\nloc= read_excel(\"./SampleData-master/location_points.xlsx\")\nprint(loc)\n\n# A tibble: 3 × 4\n  Aridity   State     Longitude Latitude\n  &lt;chr&gt;     &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n1 Humid     Louisiana     -92.7     34.3\n2 Arid      Nevada       -116.      38.7\n3 Semi-arid Kansas        -99.8     38.8\n\n# Value of the lat & lon of the locations\nlatlon=loc[,3:4] \nprint(latlon)\n\n# A tibble: 3 × 2\n  Longitude Latitude\n      &lt;dbl&gt;    &lt;dbl&gt;\n1     -92.7     34.3\n2    -116.      38.7\n3     -99.8     38.8\n\n# Extract time series using \"terra::extract\"\nloc_temp=terra::extract(tempRas,\n                         latlon,               #2-column matrix or data.frame with lat-long\n                         method='bilinear')    # Use bilinear interpolation (or ngb) option\n\nprint(loc_temp)\n\n  ID tmax_20240218\n1  1      12.58291\n2  2      11.52682\n3  3      13.41145\n\n# Add temperature attribute to the data frame as \"temp\"\nloc$temp=loc_temp$tmax_20240218\nprint(loc)\n\n# A tibble: 3 × 5\n  Aridity   State     Longitude Latitude  temp\n  &lt;chr&gt;     &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 Humid     Louisiana     -92.7     34.3  12.6\n2 Arid      Nevada       -116.      38.7  11.5\n3 Semi-arid Kansas        -99.8     38.8  13.4\n\n# Export the modified data as CSV\nwrite.csv(loc, \"df_with_temp.csv\")\n\n\n\n6.3.2 Downloading netCDF\nWe will now download a netCDF of global daily precipitation for the year 2023 from CHIRPS, accessible through the link: https://data.chc.ucsb.edu/products/CHIRPS-2.0/global_daily/netcdf/p05/chirps-v2.0.2023.days_p05.nc\n\n# Copied path of the raster\ndata_path &lt;- \"https://data.chc.ucsb.edu/products/CHIRPS-2.0/global_daily/netcdf/p05/chirps-v2.0.2023.days_p05.nc\"\n\n# Download the raster using download.file, assign the name \"daily_pcp_2023.nc\" to the downloaded \nif (file.exists(\"daily_pcp_2023.nc\")==FALSE){\n  \n download.file(url = data_path, \n              method=\"curl\",\n              destfile = \"daily_pcp_2023.nc\") \n  \n}\n\n# Plot downloaded file\nlibrary(terra)\npcp=rast(\"daily_pcp_2023.nc\")       # Import raster to the environment \nprint(pcp) # Notice the attributes (esp. nlyr, i.e. number of layers, unit and time)\n\nclass       : SpatRaster \ndimensions  : 2000, 7200, 365  (nrow, ncol, nlyr)\nresolution  : 0.05, 0.05  (x, y)\nextent      : -180, 180, -50, 50  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) \nsource      : daily_pcp_2023.nc \nvarname     : precip (Climate Hazards group InfraRed Precipitation with Stations) \nnames       : precip_1, precip_2, precip_3, precip_4, precip_5, precip_6, ... \nunit        :   mm/day,   mm/day,   mm/day,   mm/day,   mm/day,   mm/day, ... \ntime (days) : 2023-01-01 to 2023-12-31 \n\nhead(time(pcp))  # time variable in the netCDF indicating corresponding time of acquisition\n\n[1] \"2023-01-01\" \"2023-01-02\" \"2023-01-03\" \"2023-01-04\" \"2023-01-05\"\n[6] \"2023-01-06\"\n\nworldSHP=terra::vect(spData::world)    # Shapefile for CONUS\n\n# Plot data for a specific layer\nplot(pcp[[100]])   # Same as pcp[[which(time(pcp)==\"2023-04-10\")]]\nplot(worldSHP, add=TRUE)\npoints(latlon, pch=19, col=\"red\")\n\n\n\n\nNext we will extract values for the selected locations. However, compared to the temperature data, which was a single layer, precipitation NetCDF has 365 layers, one for each day in the year 2023. So, when we extract values using the point locations, 365 values for each location are extracted.\n\n# Import sample locations from contrasting hydroclimate\nlibrary(readxl)\nloc= read_excel(\"./SampleData-master/location_points.xlsx\")\nprint(loc)\n\n# A tibble: 3 × 4\n  Aridity   State     Longitude Latitude\n  &lt;chr&gt;     &lt;chr&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n1 Humid     Louisiana     -92.7     34.3\n2 Arid      Nevada       -116.      38.7\n3 Semi-arid Kansas        -99.8     38.8\n\n# Value of the lat & lon of the locations\nlatlon=loc[,3:4] \nprint(latlon)\n\n# A tibble: 3 × 2\n  Longitude Latitude\n      &lt;dbl&gt;    &lt;dbl&gt;\n1     -92.7     34.3\n2    -116.      38.7\n3     -99.8     38.8\n\n# Extract time series using \"terra::extract\"\nloc_pcp=terra::extract(pcp,\n                       latlon,               #2-column matrix or data.frame with lat-long\n                       method='bilinear')    # Use bilinear interpolation (or ngb) option\n# View data sample\nloc_pcp[,1:8] # View(loc_pcp)\n\n  ID  precip_1 precip_2 precip_3 precip_4 precip_5 precip_6 precip_7\n1  1 0.0000000 9.755308 46.80546 0.000000 0.000000 0.000000        0\n2  2 7.3486811 0.000000  0.00000 1.276224 3.523982 1.272632        0\n3  3 0.7002416 4.201450  0.00000 0.000000 0.000000 0.000000        0\n\n# Plot hyetograph for the location in Louisiana\nlibrary(ggplot2)\npcp_df1=data.frame(time=time(pcp),\n                   pcp=as.numeric(loc_pcp[1,-c(1)]))  # Select first row, exclude the first column\n\n# ggplot\nggplot(pcp_df1,aes(x=time,y=pcp)) + \n  geom_bar(stat = 'identity')+\n  theme_bw()+\n  ylab(\"Precipitation [mm/day]\")+\n  xlab(\"Time [Days]\")\n\n\n\n# Export the extracted data as CSV\nwrite.csv(loc, \"extracted_pcp2023.csv\")"
  },
  {
    "objectID": "ch6.html#exercise-3",
    "href": "ch6.html#exercise-3",
    "title": "6  Gridded Land and Climate Data",
    "section": "6.4 Exercise #3",
    "text": "6.4 Exercise #3\nThe Harmonized Landsat Sentinel-2 (HLS) project provides consistent multiband surface reflectance (SR) data from NASA/USGS Landsat 8 and Landsat 9 satellites (using Operational Land Imager, OLI) and Europe’s Copernicus Sentinel-2A and Sentinel-2B satellites (using Multi-Spectral Instrument, MSI). These measurements provide global land observations with a 2–3 days temporal frequency at 30-meter spatial resolution. Sample rasters for band 4 (Red) and band 5 (Near-Infrared) from this resource are provided at the following location in the sample data: “./SampleData-master/raster_files/landsat/”\nFor LANDSAT 8, the Normalized Difference Vegetation Index (a proxy for vegetation health) is calculated as:\n\\[\nNDVI=(Band 5- Band 4)/(Band 5+ Band 4)\n\\] For more information on the HSL products, bands and applications, please refer to: https://lpdaac.usgs.gov/data/get-started-data/collection-overview/missions/harmonized-landsat-sentinel-2-hls-overview/\n\n# Import surface reflectance bands\nlibrary(terra)\nlibrary(RColorBrewer)\n\nb4=rast(\"./SampleData-master/raster_files/landsat/HLS.L30.T15RYP.2024055T163211.v2.0.B04.tif\")\nb5=rast(\"./SampleData-master/raster_files/landsat/HLS.L30.T15RYP.2024055T163211.v2.0.B05.tif\")\n\n# Plot rasters\nplot(b4, main=\"Red band\",col= brewer.pal(11,\"BrBG\"))\nplot(b5, main=\"NIR band\",col= brewer.pal(11,\"BrBG\"))\n\n\n\n\n\n\n\n\n\n\n\n\nUsing the spectral band rasters provided above, complete the following objectives:\n\n(10) Generate NDVI raster for the areal domain.\n(10) Reproject the NDVI raster to the following projection: North American Datum 1983 (NAD83, refer to Sec 3.2.8 for help)\n(10) Clamp the range of the NDVI to [-1,1].\n(10) Plot the reprojected NDVI raster and add the following locations of interest to the map:\n\nLocation 1: Lon1=-90.6, Lat1= 30.4\nLocation 2:Lon2=-90, Lat2= 30.1\n\n(10) Extract NDVI values for the two locations given above.\n(20) Crop the NDVI raster to the following extent and plot the cropped raster: [Xmin, Xmax, Ymin, Ymax]= [-90.5, -90.4, 30.05, 30.15]\n(30) Use NASA Earth Data Search (as shown in Section 6.2.6) to download Harmonized Landsat Sentinel-2 (HLS) surface reflectance for band 4 and 5 for any date and region of your choice. Repeat objectives A through C and plot the resultant NDVI raster."
  },
  {
    "objectID": "chN.html#acknowledgement",
    "href": "chN.html#acknowledgement",
    "title": "7  Acknowledgement and References",
    "section": "7.1 Acknowledgement",
    "text": "7.1 Acknowledgement\nSpecial thanks to Debasish Mishra (Texas A&M University) for his contribution in the development of this resource."
  },
  {
    "objectID": "chN.html#references",
    "href": "chN.html#references",
    "title": "7  Acknowledgement and References",
    "section": "7.2 References",
    "text": "7.2 References"
  }
]